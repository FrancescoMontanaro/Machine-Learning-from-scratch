{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Any\n",
    "\n",
    "# Add the path to the custom library to the system path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import the module from the custom library\n",
    "from src import utils\n",
    "from src import Model\n",
    "from src import layers\n",
    "from src import  Tensor\n",
    "from src import callbacks\n",
    "from src import optimizers\n",
    "from src import activations\n",
    "from src import loss_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000 # Number of samples to generate\n",
    "train_test_split_pct = 0.2 # Percentage of samples to use for testing\n",
    "train_valid_split_pct = 0.2 # Percentage of samples to use for validation\n",
    "learning_rate = 1e-03 # Learning rate for the optimizer\n",
    "batch_size = 512 # Number of samples to use for each batch\n",
    "epochs = 200 # Number of epochs to train the model\n",
    "seed = 1234 # Seed for reproducibility\n",
    "data_noise = 0.15 # Noise to add to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Add the channel dimension to the images\n",
    "train_images = np.expand_dims(train_images, axis=-1)\n",
    "test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "# Extract the number of classes in the dataset\n",
    "num_classes = len(np.unique(train_labels))    \n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_train = Tensor(np.array(train_images))\n",
    "y_train = Tensor(np.array(train_labels), dtype=np.int8)\n",
    "X_test = Tensor(np.array(test_images))\n",
    "y_test = Tensor(np.array(test_labels), dtype=np.int8)\n",
    "\n",
    "# Split the training set into training, validation and testing sets\n",
    "X_train, X_test, y_train, y_test = utils.split_data(data=(X_train, y_train), split_pct=train_test_split_pct, shuffle=True)\n",
    "X_train, X_valid, y_train, y_valid = utils.split_data(data=(X_train, y_train), split_pct=train_valid_split_pct, shuffle=True)\n",
    "\n",
    "# Print the dataset information\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print('Training set:', X_train.shape(), y_train.shape())\n",
    "print('Validation set:', X_valid.shape(), y_valid.shape())\n",
    "print('Testing set:', X_test.shape(), y_test.shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def normalize(X: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Normalize the input data by dividing by the maximum value in the training set.\n",
    "    \n",
    "    Parameters:\n",
    "    - X (Tensor): The input data to normalize\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor: The normalized input data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize the input data\n",
    "    return X / 255.0\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = normalize(X_train)\n",
    "X_valid = normalize(X_valid)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the input data\n",
    "def add_noise(X: np.ndarray, noise: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add noise to the input data.\n",
    "    \n",
    "    Parameters:\n",
    "    - X (np.ndarray): The input data to add noise to\n",
    "    - noise (float): The noise to add to the input data\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The input data with noise added\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add noise to the input data\n",
    "    X_noisy = X + noise * np.random.normal(loc=0, scale=1.0, size=X.shape)\n",
    "    \n",
    "    # Clip the values to be between 0 and 1\n",
    "    X_noisy = np.clip(X_noisy, 0., 1.)\n",
    "    \n",
    "    # Return the noisy input data\n",
    "    return X_noisy\n",
    "\n",
    "\n",
    "# Add noise to the input data\n",
    "noise_factor = 0.6\n",
    "X_train_noisy = Tensor(add_noise(X_train.data, noise_factor))\n",
    "X_valid_noisy = Tensor(add_noise(X_valid.data, noise_factor))\n",
    "X_test_noisy = Tensor(add_noise(X_test.data, noise_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the original input data\n",
    "X_train_flatten = X_train.reshape((X_train.shape()[0], -1))\n",
    "X_valid_flatten = X_valid.reshape((X_valid.shape()[0], -1))\n",
    "X_test_flatten = X_test.reshape((X_test.shape()[0], -1))\n",
    "\n",
    "# Flatten the noisy input data\n",
    "X_train_noisy_flatten = X_train_noisy.reshape((X_train.shape()[0], -1))\n",
    "X_valid_noisy_flatten = X_valid_noisy.reshape((X_valid.shape()[0], -1))\n",
    "X_test_noisy_flatten = X_test_noisy.reshape((X_test.shape()[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(originale_samples: list[np.ndarray], noisy_samples: Optional[list[np.ndarray]] = None, reconstructed_samples: Optional[list[np.ndarray]] = None, labels: Optional[list[np.ndarray]] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot the samples in a grid.\n",
    "    \n",
    "    Parameters:\n",
    "    - originale_samples (list[np.ndarray]): The original samples\n",
    "    - noisy_samples (list[np.ndarray]): The noisy samples\n",
    "    - reconstructed_samples (list[np.ndarray]): The reconstructed samples\n",
    "    - labels (list[np.ndarray]): The labels of the samples\n",
    "    \"\"\"\n",
    "        \n",
    "    # Build a list of image types to plot along with their titles.\n",
    "    plot_info = [('Original', originale_samples)]\n",
    "    if noisy_samples is not None:\n",
    "        plot_info.append(('Noisy', noisy_samples))\n",
    "    if reconstructed_samples is not None:\n",
    "        plot_info.append(('Reconstructed', reconstructed_samples))\n",
    "        \n",
    "    # Get the number of rows and samples.\n",
    "    n_rows = len(plot_info)\n",
    "    n_samples = len(originale_samples)\n",
    "    \n",
    "    # Create a grid: columns = samples, rows = image types.\n",
    "    _, axes = plt.subplots(n_rows, n_samples, figsize=(n_samples * 4, n_rows * 4))\n",
    "    \n",
    "    # Ensure axes is a 2D array.\n",
    "    if n_rows == 1:\n",
    "        axes = np.expand_dims(axes, axis=0) \n",
    "    if n_samples == 1:\n",
    "        axes = np.expand_dims(axes, axis=1)\n",
    "    \n",
    "    def add_image(ax: Any, image: np.ndarray, title: str) -> None:\n",
    "        \"\"\"\n",
    "        Add an image to a subplot.\n",
    "        \n",
    "        Parameters:\n",
    "        - ax (Any): The subplot to add the image to\n",
    "        - image (np.ndarray): The image to add to the subplot\n",
    "        - title (str): The title of the subplot\n",
    "        \"\"\"\n",
    "        \n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    # Loop over rows (image types) and columns (samples).\n",
    "    for i, (title_prefix, samples) in enumerate(plot_info):\n",
    "        for j in range(n_samples):\n",
    "            # Append the label text if provided.\n",
    "            add_image(axes[i, j], samples[j], f\"{title_prefix}\\nlabel: {labels[j]}\" if labels is not None else title_prefix)\n",
    "        \n",
    "# Plot the first 10 samples\n",
    "plot_samples(\n",
    "    originale_samples = list(X_test.data[:10]),\n",
    "    noisy_samples = list(X_test_noisy.data[:10]),\n",
    "    labels = list(y_test.data[:10])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimension of the latent space\n",
    "latent_dim = 32\n",
    "\n",
    "# Create the encoder\n",
    "encoder = Model(\n",
    "    name = 'Encoder',\n",
    "    modules = [\n",
    "        layers.Input(shape=X_train_noisy_flatten.shape()[1:]),\n",
    "        layers.Dense(num_units=128, activation=activations.ReLU()),\n",
    "        layers.Dense(num_units=64, activation=activations.ReLU()),\n",
    "        layers.Dense(num_units=latent_dim, activation=activations.ReLU(), name='Latent'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the decoder\n",
    "decoder = Model(\n",
    "    name = 'Decoder',\n",
    "    modules = [\n",
    "        layers.Input(shape=(latent_dim,), name='Latent'),\n",
    "        layers.Dense(num_units=64, activation=activations.ReLU()),\n",
    "        layers.Dense(num_units=128, activation=activations.ReLU()),\n",
    "        layers.Dense(num_units=X_train_noisy_flatten.shape()[1], activation=activations.Sigmoid())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the auto-encoder\n",
    "autoencoder = Model(\n",
    "    name = 'Autoencoder',\n",
    "    modules = [\n",
    "        encoder,\n",
    "        decoder\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optimizers.Adam(learning_rate)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = loss_functions.BinaryCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model with a first batch to initialize the weights\n",
    "# This is not necessary, but it is useful to know the input size\n",
    "autoencoder(X_train_noisy_flatten[:batch_size]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the encoder summary\n",
    "encoder.summary()\n",
    "\n",
    "# Display the decoder summary\n",
    "decoder.summary()\n",
    "\n",
    "# Display the model summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(\n",
    "    X_train = X_train_noisy_flatten,\n",
    "    y_train = X_train_flatten,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = loss_fn,\n",
    "    X_valid = X_valid_noisy_flatten,\n",
    "    y_valid = X_valid_flatten,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = [callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "utils.plot_history(history[\"loss\"], history[\"val_loss\"], \"Training and Validation Loss\", \"Epoch\", \"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoise the test set\n",
    "X_test_denoised = autoencoder(\n",
    "    x = X_test_noisy_flatten,\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "# Reshape the denoised test set\n",
    "X_test_denoised = X_test_denoised.reshape((-1, *X_test.shape()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 10 samples\n",
    "plot_samples(\n",
    "    originale_samples = list(X_test.data[:10]),\n",
    "    noisy_samples = list(X_test_noisy.data[:10]),\n",
    "    reconstructed_samples = list(X_test_denoised.data[:10]),\n",
    "    labels = list(y_test.data[:10])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n random images (noise)\n",
    "n_images = 10\n",
    "random_images = np.random.rand(n_images, *X_train.shape()[1:])\n",
    "random_images = add_noise(random_images, noise_factor)\n",
    "\n",
    "# Convert the random images to a tensor\n",
    "random_images = Tensor(random_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of denoise steps\n",
    "denoise_steps = 10\n",
    "\n",
    "# Create a list to store the denoised images\n",
    "denoised_images = [random_images.reshape((-1, *X_train.shape()[1:]))]\n",
    "\n",
    "# Iterate over the denoise steps\n",
    "for i in range(denoise_steps):\n",
    "    # Denoise the random images using the autoencoder\n",
    "    random_images = autoencoder(random_images.reshape((random_images.shape()[0], -1))) # type: ignore\n",
    "    \n",
    "    # Add the denoised images to the list\n",
    "    denoised_images.append(random_images.reshape((-1, *X_train.shape()[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the denoised images\n",
    "plt.figure(figsize=(2*n_images, 2*len(denoised_images)))\n",
    "for i, images in enumerate(denoised_images):\n",
    "    for j, image in enumerate(images.data):\n",
    "        plt.title(f'Sample {j} ' if i == 0 else f'Denoise step {i}')\n",
    "        plt.subplot(len(denoised_images), n_images, i * n_images + j + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
