{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras import datasets\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the path to the custom library to the system path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import the module from the custom library\n",
    "from src.core.utils import data_analysis, data_processing, context_manager\n",
    "from src import Tensor, Sequential, layers, activations, loss_functions, optimizers, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000 # Number of samples to generate\n",
    "train_test_split_pct = 0.2 # Percentage of samples to use for testing\n",
    "learning_rate = 1e-03 # Learning rate for the optimizer\n",
    "batch_size = 512 # Number of samples to use for each batch\n",
    "epochs = 200 # Number of epochs to train the model\n",
    "seed = 1234 # Seed for reproducibility\n",
    "data_noise = 0.15 # Noise to add to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n",
      "Training set: (48000, 28, 28, 1) (48000,)\n",
      "Testing set: (12000, 28, 28, 1) (12000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Add the channel dimension to the images\n",
    "train_images = np.expand_dims(train_images, axis=-1)\n",
    "test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "# Extract the number of classes in the dataset\n",
    "num_classes = len(np.unique(train_labels))    \n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_train = Tensor(np.array(train_images))\n",
    "y_train = Tensor(np.array(train_labels), dtype=np.int8)\n",
    "X_test = Tensor(np.array(test_images))\n",
    "y_test = Tensor(np.array(test_labels), dtype=np.int8)\n",
    "\n",
    "# Split the training set into training, validation and testing sets\n",
    "X_train, X_test, y_train, y_test = data_processing.split_data(data=(X_train, y_train), split_pct=train_test_split_pct, shuffle=True)\n",
    "\n",
    "# Print the dataset information\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print('Training set:', X_train.shape(), y_train.shape())\n",
    "print('Testing set:', X_test.shape(), y_test.shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def normalize(X: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Normalize the input data by dividing by the maximum value in the training set.\n",
    "    \n",
    "    Parameters:\n",
    "    - X (Tensor): The input data to normalize\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor: The normalized input data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize the input data\n",
    "    return X / 255.0\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the input data\n",
    "X_train_flatten = X_train.reshape((X_train.shape()[0], -1))\n",
    "X_test_flatten = X_test.reshape((X_test.shape()[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_samples(samples: list[np.ndarray], labels: Optional[list[np.ndarray]] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot the samples in a grid.\n",
    "    \n",
    "    Parameters:\n",
    "    - samples (list[np.ndarray]): The samples to plot\n",
    "    - labels (list[np.ndarray]): The labels of the samples\n",
    "    \"\"\"\n",
    "        \n",
    "    # Plot the samples in a grid\n",
    "    _, axes = plt.subplots(1, len(samples), figsize=(20, 5))\n",
    "    \n",
    "    # Ensure axes is always iterable\n",
    "    if len(samples) == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    # Iterate through the samples\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(samples[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "        if isinstance(labels, np.ndarray): \n",
    "            ax.set_title(f'Label: {labels[i]}')\n",
    "        \n",
    "# Plot the first 10 samples\n",
    "plot_samples(list(X_train.data[:10]), list(y_train.data[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimension of the latent space\n",
    "latent_dim = 32\n",
    "\n",
    "# Create the encoder\n",
    "encoder = Sequential(\n",
    "    name = 'Encoder',\n",
    "    modules = [\n",
    "        layers.Input(shape=X_train_flatten.shape()[1:]),\n",
    "        layers.Dense(num_units=128, activation=activations.ReLU()),\n",
    "        layers.Dense(num_units=64, activation=activations.ReLU()),\n",
    "        layers.Dense(num_units=latent_dim, activation=activations.ReLU(), name='Latent'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the decoder\n",
    "decoder = Sequential(\n",
    "    name = 'Decoder',\n",
    "    modules = [\n",
    "        layers.Input(shape=(latent_dim,), name='Latent'),\n",
    "        layers.Dense(num_units=64, activation=activations.ReLU()),\n",
    "        layers.Dense(num_units=128, activation=activations.ReLU()),\n",
    "        layers.Dense(num_units=X_train_flatten.shape()[1], activation=activations.Sigmoid())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the auto-encoder\n",
    "autoencoder = Sequential(\n",
    "    name = 'Autoencoder',\n",
    "    modules = [\n",
    "        encoder,\n",
    "        decoder\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optimizers.Adam(learning_rate)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = loss_functions.BinaryCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model with a first batch to initialize the weights\n",
    "# This is not necessary, but it is useful to know the input size\n",
    "\n",
    "# Disable gradient computation\n",
    "with context_manager.no_grad():\n",
    "    # Set the model in evaluation mode\n",
    "    autoencoder.eval()\n",
    "    \n",
    "    # Call the model with a batch of data to initialize it\n",
    "    autoencoder(X_train_flatten[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "autoencoder.modules[0].encoder\n",
      "\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Module (type)                                          Output Shape        Trainable params #  \n",
      "===============================================================================================\n",
      "encoder.modules[0].input (Input)                       (512, 784)          0                   \n",
      "-----------------------------------------------------------------------------------------------\n",
      "encoder.modules[1].dense (Dense)                       (512, 128)          100480              \n",
      "-----------------------------------------------------------------------------------------------\n",
      "encoder.modules[2].dense (Dense)                       (512, 64)           8256                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "encoder.modules[3].latent (Dense)                      (512, 32)           2080                \n",
      "===============================================================================================\n",
      "Total trainable parameters: 110816\n",
      "-----------------------------------------------------------------------------------------------\n",
      "\n",
      "autoencoder.modules[1].decoder\n",
      "\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Module (type)                                          Output Shape        Trainable params #  \n",
      "===============================================================================================\n",
      "decoder.modules[0].latent (Input)                      (512, 32)           0                   \n",
      "-----------------------------------------------------------------------------------------------\n",
      "decoder.modules[1].dense (Dense)                       (512, 64)           2112                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "decoder.modules[2].dense (Dense)                       (512, 128)          8320                \n",
      "-----------------------------------------------------------------------------------------------\n",
      "decoder.modules[3].dense (Dense)                       (512, 784)          101136              \n",
      "===============================================================================================\n",
      "Total trainable parameters: 111568\n",
      "-----------------------------------------------------------------------------------------------\n",
      "\n",
      "Autoencoder\n",
      "\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Module (type)                                          Output Shape        Trainable params #  \n",
      "===============================================================================================\n",
      "autoencoder.modules[0].encoder (Sequential)            (512, 32)           110816              \n",
      "-----------------------------------------------------------------------------------------------\n",
      "autoencoder.modules[1].decoder (Sequential)            (512, 784)          111568              \n",
      "===============================================================================================\n",
      "Total trainable parameters: 222384\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display the encoder summary\n",
    "encoder.summary()\n",
    "\n",
    "# Display the decoder summary\n",
    "decoder.summary()\n",
    "\n",
    "# Display the model summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 --> loss: 0.3484  | Validation loss: 0.2634                                                   \n",
      "Epoch 2/200 --> loss: 0.2554  | Validation loss: 0.2461                                                   \n",
      "Epoch 3/200 --> loss: 0.2389  | Validation loss: 0.2314                                                   \n",
      "Epoch 4/200 --> loss: 0.2131  | Validation loss: 0.1955                                                   \n",
      "Epoch 5/200 --> loss: 0.1817  | Validation loss: 0.1711                                                   \n",
      "Epoch 6/200 --> loss: 0.1661  | Validation loss: 0.1610                                                   \n",
      "Epoch 7/200 --> loss: 0.1580  | Validation loss: 0.1549                                                   \n",
      "Epoch 8/200 --> loss: 0.1525  | Validation loss: 0.1502                                                   \n",
      "Epoch 9/200 --> loss: 0.1485  | Validation loss: 0.1467                                                   \n",
      "Epoch 10/200 --> loss: 0.1451  | Validation loss: 0.1433                                                   \n",
      "Epoch 11/200 --> loss: 0.1421  | Validation loss: 0.1405                                                   \n",
      "Epoch 12/200 (52.13%) | 189 tensors in memory | 10.93 ms/step --> loss: 0.1359"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_test_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_test_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Auto-Encoder/../src/core/sequential.py:193\u001b[0m, in \u001b[0;36mSequential.fit\u001b[0;34m(self, X_train, y_train, X_valid, y_valid, optimizer, loss_fn, batch_size, gradient_accumulation_steps, epochs, metrics, callbacks)\u001b[0m\n\u001b[1;32m    191\u001b[0m elapsed_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;66;03m# Update the elapsed time\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ms_per_step \u001b[38;5;241m=\u001b[39m elapsed_time \u001b[38;5;241m/\u001b[39m (training_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;66;03m# Compute the milliseconds per step\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m tensors_in_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;66;03m# Compute the number of tensors in memory\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Display epoch progress\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m((((training_step\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39mn_training_steps)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m),\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%) | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensors_in_memory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors in memory | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(ms_per_step,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms/step --> loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Auto-Encoder/../src/core/sequential.py:193\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    191\u001b[0m elapsed_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;66;03m# Update the elapsed time\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ms_per_step \u001b[38;5;241m=\u001b[39m elapsed_time \u001b[38;5;241m/\u001b[39m (training_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;66;03m# Compute the milliseconds per step\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m tensors_in_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m gc\u001b[38;5;241m.\u001b[39mget_objects() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, Tensor)]) \u001b[38;5;66;03m# Compute the number of tensors in memory\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Display epoch progress\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m((((training_step\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39mn_training_steps)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m),\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%) | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensors_in_memory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors in memory | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(ms_per_step,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms/step --> loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_loss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(\n",
    "    X_train = X_train_flatten,\n",
    "    y_train = X_train_flatten,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = loss_fn,\n",
    "    X_valid = X_test_flatten,\n",
    "    y_valid = X_test_flatten,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = [callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "data_analysis.plot_history(history[\"loss\"], history[\"val_loss\"], \"Training and Validation Loss\", \"Epoch\", \"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient computation\n",
    "with context_manager.no_grad():\n",
    "    # Set the model in evaluation mode\n",
    "    encoder.eval()\n",
    "    \n",
    "    # Execute the encoder on the whole training set\n",
    "    latent_train = encoder(\n",
    "        x = X_train_flatten,\n",
    "        batch_size = batch_size,\n",
    "        verbose = True\n",
    "    )\n",
    "\n",
    "# Extract the mean and standard deviation of the latent space\n",
    "latent_mean = np.mean(latent_train.data, axis=0)\n",
    "latent_std = np.std(latent_train.data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_latent(num_samples: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to sample n random vectors from the latent space.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_samples (int): The number of samples to generate\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: The samples generated from the latent space\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample the latent space\n",
    "    return np.random.normal(latent_mean, latent_std, (num_samples, latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one random vector from the latent space\n",
    "samples = sample_latent(num_samples=10)\n",
    "\n",
    "# Convert the samples to a tensor\n",
    "samples = Tensor(samples)\n",
    "\n",
    "# Disable gradient computation\n",
    "with context_manager.no_grad():\n",
    "    # Set the model in evaluation mode\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Decode the sample to generate a new image\n",
    "    decoded_imges = decoder(samples)\n",
    "\n",
    "# Reshape the image to the original shape\n",
    "decoded_imges = decoded_imges.reshape((-1, *X_train.shape()[1:]))\n",
    "\n",
    "# Plot the generated image\n",
    "plot_samples(list(decoded_imges.data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
