{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the path to the custom library to the system path\n",
    "sys.path.append(str(Path().resolve().parent.parent.parent))\n",
    "\n",
    "# Import custom modules\n",
    "from src import Tensor\n",
    "from src.architectures.transformer import Tokenizer, DecoderTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants & Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the tokenizer and model files\n",
    "tokenizer_path = os.path.join(os.getcwd(), 'checkpoints', 'tokenizer_divina_commedia.json')\n",
    "model_path = os.path.join(os.getcwd(), 'checkpoints', 'language_model_divina_commedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 256 # The size of the sequence length (the context window)\n",
    "n_embed = 384 # The size of the token embeddings (the dimensionality of the embeddings)\n",
    "n_attention_heads = 6 # The number of attention heads in the multi-head attention mechanism\n",
    "n_decoder_blocks = 6 # The number of transformer'decoder blocks in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Load the state of the tokenizer\n",
    "tokenizer.load(tokenizer_path)\n",
    "\n",
    "# Extract the vocabulary size\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model from the checkpoint...\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if the model is already trained\n",
    "if os.path.exists(model_path):\n",
    "    # Printa status\n",
    "    print(\"Loading the model from the checkpoint...\")\n",
    "    \n",
    "    # Load the model\n",
    "    language_model = DecoderTransformer.load(model_path)\n",
    "    \n",
    "    # Print status\n",
    "    print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genisse uom mend'acquifallida,\n",
      "ce lo sol, fitto.\n",
      "Quando la la si che in che manobe s'acquifestendo poi dotti,\n",
      "poco so de' erano pavi la ango, per fu' riviva 'l pastmiglio ha cose.\n",
      "commia mi setter del prende,\n",
      "s'l piobasse.\n",
      "Traccia,\n",
      "che Beno di latimai Gio nostra noto,\n",
      "si piangolgira via per mano;\n",
      "chiesa\n",
      "apprende invi allaspetto che remo la tuo umana pecca la riffranno per li volse in sano\n",
      "del si per lo rimonche veder no pone,\n",
      "squi,\n",
      "che; seguiva mordeva fiamma che asser che dentro, intra e in grande\n",
      "che"
     ]
    }
   ],
   "source": [
    "# Generate some text context from the trained model\n",
    "context = Tensor(np.zeros((1, 1), dtype=np.int32))\n",
    "\n",
    "# Iterate over the tokens generated by the transformer\n",
    "for token in language_model.autoregressive_generation(x=context, num_steps=300, stream=True): # type: ignore\n",
    "    # Decode the token\n",
    "    decoded_token = tokenizer.decode([token.data.squeeze().tolist()])\n",
    "\n",
    "    # Print the decoded token\n",
    "    print(decoded_token, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
