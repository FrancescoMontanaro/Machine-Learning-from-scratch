{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the path to the custom library to the system path\n",
    "sys.path.append(str(Path().resolve().parent.parent.parent))\n",
    "\n",
    "# Import custom modules\n",
    "from src import Tensor, loss_functions, optimizers, metrics\n",
    "from src.architectures.transformer import Tokenizer, DecoderTransformer\n",
    "from src.core.utils import data_analysis, data_processing, context_manager\n",
    "from src.architectures.transformer.config import TransformerConfig, DeepSeekTransformerBlockConfig, LatentAttentionConfig, MOEConfig, MLPConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dataset_path = os.path.join(os.getcwd(), 'dataset', 'shakespeare.txt')\n",
    "tokenizer_path = os.path.join(os.getcwd(), 'checkpoints', 'tokenizer_shakespeare.json')\n",
    "model_path = os.path.join(os.getcwd(), 'checkpoints', 'language_model_shakespeare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "dropout = 0.1 # The dropout rate\n",
    "maximum_samples = 19840 # The maximum number of samples to use from the dataset\n",
    "train_test_split_pct = 0.1 # 90% of the data will be used for training, 10% for testing\n",
    "train_valid_split_pct = 0.1 # 90% of the training data will be used for training, 10% for validation\n",
    "batch_size = 8 # The number of samples to use for each batch\n",
    "grad_accumulation_steps = 4 # The number of steps to accumulate gradients before updating the model\n",
    "max_sequence_length = 256 # The size of the sequence length (the context window)\n",
    "learning_rate = 5e-4 # The learning rate for the optimizer\n",
    "weight_decay = 0.01 # The weight decay for the optimizer\n",
    "epochs = 1 # The number of epochs to train the model for\n",
    "n_embed = 384 # The size of the token embeddings (the dimensionality of the embeddings)\n",
    "n_attention_heads = 6 # The number of attention heads in the multi-head attention mechanism\n",
    "n_decoder_blocks = 6 # The number of transformer decoder blocks in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_file(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load a text file from the specified path.\n",
    "    \n",
    "    Parameters:\n",
    "    - path (str): The path to the text file.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The contents of the text file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f'The file \"{path}\" does not exist.')\n",
    "    \n",
    "    # Read the file\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Load the state of the tokenizer\n",
    "tokenizer.load(tokenizer_path)\n",
    "\n",
    "# Extract the vocabulary size\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "text = load_txt_file(dataset_path)\n",
    "\n",
    "# Encode the text using the tokenizer\n",
    "encoded_text = tokenizer.encode(text)\n",
    "\n",
    "# Convert the data to a tensor\n",
    "data = np.array(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(input_data: np.ndarray, seq_length: int, num_samples: int) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Build sequences\n",
    "    \n",
    "    Parameters:\n",
    "    - input_data: np.ndarray, input features\n",
    "    - seq_length: int, length of input sequences\n",
    "    - num_samples: int, number of sequences to generate\n",
    "    \n",
    "    Returns:\n",
    "    - tuple[Tensor, Tensor], input sequences and targets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize lists to hold sequences and targets\n",
    "    X, y = [], []\n",
    "    n = len(input_data)\n",
    "    \n",
    "    # Iterate over the number of samples to create\n",
    "    for _ in range(num_samples):\n",
    "        # Generate a random starting index for the sequence\n",
    "        start_idx = np.random.randint(0, n - seq_length - 1)\n",
    "        \n",
    "        # Extract the input and target sequences\n",
    "        # The input sequence is the current sequence of length seq_length\n",
    "        # The target sequence is the next sequence of length seq_length (shifted by one)\n",
    "        input_sequence = input_data[start_idx : start_idx + seq_length]\n",
    "        target_sequence = input_data[start_idx + 1 : start_idx + seq_length + 1]\n",
    "        \n",
    "        # Aggiungi le sequenze alle liste\n",
    "        X.append(input_sequence)\n",
    "        y.append(target_sequence)\n",
    "    \n",
    "    # Convert the lists to numpy arrays and return as Tensors\n",
    "    return Tensor(np.array(X, dtype=np.int32)), Tensor(np.array(y, dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequences from the encoded text data\n",
    "X, y = build_sequences(data, max_sequence_length, maximum_samples)\n",
    "\n",
    "# Shuffle the data\n",
    "X_shuffled, y_shuffled = data_processing.shuffle_data((X, y))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (16071, 256) (16071, 256)\n",
      "Validation set: (1785, 256) (1785, 256)\n",
      "Testing set: (1984, 256) (1984, 256)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = data_processing.split_data((X_shuffled, y_shuffled), train_test_split_pct, shuffle=True)[0]\n",
    "X_train, X_valid, y_train, y_valid = data_processing.split_data((X_train, y_train), train_valid_split_pct, shuffle=True)[0]\n",
    "\n",
    "# Print the dataset information\n",
    "print('Training set:', X_train.shape, y_train.shape)\n",
    "print('Validation set:', X_valid.shape, y_valid.shape)\n",
    "print('Testing set:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the language model\n",
    "language_model = DecoderTransformer(\n",
    "    name = \"Language Model\",\n",
    "    config = TransformerConfig(\n",
    "        input_dim = vocab_size, # The size of the input vocabulary\n",
    "        embed_dim = n_embed, # The size of the token embeddings\n",
    "        num_blocks = n_decoder_blocks, # The number of transformer blocks\n",
    "        max_sequence_length = max_sequence_length, # The length of the input sequences\n",
    "        return_sequence = True, # Return the full sequence output\n",
    "        input_type = \"discrete\", # Discrete input type for token IDs\n",
    "        positional_encoding_type = \"learned\", # Positional encoding type\n",
    "        block_config = DeepSeekTransformerBlockConfig(\n",
    "            attention_config = LatentAttentionConfig(\n",
    "                num_heads = 4, # Number of attention heads\n",
    "                dropout = 0.1, # Dropout rate\n",
    "                causal = True, # Causal attention\n",
    "                q_lora_rank = 64, # LoRA rank for query\n",
    "                qk_lora_rank = 64, # LoRA rank for query-key\n",
    "                qk_nope_head_dim = 16, # NoPE head dimension for query-key\n",
    "                qk_rope_head_dim = 16, # RoPE head dimension for query-key\n",
    "                kv_lora_rank = 32, # LoRA rank for key-value\n",
    "                v_head_dim = 32, # Head dimension for value\n",
    "                softmax_scale = 1.0 # Softmax scaling factor\n",
    "            ),\n",
    "            ffn_config = MOEConfig(\n",
    "                top_k = 2, # Top-k experts to use\n",
    "                n_groups = 1, # Number of expert groups\n",
    "                top_k_groups = 1, # Number of top-k groups\n",
    "                score_function = \"softmax\", # Scoring function for routing\n",
    "                route_scale = 1.0, # Scaling factor for routing scores\n",
    "                n_routed_experts = 4, # Number of routed experts\n",
    "                n_shared_experts = 2, # Number of shared experts\n",
    "                mlp_config = MLPConfig(\n",
    "                    dropout = 0.1, # Dropout rate\n",
    "                    hidden_dim = 256 # Hidden dimension\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = loss_functions.CrossEntropy(from_sequence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model with a first batch to initialize the weights\n",
    "# This is not necessary, but it is useful to know the input size\n",
    "\n",
    "# Disable gradient computation\n",
    "with context_manager.no_grad():\n",
    "    # Set the model in evaluation mode\n",
    "    language_model.eval()\n",
    "    \n",
    "    # Call the model with a batch of data to initialize it\n",
    "    language_model(x=X_train[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model (DecoderTransformer) [output_shape=(8, 256, 1024), params=12186712]\n",
      "└── language_model.modules (ModuleList) [output_shape=(8, 256, 1024), params=12186712]\n",
      "    └── module_list.0 (Decoder) [output_shape=(8, 256, 1024), params=12186712]\n",
      "        ├── decoder.input_proj (Embedding) [output_shape=(8, 256, 384), params=393216]\n",
      "        ├── decoder.positional_encoding (Embedding) [output_shape=(256, 384), params=98304]\n",
      "        ├── decoder.decoder_blocks (ModuleList) [output_shape=?, params=11300184]\n",
      "        │   ├── module_list.0 (Block) [output_shape=(8, 256, 384), params=1883364]\n",
      "        │   │   ├── block.mla (SelfMultiHeadLatentAttention) [output_shape=(8, 256, 384), params=106592]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wq_a (Dense) [output_shape=(8, 256, 64), params=24576]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.q_norm (RMSNorm) [output_shape=(8, 256, 64), params=64]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wq_b (Dense) [output_shape=(8, 256, 128), params=8192]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wkv_a (Dense) [output_shape=(8, 256, 48), params=18432]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.kv_norm (RMSNorm) [output_shape=(8, 256, 32), params=32]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wkv_b (Dense) [output_shape=(1, 1, 192), params=6144]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.rope (RoPE) [output_shape=(8, 256, 1, 16), params=0]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.attn_dropout (Dropout) [output_shape=(8, 256, 4, 256), params=0]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.proj_dropout (Dropout) [output_shape=(8, 256, 384), params=0]\n",
      "        │   │   │   └── block.mla.wo (Dense) [output_shape=(8, 256, 384), params=49152]\n",
      "        │   │   ├── block.ffn_moe (MoE) [output_shape=(8, 256, 384), params=1776004]\n",
      "        │   │   │   ├── mo_e.gate (Gate) [output_shape=((2048, 2), (2048, 2)), params=1540]\n",
      "        │   │   │   ├── mo_e.routed_experts (ModuleList) [output_shape=?, params=1183232]\n",
      "        │   │   │   │   ├── module_list.0 (MLP) [output_shape=(863, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(863, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.0.w1 (Dense) [output_shape=(863, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.0.w2 (Dense) [output_shape=(863, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.0.w3 (Dense) [output_shape=(863, 256), params=98560]\n",
      "        │   │   │   │   ├── module_list.1 (MLP) [output_shape=(1082, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(1082, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.1.w1 (Dense) [output_shape=(1082, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.1.w2 (Dense) [output_shape=(1082, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.1.w3 (Dense) [output_shape=(1082, 256), params=98560]\n",
      "        │   │   │   │   ├── module_list.2 (MLP) [output_shape=(1108, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(1108, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.2.w1 (Dense) [output_shape=(1108, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.2.w2 (Dense) [output_shape=(1108, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.2.w3 (Dense) [output_shape=(1108, 256), params=98560]\n",
      "        │   │   │   │   └── module_list.3 (MLP) [output_shape=(1043, 384), params=295808]\n",
      "        │   │   │   │       ├── mlp.dropout (Dropout) [output_shape=(1043, 384), params=0]\n",
      "        │   │   │   │       ├── module_list.3.w1 (Dense) [output_shape=(1043, 256), params=98560]\n",
      "        │   │   │   │       ├── module_list.3.w2 (Dense) [output_shape=(1043, 384), params=98688]\n",
      "        │   │   │   │       └── module_list.3.w3 (Dense) [output_shape=(1043, 256), params=98560]\n",
      "        │   │   │   └── mo_e.shared_experts (MLP) [output_shape=(2048, 384), params=591232]\n",
      "        │   │   │       ├── mlp.dropout (Dropout) [output_shape=(2048, 384), params=0]\n",
      "        │   │   │       ├── mo_e.shared_experts.w1 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │   │   │       ├── mo_e.shared_experts.w2 (Dense) [output_shape=(2048, 384), params=196992]\n",
      "        │   │   │       └── mo_e.shared_experts.w3 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │   │   ├── block.attn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        │   │   └── block.ffn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        │   ├── module_list.1 (Block) [output_shape=(8, 256, 384), params=1883364]\n",
      "        │   │   ├── block.mla (SelfMultiHeadLatentAttention) [output_shape=(8, 256, 384), params=106592]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wq_a (Dense) [output_shape=(8, 256, 64), params=24576]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.q_norm (RMSNorm) [output_shape=(8, 256, 64), params=64]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wq_b (Dense) [output_shape=(8, 256, 128), params=8192]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wkv_a (Dense) [output_shape=(8, 256, 48), params=18432]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.kv_norm (RMSNorm) [output_shape=(8, 256, 32), params=32]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wkv_b (Dense) [output_shape=(1, 1, 192), params=6144]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.rope (RoPE) [output_shape=(8, 256, 1, 16), params=0]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.attn_dropout (Dropout) [output_shape=(8, 256, 4, 256), params=0]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.proj_dropout (Dropout) [output_shape=(8, 256, 384), params=0]\n",
      "        │   │   │   └── block.mla.wo (Dense) [output_shape=(8, 256, 384), params=49152]\n",
      "        │   │   ├── block.ffn_moe (MoE) [output_shape=(8, 256, 384), params=1776004]\n",
      "        │   │   │   ├── mo_e.gate (Gate) [output_shape=((2048, 2), (2048, 2)), params=1540]\n",
      "        │   │   │   ├── mo_e.routed_experts (ModuleList) [output_shape=?, params=1183232]\n",
      "        │   │   │   │   ├── module_list.0 (MLP) [output_shape=(1126, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(1126, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.0.w1 (Dense) [output_shape=(1126, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.0.w2 (Dense) [output_shape=(1126, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.0.w3 (Dense) [output_shape=(1126, 256), params=98560]\n",
      "        │   │   │   │   ├── module_list.1 (MLP) [output_shape=(956, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(956, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.1.w1 (Dense) [output_shape=(956, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.1.w2 (Dense) [output_shape=(956, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.1.w3 (Dense) [output_shape=(956, 256), params=98560]\n",
      "        │   │   │   │   ├── module_list.2 (MLP) [output_shape=(1134, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(1134, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.2.w1 (Dense) [output_shape=(1134, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.2.w2 (Dense) [output_shape=(1134, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.2.w3 (Dense) [output_shape=(1134, 256), params=98560]\n",
      "        │   │   │   │   └── module_list.3 (MLP) [output_shape=(880, 384), params=295808]\n",
      "        │   │   │   │       ├── mlp.dropout (Dropout) [output_shape=(880, 384), params=0]\n",
      "        │   │   │   │       ├── module_list.3.w1 (Dense) [output_shape=(880, 256), params=98560]\n",
      "        │   │   │   │       ├── module_list.3.w2 (Dense) [output_shape=(880, 384), params=98688]\n",
      "        │   │   │   │       └── module_list.3.w3 (Dense) [output_shape=(880, 256), params=98560]\n",
      "        │   │   │   └── mo_e.shared_experts (MLP) [output_shape=(2048, 384), params=591232]\n",
      "        │   │   │       ├── mlp.dropout (Dropout) [output_shape=(2048, 384), params=0]\n",
      "        │   │   │       ├── mo_e.shared_experts.w1 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │   │   │       ├── mo_e.shared_experts.w2 (Dense) [output_shape=(2048, 384), params=196992]\n",
      "        │   │   │       └── mo_e.shared_experts.w3 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │   │   ├── block.attn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        │   │   └── block.ffn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        │   ├── module_list.2 (Block) [output_shape=(8, 256, 384), params=1883364]\n",
      "        │   │   ├── block.mla (SelfMultiHeadLatentAttention) [output_shape=(8, 256, 384), params=106592]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wq_a (Dense) [output_shape=(8, 256, 64), params=24576]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.q_norm (RMSNorm) [output_shape=(8, 256, 64), params=64]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wq_b (Dense) [output_shape=(8, 256, 128), params=8192]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wkv_a (Dense) [output_shape=(8, 256, 48), params=18432]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.kv_norm (RMSNorm) [output_shape=(8, 256, 32), params=32]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wkv_b (Dense) [output_shape=(1, 1, 192), params=6144]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.rope (RoPE) [output_shape=(8, 256, 1, 16), params=0]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.attn_dropout (Dropout) [output_shape=(8, 256, 4, 256), params=0]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.proj_dropout (Dropout) [output_shape=(8, 256, 384), params=0]\n",
      "        │   │   │   └── block.mla.wo (Dense) [output_shape=(8, 256, 384), params=49152]\n",
      "        │   │   ├── block.ffn_moe (MoE) [output_shape=(8, 256, 384), params=1776004]\n",
      "        │   │   │   ├── mo_e.gate (Gate) [output_shape=((2048, 2), (2048, 2)), params=1540]\n",
      "        │   │   │   ├── mo_e.routed_experts (ModuleList) [output_shape=?, params=1183232]\n",
      "        │   │   │   │   ├── module_list.0 (MLP) [output_shape=(922, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(922, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.0.w1 (Dense) [output_shape=(922, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.0.w2 (Dense) [output_shape=(922, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.0.w3 (Dense) [output_shape=(922, 256), params=98560]\n",
      "        │   │   │   │   ├── module_list.1 (MLP) [output_shape=(836, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(836, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.1.w1 (Dense) [output_shape=(836, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.1.w2 (Dense) [output_shape=(836, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.1.w3 (Dense) [output_shape=(836, 256), params=98560]\n",
      "        │   │   │   │   ├── module_list.2 (MLP) [output_shape=(1247, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(1247, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.2.w1 (Dense) [output_shape=(1247, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.2.w2 (Dense) [output_shape=(1247, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.2.w3 (Dense) [output_shape=(1247, 256), params=98560]\n",
      "        │   │   │   │   └── module_list.3 (MLP) [output_shape=(1091, 384), params=295808]\n",
      "        │   │   │   │       ├── mlp.dropout (Dropout) [output_shape=(1091, 384), params=0]\n",
      "        │   │   │   │       ├── module_list.3.w1 (Dense) [output_shape=(1091, 256), params=98560]\n",
      "        │   │   │   │       ├── module_list.3.w2 (Dense) [output_shape=(1091, 384), params=98688]\n",
      "        │   │   │   │       └── module_list.3.w3 (Dense) [output_shape=(1091, 256), params=98560]\n",
      "        │   │   │   └── mo_e.shared_experts (MLP) [output_shape=(2048, 384), params=591232]\n",
      "        │   │   │       ├── mlp.dropout (Dropout) [output_shape=(2048, 384), params=0]\n",
      "        │   │   │       ├── mo_e.shared_experts.w1 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │   │   │       ├── mo_e.shared_experts.w2 (Dense) [output_shape=(2048, 384), params=196992]\n",
      "        │   │   │       └── mo_e.shared_experts.w3 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │   │   ├── block.attn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        │   │   └── block.ffn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        │   ├── module_list.3 (Block) [output_shape=(8, 256, 384), params=1883364]\n",
      "        │   │   ├── block.mla (SelfMultiHeadLatentAttention) [output_shape=(8, 256, 384), params=106592]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wq_a (Dense) [output_shape=(8, 256, 64), params=24576]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.q_norm (RMSNorm) [output_shape=(8, 256, 64), params=64]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wq_b (Dense) [output_shape=(8, 256, 128), params=8192]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wkv_a (Dense) [output_shape=(8, 256, 48), params=18432]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.kv_norm (RMSNorm) [output_shape=(8, 256, 32), params=32]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wkv_b (Dense) [output_shape=(1, 1, 192), params=6144]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.rope (RoPE) [output_shape=(8, 256, 1, 16), params=0]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.attn_dropout (Dropout) [output_shape=(8, 256, 4, 256), params=0]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.proj_dropout (Dropout) [output_shape=(8, 256, 384), params=0]\n",
      "        │   │   │   └── block.mla.wo (Dense) [output_shape=(8, 256, 384), params=49152]\n",
      "        │   │   ├── block.ffn_moe (MoE) [output_shape=(8, 256, 384), params=1776004]\n",
      "        │   │   │   ├── mo_e.gate (Gate) [output_shape=((2048, 2), (2048, 2)), params=1540]\n",
      "        │   │   │   ├── mo_e.routed_experts (ModuleList) [output_shape=?, params=1183232]\n",
      "        │   │   │   │   ├── module_list.0 (MLP) [output_shape=(1059, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(1059, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.0.w1 (Dense) [output_shape=(1059, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.0.w2 (Dense) [output_shape=(1059, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.0.w3 (Dense) [output_shape=(1059, 256), params=98560]\n",
      "        │   │   │   │   ├── module_list.1 (MLP) [output_shape=(1034, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(1034, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.1.w1 (Dense) [output_shape=(1034, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.1.w2 (Dense) [output_shape=(1034, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.1.w3 (Dense) [output_shape=(1034, 256), params=98560]\n",
      "        │   │   │   │   ├── module_list.2 (MLP) [output_shape=(968, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(968, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.2.w1 (Dense) [output_shape=(968, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.2.w2 (Dense) [output_shape=(968, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.2.w3 (Dense) [output_shape=(968, 256), params=98560]\n",
      "        │   │   │   │   └── module_list.3 (MLP) [output_shape=(1035, 384), params=295808]\n",
      "        │   │   │   │       ├── mlp.dropout (Dropout) [output_shape=(1035, 384), params=0]\n",
      "        │   │   │   │       ├── module_list.3.w1 (Dense) [output_shape=(1035, 256), params=98560]\n",
      "        │   │   │   │       ├── module_list.3.w2 (Dense) [output_shape=(1035, 384), params=98688]\n",
      "        │   │   │   │       └── module_list.3.w3 (Dense) [output_shape=(1035, 256), params=98560]\n",
      "        │   │   │   └── mo_e.shared_experts (MLP) [output_shape=(2048, 384), params=591232]\n",
      "        │   │   │       ├── mlp.dropout (Dropout) [output_shape=(2048, 384), params=0]\n",
      "        │   │   │       ├── mo_e.shared_experts.w1 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │   │   │       ├── mo_e.shared_experts.w2 (Dense) [output_shape=(2048, 384), params=196992]\n",
      "        │   │   │       └── mo_e.shared_experts.w3 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │   │   ├── block.attn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        │   │   └── block.ffn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        │   ├── module_list.4 (Block) [output_shape=(8, 256, 384), params=1883364]\n",
      "        │   │   ├── block.mla (SelfMultiHeadLatentAttention) [output_shape=(8, 256, 384), params=106592]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wq_a (Dense) [output_shape=(8, 256, 64), params=24576]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.q_norm (RMSNorm) [output_shape=(8, 256, 64), params=64]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wq_b (Dense) [output_shape=(8, 256, 128), params=8192]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wkv_a (Dense) [output_shape=(8, 256, 48), params=18432]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.kv_norm (RMSNorm) [output_shape=(8, 256, 32), params=32]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.wkv_b (Dense) [output_shape=(1, 1, 192), params=6144]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.rope (RoPE) [output_shape=(8, 256, 1, 16), params=0]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.attn_dropout (Dropout) [output_shape=(8, 256, 4, 256), params=0]\n",
      "        │   │   │   ├── self_multi_head_latent_attention.proj_dropout (Dropout) [output_shape=(8, 256, 384), params=0]\n",
      "        │   │   │   └── block.mla.wo (Dense) [output_shape=(8, 256, 384), params=49152]\n",
      "        │   │   ├── block.ffn_moe (MoE) [output_shape=(8, 256, 384), params=1776004]\n",
      "        │   │   │   ├── mo_e.gate (Gate) [output_shape=((2048, 2), (2048, 2)), params=1540]\n",
      "        │   │   │   ├── mo_e.routed_experts (ModuleList) [output_shape=?, params=1183232]\n",
      "        │   │   │   │   ├── module_list.0 (MLP) [output_shape=(836, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(836, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.0.w1 (Dense) [output_shape=(836, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.0.w2 (Dense) [output_shape=(836, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.0.w3 (Dense) [output_shape=(836, 256), params=98560]\n",
      "        │   │   │   │   ├── module_list.1 (MLP) [output_shape=(1298, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(1298, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.1.w1 (Dense) [output_shape=(1298, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.1.w2 (Dense) [output_shape=(1298, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.1.w3 (Dense) [output_shape=(1298, 256), params=98560]\n",
      "        │   │   │   │   ├── module_list.2 (MLP) [output_shape=(879, 384), params=295808]\n",
      "        │   │   │   │   │   ├── mlp.dropout (Dropout) [output_shape=(879, 384), params=0]\n",
      "        │   │   │   │   │   ├── module_list.2.w1 (Dense) [output_shape=(879, 256), params=98560]\n",
      "        │   │   │   │   │   ├── module_list.2.w2 (Dense) [output_shape=(879, 384), params=98688]\n",
      "        │   │   │   │   │   └── module_list.2.w3 (Dense) [output_shape=(879, 256), params=98560]\n",
      "        │   │   │   │   └── module_list.3 (MLP) [output_shape=(1083, 384), params=295808]\n",
      "        │   │   │   │       ├── mlp.dropout (Dropout) [output_shape=(1083, 384), params=0]\n",
      "        │   │   │   │       ├── module_list.3.w1 (Dense) [output_shape=(1083, 256), params=98560]\n",
      "        │   │   │   │       ├── module_list.3.w2 (Dense) [output_shape=(1083, 384), params=98688]\n",
      "        │   │   │   │       └── module_list.3.w3 (Dense) [output_shape=(1083, 256), params=98560]\n",
      "        │   │   │   └── mo_e.shared_experts (MLP) [output_shape=(2048, 384), params=591232]\n",
      "        │   │   │       ├── mlp.dropout (Dropout) [output_shape=(2048, 384), params=0]\n",
      "        │   │   │       ├── mo_e.shared_experts.w1 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │   │   │       ├── mo_e.shared_experts.w2 (Dense) [output_shape=(2048, 384), params=196992]\n",
      "        │   │   │       └── mo_e.shared_experts.w3 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │   │   ├── block.attn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        │   │   └── block.ffn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        │   └── module_list.5 (Block) [output_shape=(8, 256, 384), params=1883364]\n",
      "        │       ├── block.mla (SelfMultiHeadLatentAttention) [output_shape=(8, 256, 384), params=106592]\n",
      "        │       │   ├── self_multi_head_latent_attention.wq_a (Dense) [output_shape=(8, 256, 64), params=24576]\n",
      "        │       │   ├── self_multi_head_latent_attention.q_norm (RMSNorm) [output_shape=(8, 256, 64), params=64]\n",
      "        │       │   ├── self_multi_head_latent_attention.wq_b (Dense) [output_shape=(8, 256, 128), params=8192]\n",
      "        │       │   ├── self_multi_head_latent_attention.wkv_a (Dense) [output_shape=(8, 256, 48), params=18432]\n",
      "        │       │   ├── self_multi_head_latent_attention.kv_norm (RMSNorm) [output_shape=(8, 256, 32), params=32]\n",
      "        │       │   ├── self_multi_head_latent_attention.wkv_b (Dense) [output_shape=(1, 1, 192), params=6144]\n",
      "        │       │   ├── self_multi_head_latent_attention.rope (RoPE) [output_shape=(8, 256, 1, 16), params=0]\n",
      "        │       │   ├── self_multi_head_latent_attention.attn_dropout (Dropout) [output_shape=(8, 256, 4, 256), params=0]\n",
      "        │       │   ├── self_multi_head_latent_attention.proj_dropout (Dropout) [output_shape=(8, 256, 384), params=0]\n",
      "        │       │   └── block.mla.wo (Dense) [output_shape=(8, 256, 384), params=49152]\n",
      "        │       ├── block.ffn_moe (MoE) [output_shape=(8, 256, 384), params=1776004]\n",
      "        │       │   ├── mo_e.gate (Gate) [output_shape=((2048, 2), (2048, 2)), params=1540]\n",
      "        │       │   ├── mo_e.routed_experts (ModuleList) [output_shape=?, params=1183232]\n",
      "        │       │   │   ├── module_list.0 (MLP) [output_shape=(1053, 384), params=295808]\n",
      "        │       │   │   │   ├── mlp.dropout (Dropout) [output_shape=(1053, 384), params=0]\n",
      "        │       │   │   │   ├── module_list.0.w1 (Dense) [output_shape=(1053, 256), params=98560]\n",
      "        │       │   │   │   ├── module_list.0.w2 (Dense) [output_shape=(1053, 384), params=98688]\n",
      "        │       │   │   │   └── module_list.0.w3 (Dense) [output_shape=(1053, 256), params=98560]\n",
      "        │       │   │   ├── module_list.1 (MLP) [output_shape=(1013, 384), params=295808]\n",
      "        │       │   │   │   ├── mlp.dropout (Dropout) [output_shape=(1013, 384), params=0]\n",
      "        │       │   │   │   ├── module_list.1.w1 (Dense) [output_shape=(1013, 256), params=98560]\n",
      "        │       │   │   │   ├── module_list.1.w2 (Dense) [output_shape=(1013, 384), params=98688]\n",
      "        │       │   │   │   └── module_list.1.w3 (Dense) [output_shape=(1013, 256), params=98560]\n",
      "        │       │   │   ├── module_list.2 (MLP) [output_shape=(862, 384), params=295808]\n",
      "        │       │   │   │   ├── mlp.dropout (Dropout) [output_shape=(862, 384), params=0]\n",
      "        │       │   │   │   ├── module_list.2.w1 (Dense) [output_shape=(862, 256), params=98560]\n",
      "        │       │   │   │   ├── module_list.2.w2 (Dense) [output_shape=(862, 384), params=98688]\n",
      "        │       │   │   │   └── module_list.2.w3 (Dense) [output_shape=(862, 256), params=98560]\n",
      "        │       │   │   └── module_list.3 (MLP) [output_shape=(1168, 384), params=295808]\n",
      "        │       │   │       ├── mlp.dropout (Dropout) [output_shape=(1168, 384), params=0]\n",
      "        │       │   │       ├── module_list.3.w1 (Dense) [output_shape=(1168, 256), params=98560]\n",
      "        │       │   │       ├── module_list.3.w2 (Dense) [output_shape=(1168, 384), params=98688]\n",
      "        │       │   │       └── module_list.3.w3 (Dense) [output_shape=(1168, 256), params=98560]\n",
      "        │       │   └── mo_e.shared_experts (MLP) [output_shape=(2048, 384), params=591232]\n",
      "        │       │       ├── mlp.dropout (Dropout) [output_shape=(2048, 384), params=0]\n",
      "        │       │       ├── mo_e.shared_experts.w1 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │       │       ├── mo_e.shared_experts.w2 (Dense) [output_shape=(2048, 384), params=196992]\n",
      "        │       │       └── mo_e.shared_experts.w3 (Dense) [output_shape=(2048, 512), params=197120]\n",
      "        │       ├── block.attn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        │       └── block.ffn_norm (RMSNorm) [output_shape=(8, 256, 384), params=384]\n",
      "        ├── decoder.layer_norm (LayerNormalization) [output_shape=(8, 256, 384), params=768]\n",
      "        └── decoder.output_layer (Dense) [output_shape=(8, 256, 1024), params=394240]\n"
     ]
    }
   ],
   "source": [
    "# Display the model summary in tree format.\n",
    "# This is useful since the whole model is composed of submodules,\n",
    "# therefore, the model summary will be displayed recursively\n",
    "language_model.summary(recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 (10.25%) | 1480 tensors in memory | 1170.81 ms/step --> loss: 3.9539"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m history = \u001b[43mlanguage_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_accumulation_steps\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/architectures/sequential/sequential.py:260\u001b[39m, in \u001b[36mSequential.fit\u001b[39m\u001b[34m(self, X_train, y_train, optimizer, loss_fn, batch_size, gradient_accumulation_steps, epochs, metrics, callbacks, shuffle_between_epochs, X_valid, y_valid, forward_params)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    258\u001b[39m     \u001b[38;5;66;03m# Single tensor case: pass as positional arguments\u001b[39;00m\n\u001b[32m    259\u001b[39m     forward_args = \u001b[38;5;28mlist\u001b[39m(x_training_batch) + \u001b[38;5;28mlist\u001b[39m(forward_params.values())\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     training_batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_num_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_training_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# Compute the loss of the model\u001b[39;00m\n\u001b[32m    263\u001b[39m training_loss = loss_fn(y_training_batch, training_batch_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/module.py:243\u001b[39m, in \u001b[36mModule.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mself\u001b[39m._lazy_init(*args, **kwargs)\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[32m    241\u001b[39m \n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[32m    246\u001b[39m \n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m._output_shape = out.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/architectures/sequential/sequential.py:507\u001b[39m, in \u001b[36mSequential._forward\u001b[39m\u001b[34m(self, batch_size, verbose, _input_keys, _num_inputs, *args, **kwargs)\u001b[39m\n\u001b[32m    504\u001b[39m     batch_args = args\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# Forward pass: Pass positional arguments to modules.forward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m batch_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodules\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m outputs.append(batch_out)\n\u001b[32m    510\u001b[39m \u001b[38;5;66;03m# Measure elapsed time for the batch processing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/module.py:243\u001b[39m, in \u001b[36mModule.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mself\u001b[39m._lazy_init(*args, **kwargs)\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[32m    241\u001b[39m \n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[32m    246\u001b[39m \n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m._output_shape = out.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/modules_list.py:176\u001b[39m, in \u001b[36mModuleList._forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m._modules.values()):\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m    175\u001b[39m         \u001b[38;5;66;03m# First module gets all original inputs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m         current_output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    178\u001b[39m         \u001b[38;5;66;03m# Subsequent modules get single tensor output + other params\u001b[39;00m\n\u001b[32m    179\u001b[39m         module_args = [current_output] + other_params\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/module.py:243\u001b[39m, in \u001b[36mModule.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mself\u001b[39m._lazy_init(*args, **kwargs)\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[32m    241\u001b[39m \n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[32m    246\u001b[39m \n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m._output_shape = out.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/architectures/transformer/modules/decoder.py:109\u001b[39m, in \u001b[36mDecoder._forward\u001b[39m\u001b[34m(self, x, encoder_output, start_pos)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Apply the decoder blocks\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder_blocks:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     embeddings = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# (B, S, E) -> (B, S, E)\u001b[39;49;00m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Apply the output layer to get the logits\u001b[39;00m\n\u001b[32m    116\u001b[39m out = \u001b[38;5;28mself\u001b[39m.output_layer(\u001b[38;5;28mself\u001b[39m.layer_norm(embeddings)) \u001b[38;5;66;03m# (B, S, E) -> (B, S, O)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/module.py:90\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33;03mMethod to call the forward method of the module\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     86\u001b[39m \u001b[33;03m- Tensor: Output of the module after the forward pass\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Call the forward method\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/module.py:243\u001b[39m, in \u001b[36mModule.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mself\u001b[39m._lazy_init(*args, **kwargs)\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[32m    241\u001b[39m \n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[32m    246\u001b[39m \n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m._output_shape = out.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/architectures/transformer/modules/block.py:146\u001b[39m, in \u001b[36mBlock._forward_deepseek_transformer_block\u001b[39m\u001b[34m(self, x, start_pos, *args, **kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03mForward pass of the vanilla transformer block.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m \u001b[33;03m- ValueError: If cross-attention is used but encoder_output is not provided.\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Dimensions are:\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# - B: batch size\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# - S: sequence length\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# - E: embedding size (embedding dimension of the original data)\u001b[39;00m\n\u001b[32m    144\u001b[39m \n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Apply the self multi-head latent attention with skip connections\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmla\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, S, E) + (B, S, E) -> (B, S, E)\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# Apply the MoE feed-forward network with skip connections\u001b[39;00m\n\u001b[32m    149\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.ffn_moe(\u001b[38;5;28mself\u001b[39m.ffn_norm(x)) \u001b[38;5;66;03m# (B, S, E) + (B, S, E) -> (B, S, E)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/module.py:90\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[33;03mMethod to call the forward method of the module\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     86\u001b[39m \u001b[33;03m- Tensor: Output of the module after the forward pass\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Call the forward method\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/module.py:243\u001b[39m, in \u001b[36mModule.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mself\u001b[39m._lazy_init(*args, **kwargs)\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[32m    241\u001b[39m \n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[32m    246\u001b[39m \n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m._output_shape = out.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/architectures/transformer/modules/self_latent_attention.py:161\u001b[39m, in \u001b[36mSelfMultiHeadLatentAttention._forward\u001b[39m\u001b[34m(self, x, start_pos, *args, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m     q_nope_proj = einsum(\u001b[33m\"\u001b[39m\u001b[33mbshd,hdc->bshc\u001b[39m\u001b[33m\"\u001b[39m, q_nope, wkv_b[:, :\u001b[38;5;28mself\u001b[39m.qk_nope_head_dim])\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# Compute the attention scores using the full sequence\u001b[39;00m\n\u001b[32m    160\u001b[39m     scores = (\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m         \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbshc,btc->bsht\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_nope_proj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_normalized\u001b[49m\u001b[43m)\u001b[49m +\n\u001b[32m    162\u001b[39m         einsum(\u001b[33m\"\u001b[39m\u001b[33mbshr,bthr->bsht\u001b[39m\u001b[33m\"\u001b[39m, q_rope, k_pe.expand(-\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_heads, -\u001b[32m1\u001b[39m))\n\u001b[32m    163\u001b[39m     ) * \u001b[38;5;28mself\u001b[39m.softmax_scale\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# Use caching for faster inference\u001b[39;00m\n\u001b[32m    167\u001b[39m     end_pos = start_pos + S\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/utils/data_processing.py:319\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(subscripts, *operands)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03mMethod to perform the einsum operation on the given operands\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \u001b[33;03m- Tensor: Result of the einsum operation\u001b[39;00m\n\u001b[32m    316\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# Compute and return the result of the einsum operation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubscripts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/tensor.py:2050\u001b[39m, in \u001b[36mTensor.einsum\u001b[39m\u001b[34m(subscripts, *operands)\u001b[39m\n\u001b[32m   2047\u001b[39m     out_buffer += grad\n\u001b[32m   2049\u001b[39m \u001b[38;5;66;03m# Return the tensor operation with the specified forward and backward functions\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2050\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_nary_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackward_fn\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_cls\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/functional/base.py:171\u001b[39m, in \u001b[36mtensor_nary_op\u001b[39m\u001b[34m(tensors, forward_fn, backward_fn, tensor_cls)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[33;03mFunction to create a tensor operation with automatic differentiation.\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m \u001b[33;03m- Tensor: Output tensor with the computed gradients.\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Call the forward function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m out_data, tape_idx = \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Check if the gradient is required\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _NO_GRAD \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(t.requires_grad \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors):\n\u001b[32m    175\u001b[39m     \u001b[38;5;66;03m# If no gradients are required, return the output tensor without backward\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/tensor.py:2036\u001b[39m, in \u001b[36mTensor.einsum.<locals>.forward\u001b[39m\u001b[34m(tensors_data)\u001b[39m\n\u001b[32m   2034\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(tensors_data: List[np.ndarray]) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m   2035\u001b[39m     \u001b[38;5;66;03m# Perform the einsum operation\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2036\u001b[39m     out = \u001b[43meinsum_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubscripts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtensors_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2038\u001b[39m     \u001b[38;5;66;03m# Return the result\u001b[39;00m\n\u001b[32m   2039\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out, -\u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/Deep Learning/src/core/functional/kernel/einsum.py:48\u001b[39m, in \u001b[36meinsum_forward\u001b[39m\u001b[34m(subscripts, *operands)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03mComputes the einsum operation on the given operands.\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m \u001b[33;03m- np.ndarray: Result of the einsum operation\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Perform the einsum operation using NumPy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubscripts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Progetti/Machine-Learning-from-scratch/venv/lib/python3.13/site-packages/numpy/_core/einsumfunc.py:1423\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(out, optimize, *operands, **kwargs)\u001b[39m\n\u001b[32m   1421\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[32m   1422\u001b[39m         kwargs[\u001b[33m'\u001b[39m\u001b[33mout\u001b[39m\u001b[33m'\u001b[39m] = out\n\u001b[32m-> \u001b[39m\u001b[32m1423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mc_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[32m   1427\u001b[39m valid_einsum_kwargs = [\u001b[33m'\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33morder\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcasting\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = language_model.fit(\n",
    "    train_data = {'x': X_train},\n",
    "    valid_data = {'x': X_valid},\n",
    "    y_train = y_train,\n",
    "    y_valid = y_valid,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = loss_fn,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    gradient_accumulation_steps = grad_accumulation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model \n",
    "language_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation loss\n",
    "data_analysis.plot_history(\n",
    "    train_loss = language_model.history[\"loss\"], \n",
    "    valid_loss = language_model.history[\"val_loss\"], \n",
    "    title = \"Training and Validation Loss\", \n",
    "    xlabel = \"Eval iter\",\n",
    "    ylabel = \"Loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 32/32 - 539.77 ms/step"
     ]
    }
   ],
   "source": [
    "# Disable gradient computation\n",
    "with context_manager.no_grad():\n",
    "    # Set the model in evaluation mode\n",
    "    language_model.eval()\n",
    "    \n",
    "    # Compute the predictions\n",
    "    predictions = language_model(X_test[:256], batch_size=batch_size, verbose=True)\n",
    "\n",
    "# Apply the argmax function to the predictions\n",
    "predictions = Tensor(np.argmax(predictions.data, axis=-1), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.19\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy\n",
    "accuracy = metrics.accuracy(y_test[:256, -1], predictions[:, -1])\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy.data:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " such conuch con hnover hnoverlflfENKINGENKING.\n",
      ".\n",
      "AA:\n",
      ":\n",
      "NNemempp  WoWo b beaiseais my my to tot t stst hav have fe fgood tgood t theon theonce hce hciciouldould. . ielielverver'd 'd notirnotir,\n",
      "T,\n",
      "Tinging\n",
      "U\n",
      "URLORLOan man myselfenyselfen.\n",
      ";.\n",
      ";s,\n",
      "s,\n",
      "SSealterealterleletyty; ; y,y, n nowerower oun ounOlaOla.\n",
      "\n",
      "C.\n",
      "\n",
      "CAAongenongen is is to toyy his his k k'd con'd conerrerrOF pOF pursesurses w we thee theendend you  you ecaecaipilipilToToopopON:\n",
      "ON:\n",
      "Th.\n",
      "\n",
      "Th.\n",
      "\n",
      "WWorioriideideFor sirst For sirst andand p pustessustess in the in the c cliyliy,!\n",
      ",!\n",
      "A wA wbe be  our ourbidbid  vcvce heae heavenvenessessA gA gentlementlematat' hea' heaven;\n",
      "ven;\n",
      "MPMP==ouldould, , is sis sINAINA way way  liellielerers ss sodeode wow wow haand haand t tratrat him,\n",
      " him,\n",
      "iv tiv too her her"
     ]
    }
   ],
   "source": [
    "# Generate some text context from the trained model\n",
    "context = Tensor(np.zeros((1, 1), dtype=np.int32))\n",
    "\n",
    "# Iterate over the tokens generated by the transformer\n",
    "for token in language_model.autoregressive_generation(x=context, num_steps=200, stream=True, do_sample=True):\n",
    "    # Decode the token - use .item() to extract the scalar value\n",
    "    decoded_token = tokenizer.decode([int(token.data.item())])\n",
    "\n",
    "    # Print the decoded token\n",
    "    print(decoded_token, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
