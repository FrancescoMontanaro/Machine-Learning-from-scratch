{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add the path to the custom library to the system path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import the module from the custom library\n",
    "from src import utils\n",
    "from src import Model\n",
    "from src import layers\n",
    "from src import callbacks\n",
    "from src import optimizers\n",
    "from src import activations\n",
    "from src import loss_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000 # Number of samples to generate\n",
    "train_test_split_pct = 0.2 # Percentage of samples to use for testing\n",
    "train_valid_split = 0.2 # Percentage of samples to use for validation\n",
    "learning_rate = 2e-04 # Learning rate for the optimizer\n",
    "batch_size = 512 # Number of samples to use for each batch\n",
    "epochs = 300 # Number of epochs to train the model\n",
    "seed = 1234 # Seed for reproducibility\n",
    "data_noise = 0.15 # Noise to add to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data() # type: ignore\n",
    "\n",
    "# Add a channel dimension to the images\n",
    "X_train = np.expand_dims(X_train, axis=-1) # type: ignore\n",
    "X_test = np.expand_dims(X_test, axis=-1) # type: ignore\n",
    "\n",
    "# Extract the number of classes in the dataset\n",
    "num_classes = len(np.unique(y_train)) # type: ignore\n",
    "\n",
    "# Split the training set into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=train_valid_split, random_state=seed) # type: ignore\n",
    "\n",
    "# Print the dataset information\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print('Training set:', X_train.shape, y_train.shape)\n",
    "print('Validation set:', X_valid.shape, y_valid.shape)\n",
    "print('Testing set:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def normalize(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize the input data by dividing by the maximum value in the training set.\n",
    "    \n",
    "    Parameters:\n",
    "    - X (np.ndarray): The input data to normalize\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The normalized input data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize the input data\n",
    "    return X / 255.0\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = normalize(X_train)\n",
    "X_valid = normalize(X_valid)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(samples: list[np.ndarray], labels: Optional[list[np.ndarray]] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plot the samples in a grid.\n",
    "    \n",
    "    Parameters:\n",
    "    - samples (list[np.ndarray]): The samples to plot\n",
    "    - labels (list[np.ndarray]): The labels of the samples\n",
    "    \"\"\"\n",
    "        \n",
    "    # Plot the samples in a grid\n",
    "    _, axes = plt.subplots(1, len(samples), figsize=(20, 5))\n",
    "    \n",
    "    # Ensure axes is always iterable\n",
    "    if len(samples) == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    # Iterate through the samples\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(samples[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "        if isinstance(labels, np.ndarray): \n",
    "            ax.set_title(f'Label: {labels[i]}')\n",
    "        \n",
    "# Plot the first 10 samples\n",
    "plot_samples(list(X_train[:10]), y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimension of the latent space\n",
    "latent_dim = 32\n",
    "\n",
    "# Create the encoder\n",
    "encoder = Model(\n",
    "    name = 'Encoder',\n",
    "    modules = [\n",
    "        layers.Input(shape=X_train.shape[1:]),\n",
    "        layers.Conv2D(num_filters=32, kernel_size=(3, 3), activation=activations.ReLU(), padding='same'),\n",
    "        layers.MaxPool2D(size=(2, 2), padding='same'),\n",
    "        layers.Conv2D(num_filters=64, kernel_size=(3, 3), activation=activations.ReLU(), padding='same'),\n",
    "        layers.MaxPool2D(size=(2, 2), padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_units=latent_dim, activation=activations.ReLU(), name='Latent'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the decoder\n",
    "decoder = Model(\n",
    "    name = 'Decoder',\n",
    "    modules = [\n",
    "        layers.Input(shape=(latent_dim,), name='Latent'),\n",
    "        layers.Dense(num_units=7 * 7 * 64, activation=activations.ReLU()),\n",
    "        layers.Reshape(shape=(7, 7, 64)),\n",
    "        layers.Conv2D(num_filters=64, kernel_size=(3, 3), activation=activations.ReLU(), padding='same'),\n",
    "        layers.UpSampling2D(size=(2, 2)),\n",
    "        layers.Conv2D(num_filters=32, kernel_size=(3, 3), activation=activations.ReLU(), padding='same'),\n",
    "        layers.UpSampling2D(size=(2, 2)),\n",
    "        layers.Conv2D(num_filters=1, kernel_size=(3, 3), activation=activations.Sigmoid(), padding='same'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the auto-encoder\n",
    "autoencoder = Model(\n",
    "    name = 'Autoencoder',\n",
    "    modules = [\n",
    "        encoder,\n",
    "        decoder\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optimizers.Adam(learning_rate)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = loss_functions.BinaryCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model with a first batch to initialize the weights\n",
    "# This is not necessary, but it is useful to know the input size\n",
    "autoencoder(X_train[:batch_size]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the encoder summary\n",
    "encoder.summary()\n",
    "\n",
    "# Display the decoder summary\n",
    "decoder.summary()\n",
    "\n",
    "# Display the model summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(\n",
    "    X_train = X_train,\n",
    "    y_train = X_train,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = loss_fn,\n",
    "    X_valid = X_valid,\n",
    "    y_valid = X_valid,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = [callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "utils.plot_history(autoencoder.history[\"loss\"], autoencoder.history[\"val_loss\"], \"Training and Validation Loss\", \"Epoch\", \"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the statistics of the latent space\n",
    "latent_train = encoder(X_train)\n",
    "latent_mean = np.mean(latent_train, axis=0)\n",
    "latent_std = np.std(latent_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_latent(num_samples: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to sample n random vectors from the latent space.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_samples (int): The number of samples to generate\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: The samples generated from the latent space\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample the latent space\n",
    "    return np.random.normal(latent_mean, latent_std, (num_samples, latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one random vector from the latent space\n",
    "samples = sample_latent(num_samples=10)\n",
    "\n",
    "# Decode the sample to generate a new image\n",
    "decoded_imges = decoder(samples)\n",
    "\n",
    "# Reshape the image to the original shape\n",
    "decoded_imges = decoded_imges.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Plot the generated image\n",
    "plot_samples(list(decoded_imges))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
