{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Add the path to the custom library to the system path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import custom modules\n",
    "from src import Tensor\n",
    "from src.core.utils import data_analysis, context_manager\n",
    "from src.architectures.transformer import Tokenizer, Transformer, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dataset_path = os.path.join(os.getcwd(), 'dataset', 'divina_commedia.txt')\n",
    "tokenizer_path = os.path.join(os.getcwd(), 'checkpoints', 'tokenizer.json')\n",
    "model_path = os.path.join(os.getcwd(), 'checkpoints', 'language_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "dropout = 0.2 # The dropout rate\n",
    "train_val_split = 0.9 # 90% of the data will be used for training, 10% for validation\n",
    "batch_size = 32 # The number of samples to use for each batch\n",
    "grad_accumulation_steps = 1 # The number of steps to accumulate gradients before updating the model\n",
    "sequence_length = 256 # The size of the sequence length (the context window)\n",
    "learning_rate = 1e-3 # The learning rate for the optimizer\n",
    "training_steps = 500 # The number of steps to train the model for\n",
    "n_embed = 384 # The size of the token embeddings (the dimensionality of the embeddings)\n",
    "eval_iters = 1 # The number of iterations to evaluate the model\n",
    "n_attention_heads = 6 # The number of attention heads in the multi-head attention mechanism\n",
    "n_decoder_blocks = 6 # The number of transformer'decoder blocks in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_file(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load a text file from the specified path.\n",
    "    \n",
    "    Parameters:\n",
    "    - path (str): The path to the text file.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The contents of the text file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f'The file \"{path}\" does not exist.')\n",
    "    \n",
    "    # Read the file\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Load the state of the tokenizer\n",
    "tokenizer.load(tokenizer_path)\n",
    "\n",
    "# Extract the vocabulary size\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "text = load_txt_file(dataset_path)\n",
    "\n",
    "# Encode the text using the tokenizer\n",
    "encoded_text = tokenizer.encode(text)\n",
    "\n",
    "# Convert the data to a tensor\n",
    "data = Tensor(np.array(encoded_text), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the data loader\n",
    "data_loader = DataLoader(\n",
    "    data = data, \n",
    "    train_val_split = train_val_split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the language model\n",
    "language_model = Transformer(\n",
    "    name = \"Language Model\",\n",
    "    vocab_size = vocab_size,\n",
    "    n_embed = n_embed,\n",
    "    n_attention_heads = n_attention_heads,\n",
    "    sequence_length = sequence_length,\n",
    "    n_decoder_blocks = n_decoder_blocks,\n",
    "    dropout = dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model with a first batch to initialize the weights\n",
    "# This is not necessary, but it is useful to know the input size\n",
    "\n",
    "# Disable gradient computation\n",
    "with context_manager.no_grad():\n",
    "    # Set the model in evaluation mode\n",
    "    language_model.eval()\n",
    "    \n",
    "    # Get a batch of data\n",
    "    x, _ = data_loader.get_batch(\n",
    "        batch_size = batch_size,\n",
    "        sequence_length = sequence_length\n",
    "    )\n",
    "    \n",
    "    # Call the model with a batch of data to initialize it\n",
    "    language_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model (Transformer) [output_shape=(32, 256, 1024), params=11526400]\n",
      "└── language_model.decoder (Decoder) [output_shape=(32, 256, 1024), params=11526400]\n",
      "    ├── decoder.embedding (Embedding) [output_shape=(32, 256, 384), params=393216]\n",
      "    ├── decoder.positional_embedding (Embedding) [output_shape=(256, 384), params=98304]\n",
      "    ├── decoder.decoder_blocks (ModuleList) [output_shape=?, params=10639872]\n",
      "    │   ├── module_list.0 (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │   │   ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   │   ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │   │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │   │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │   │   │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   │   ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   │   └── module_list.0.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   │       ├── multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "    │   │       │   ├── module_list.0 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.1 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.2 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.3 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.4 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   └── module_list.5 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │       ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │       └── module_list.0.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    │   ├── module_list.1 (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │   │   ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   │   ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │   │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │   │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │   │   │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   │   ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   │   └── module_list.1.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   │       ├── multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "    │   │       │   ├── module_list.0 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.1 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.2 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.3 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.4 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   └── module_list.5 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │       ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │       └── module_list.1.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    │   ├── module_list.2 (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │   │   ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   │   ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │   │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │   │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │   │   │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   │   ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   │   └── module_list.2.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   │       ├── multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "    │   │       │   ├── module_list.0 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.1 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.2 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.3 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.4 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   └── module_list.5 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │       ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │       └── module_list.2.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    │   ├── module_list.3 (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │   │   ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   │   ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │   │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │   │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │   │   │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   │   ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   │   └── module_list.3.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   │       ├── multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "    │   │       │   ├── module_list.0 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.1 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.2 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.3 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.4 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   └── module_list.5 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │       ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │       └── module_list.3.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    │   ├── module_list.4 (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │   │   ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   │   ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │   │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │   │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │   │   │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   │   ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   │   └── module_list.4.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   │       ├── multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "    │   │       │   ├── module_list.0 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.1 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.2 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.3 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   ├── module_list.4 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       │   └── module_list.5 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │   │       │       ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │   │       │       └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │   │       ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │       └── module_list.4.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    │   └── module_list.5 (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │       ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │       ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │       │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │       │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │       │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │       ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │       └── module_list.5.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │           ├── multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "    │           │   ├── module_list.0 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │           │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │           │   ├── module_list.1 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │           │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │           │   ├── module_list.2 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │           │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │           │   ├── module_list.3 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │           │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │           │   ├── module_list.4 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │           │   │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │   │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │           │   └── module_list.5 (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │           │       ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │       ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │       ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │           │       └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │           ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │           └── module_list.5.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    ├── decoder.layer_norm (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    └── decoder.output_layer (Dense) [output_shape=(32, 256, 1024), params=394240]\n"
     ]
    }
   ],
   "source": [
    "# Display the model summary in tree format.\n",
    "# This is useful since the whole model is composed of submodules,\n",
    "# therefore, the model summary will be displayed recursively\n",
    "language_model.summary(recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/500 | 217 tensors in memory | 4337.34 ms/step - Train Loss: 7.1238 | Validation loss: 7.1094\n",
      "Step 2/500 | 222 tensors in memory | 4506.93 ms/step - Train Loss: 6.2630 | Validation loss: 6.2795\n",
      "Step 3/500 | 222 tensors in memory | 4443.92 ms/step - Train Loss: 5.5033 | Validation loss: 5.4793\n",
      "Step 4/500 | 222 tensors in memory | 4436.19 ms/step - Train Loss: 5.0870 | Validation loss: 5.0780\n",
      "Step 5/500 | 222 tensors in memory | 4402.69 ms/step - Train Loss: 4.7548 | Validation loss: 4.7245\n",
      "Step 6/500 | 222 tensors in memory | 4378.61 ms/step - Train Loss: 4.5381 | Validation loss: 4.5354\n",
      "Step 7/500 | 222 tensors in memory | 4349.15 ms/step - Train Loss: 4.4761 | Validation loss: 4.4641\n",
      "Step 8/500 | 222 tensors in memory | 4329.11 ms/step - Train Loss: 4.3880 | Validation loss: 4.4300\n",
      "Step 9/500 | 222 tensors in memory | 4363.27 ms/step - Train Loss: 4.3219 | Validation loss: 4.3191\n",
      "Step 10/500 | 222 tensors in memory | 4373.28 ms/step - Train Loss: 4.3475 | Validation loss: 4.2852\n",
      "Step 11/500 | 222 tensors in memory | 4388.06 ms/step - Train Loss: 4.2897 | Validation loss: 4.3052\n",
      "Step 12/500 | 222 tensors in memory | 4365.12 ms/step - Train Loss: 4.3084 | Validation loss: 4.2687\n",
      "Step 13/500 | 222 tensors in memory | 4339.66 ms/step - Train Loss: 4.2684 | Validation loss: 4.2846\n",
      "Step 14/500 | 222 tensors in memory | 4311.22 ms/step - Train Loss: 4.2373 | Validation loss: 4.2643\n",
      "Step 15/500 | 222 tensors in memory | 4299.74 ms/step - Train Loss: 4.2235 | Validation loss: 4.2534\n",
      "Step 16/500 | 222 tensors in memory | 4297.47 ms/step - Train Loss: 4.1932 | Validation loss: 4.1930\n",
      "Step 17/500 | 222 tensors in memory | 4280.35 ms/step - Train Loss: 4.1866 | Validation loss: 4.1810\n",
      "Step 18/500 | 222 tensors in memory | 4263.05 ms/step - Train Loss: 4.1917 | Validation loss: 4.1605\n",
      "Step 19/500 | 222 tensors in memory | 4272.86 ms/step - Train Loss: 4.1442 | Validation loss: 4.1428\n",
      "Step 20/500 | 222 tensors in memory | 4288.24 ms/step - Train Loss: 4.1084 | Validation loss: 4.1371\n",
      "Step 21/500 | 222 tensors in memory | 4293.02 ms/step - Train Loss: 4.0351 | Validation loss: 4.1027\n",
      "Step 22/500 | 222 tensors in memory | 4279.68 ms/step - Train Loss: 4.0712 | Validation loss: 4.0705\n",
      "Step 23/500 | 222 tensors in memory | 4259.22 ms/step - Train Loss: 4.0459 | Validation loss: 4.0602\n",
      "Step 24/500 | 222 tensors in memory | 4254.60 ms/step - Train Loss: 4.0362 | Validation loss: 4.0389\n",
      "Step 25/500 | 222 tensors in memory | 4250.49 ms/step - Train Loss: 3.9834 | Validation loss: 4.0622\n",
      "Step 26/500 | 222 tensors in memory | 4277.06 ms/step - Train Loss: 3.9988 | Validation loss: 4.0328\n",
      "Step 27/500 | 222 tensors in memory | 4282.59 ms/step - Train Loss: 4.0028 | Validation loss: 3.9837\n",
      "Step 28/500 | 222 tensors in memory | 4284.07 ms/step - Train Loss: 3.9823 | Validation loss: 3.9877\n",
      "Step 29/500 | 222 tensors in memory | 4279.32 ms/step - Train Loss: 3.9556 | Validation loss: 3.9833\n",
      "Step 30/500 | 222 tensors in memory | 4278.23 ms/step - Train Loss: 3.9487 | Validation loss: 3.9540\n",
      "Step 31/500 | 222 tensors in memory | 4269.36 ms/step - Train Loss: 3.9330 | Validation loss: 3.9647\n",
      "Step 32/500 | 222 tensors in memory | 4262.70 ms/step - Train Loss: 3.9577 | Validation loss: 3.9071\n",
      "Step 33/500 | 222 tensors in memory | 4245.49 ms/step - Train Loss: 3.8817 | Validation loss: 3.9198\n",
      "Step 34/500 | 222 tensors in memory | 4249.38 ms/step - Train Loss: 3.8979 | Validation loss: 3.8865\n",
      "Step 35/500 | 222 tensors in memory | 4241.48 ms/step - Train Loss: 3.8792 | Validation loss: 3.8669\n",
      "Step 36/500 | 222 tensors in memory | 4231.02 ms/step - Train Loss: 3.8539 | Validation loss: 3.8689\n",
      "Step 37/500 | 222 tensors in memory | 4223.75 ms/step - Train Loss: 3.8277 | Validation loss: 3.8743\n",
      "Step 38/500 | 222 tensors in memory | 4215.63 ms/step - Train Loss: 3.8646 | Validation loss: 3.8560\n",
      "Step 39/500 | 222 tensors in memory | 4207.96 ms/step - Train Loss: 3.8266 | Validation loss: 3.8851\n",
      "Step 40/500 | 222 tensors in memory | 4196.84 ms/step - Train Loss: 3.8561 | Validation loss: 3.8617\n",
      "Step 41/500 | 222 tensors in memory | 4191.94 ms/step - Train Loss: 3.8442 | Validation loss: 3.8383\n",
      "Step 42/500 | 222 tensors in memory | 4183.44 ms/step - Train Loss: 3.8538 | Validation loss: 3.7876\n",
      "Step 43/500 | 222 tensors in memory | 4169.22 ms/step - Train Loss: 3.8531 | Validation loss: 3.8447\n",
      "Step 44/500 | 222 tensors in memory | 4162.49 ms/step - Train Loss: 3.8135 | Validation loss: 3.7856\n",
      "Step 45/500 | 222 tensors in memory | 4158.27 ms/step - Train Loss: 3.8440 | Validation loss: 3.8107\n",
      "Step 46/500 | 222 tensors in memory | 4154.01 ms/step - Train Loss: 3.7986 | Validation loss: 3.7851\n",
      "Step 47/500 | 222 tensors in memory | 4144.28 ms/step - Train Loss: 3.7745 | Validation loss: 3.7844\n",
      "Step 48/500 | 222 tensors in memory | 4147.03 ms/step - Train Loss: 3.7859 | Validation loss: 3.8115\n",
      "Step 49/500 | 222 tensors in memory | 4147.50 ms/step - Train Loss: 3.7760 | Validation loss: 3.7507\n",
      "Step 50/500 | 222 tensors in memory | 4147.66 ms/step - Train Loss: 3.7811 | Validation loss: 3.8001\n",
      "Step 51/500 | 222 tensors in memory | 4148.89 ms/step - Train Loss: 3.7620 | Validation loss: 3.7774\n",
      "Step 52/500 | 222 tensors in memory | 4149.75 ms/step - Train Loss: 3.7745 | Validation loss: 3.7665\n",
      "Step 53/500 | 222 tensors in memory | 4166.76 ms/step - Train Loss: 3.7303 | Validation loss: 3.7705\n",
      "Step 54/500 | 222 tensors in memory | 4175.17 ms/step - Train Loss: 3.7371 | Validation loss: 3.7609\n",
      "Step 55/500 | 222 tensors in memory | 4176.12 ms/step - Train Loss: 3.6971 | Validation loss: 3.7506\n",
      "Step 56/500 | 222 tensors in memory | 4172.14 ms/step - Train Loss: 3.7593 | Validation loss: 3.7201\n",
      "Step 57/500 | 222 tensors in memory | 4168.98 ms/step - Train Loss: 3.7313 | Validation loss: 3.7151\n",
      "Step 58/500 | 222 tensors in memory | 4171.10 ms/step - Train Loss: 3.7136 | Validation loss: 3.7191\n",
      "Step 59/500 | 222 tensors in memory | 4177.99 ms/step - Train Loss: 3.7031 | Validation loss: 3.6893\n",
      "Step 60/500 | 222 tensors in memory | 4182.55 ms/step - Train Loss: 3.7221 | Validation loss: 3.6897\n",
      "Step 61/500 | 222 tensors in memory | 4193.22 ms/step - Train Loss: 3.6702 | Validation loss: 3.6616\n",
      "Step 62/500 | 222 tensors in memory | 4191.00 ms/step - Train Loss: 3.6864 | Validation loss: 3.6730\n",
      "Step 63/500 | 222 tensors in memory | 4190.99 ms/step - Train Loss: 3.7460 | Validation loss: 3.6843\n",
      "Step 64/500 | 222 tensors in memory | 4199.28 ms/step - Train Loss: 3.6138 | Validation loss: 3.6522\n",
      "Step 65/500 | 222 tensors in memory | 4206.95 ms/step - Train Loss: 3.6443 | Validation loss: 3.6453\n",
      "Step 66/500 | 222 tensors in memory | 4203.97 ms/step - Train Loss: 3.5863 | Validation loss: 3.6377\n",
      "Step 67/500 | 222 tensors in memory | 4211.01 ms/step - Train Loss: 3.6454 | Validation loss: 3.6378\n",
      "Step 68/500 | 222 tensors in memory | 4205.83 ms/step - Train Loss: 3.6084 | Validation loss: 3.6193\n",
      "Step 69/500 | 222 tensors in memory | 4203.65 ms/step - Train Loss: 3.5748 | Validation loss: 3.6131\n",
      "Step 70/500 | 222 tensors in memory | 4201.00 ms/step - Train Loss: 3.5902 | Validation loss: 3.5802\n",
      "Step 71/500 | 222 tensors in memory | 4196.35 ms/step - Train Loss: 3.6341 | Validation loss: 3.6296\n",
      "Step 72/500 | 222 tensors in memory | 4194.88 ms/step - Train Loss: 3.5170 | Validation loss: 3.5811\n",
      "Step 73/500 | 222 tensors in memory | 4190.47 ms/step - Train Loss: 3.6017 | Validation loss: 3.5667\n",
      "Step 74/500 | 222 tensors in memory | 4187.43 ms/step - Train Loss: 3.5846 | Validation loss: 3.5785\n",
      "Step 75/500 | 222 tensors in memory | 4180.98 ms/step - Train Loss: 3.5569 | Validation loss: 3.5850\n",
      "Step 76/500 | 222 tensors in memory | 4179.28 ms/step - Train Loss: 3.5344 | Validation loss: 3.5595\n",
      "Step 77/500 | 222 tensors in memory | 4178.83 ms/step - Train Loss: 3.5762 | Validation loss: 3.5706\n",
      "Step 78/500 | 222 tensors in memory | 4174.39 ms/step - Train Loss: 3.5258 | Validation loss: 3.5998\n",
      "Step 79/500 | 222 tensors in memory | 4171.42 ms/step - Train Loss: 3.5517 | Validation loss: 3.5368\n",
      "Step 80/500 | 222 tensors in memory | 4168.48 ms/step - Train Loss: 3.5650 | Validation loss: 3.5124\n",
      "Step 81/500 | 222 tensors in memory | 4166.56 ms/step - Train Loss: 3.4958 | Validation loss: 3.5425\n",
      "Step 82/500 | 222 tensors in memory | 4162.74 ms/step - Train Loss: 3.5109 | Validation loss: 3.5217\n",
      "Step 83/500 | 222 tensors in memory | 4164.84 ms/step - Train Loss: 3.5054 | Validation loss: 3.5090\n",
      "Step 84/500 | 222 tensors in memory | 4162.71 ms/step - Train Loss: 3.4750 | Validation loss: 3.5554\n",
      "Step 85/500 | 222 tensors in memory | 4159.96 ms/step - Train Loss: 3.5231 | Validation loss: 3.5100\n",
      "Step 86/500 | 222 tensors in memory | 4155.96 ms/step - Train Loss: 3.4445 | Validation loss: 3.4620\n",
      "Step 87/500 | 222 tensors in memory | 4151.05 ms/step - Train Loss: 3.4817 | Validation loss: 3.4501\n",
      "Step 88/500 | 222 tensors in memory | 4151.28 ms/step - Train Loss: 3.4775 | Validation loss: 3.4955\n",
      "Step 89/500 | 222 tensors in memory | 4146.28 ms/step - Train Loss: 3.4379 | Validation loss: 3.4565\n",
      "Step 90/500 | 222 tensors in memory | 4142.40 ms/step - Train Loss: 3.4512 | Validation loss: 3.4732\n",
      "Step 91/500 | 222 tensors in memory | 4137.80 ms/step - Train Loss: 3.3884 | Validation loss: 3.4874\n",
      "Step 92/500 | 222 tensors in memory | 4134.07 ms/step - Train Loss: 3.4357 | Validation loss: 3.4138\n",
      "Step 93/500 | 222 tensors in memory | 4128.76 ms/step - Train Loss: 3.4174 | Validation loss: 3.4360\n",
      "Step 94/500 | 222 tensors in memory | 4122.69 ms/step - Train Loss: 3.4006 | Validation loss: 3.4620\n",
      "Step 95/500 | 222 tensors in memory | 4130.96 ms/step - Train Loss: 3.3986 | Validation loss: 3.4165\n",
      "Step 96/500 | 222 tensors in memory | 4133.87 ms/step - Train Loss: 3.4090 | Validation loss: 3.4114\n",
      "Step 97/500 | 222 tensors in memory | 4135.38 ms/step - Train Loss: 3.3986 | Validation loss: 3.4348\n",
      "Step 98/500 | 222 tensors in memory | 4134.54 ms/step - Train Loss: 3.4118 | Validation loss: 3.4141\n",
      "Step 99/500 | 222 tensors in memory | 4134.17 ms/step - Train Loss: 3.4095 | Validation loss: 3.3967\n",
      "Step 100/500 | 222 tensors in memory | 4133.15 ms/step - Train Loss: 3.3680 | Validation loss: 3.3937\n",
      "Step 101/500 | 222 tensors in memory | 4133.54 ms/step - Train Loss: 3.3731 | Validation loss: 3.3805\n",
      "Step 102/500 | 222 tensors in memory | 4131.99 ms/step - Train Loss: 3.3566 | Validation loss: 3.4012\n",
      "Step 103/500 | 222 tensors in memory | 4127.56 ms/step - Train Loss: 3.3787 | Validation loss: 3.3389\n",
      "Step 104/500 | 222 tensors in memory | 4125.00 ms/step - Train Loss: 3.3750 | Validation loss: 3.3762\n",
      "Step 105/500 | 222 tensors in memory | 4121.99 ms/step - Train Loss: 3.3684 | Validation loss: 3.3648\n",
      "Step 106/500 | 222 tensors in memory | 4118.64 ms/step - Train Loss: 3.3701 | Validation loss: 3.4002\n",
      "Step 107/500 | 222 tensors in memory | 4114.87 ms/step - Train Loss: 3.3322 | Validation loss: 3.3887\n",
      "Step 108/500 | 222 tensors in memory | 4110.19 ms/step - Train Loss: 3.3430 | Validation loss: 3.4156\n",
      "Step 109/500 | 222 tensors in memory | 4105.44 ms/step - Train Loss: 3.3319 | Validation loss: 3.3582\n",
      "Step 110/500 | 222 tensors in memory | 4101.75 ms/step - Train Loss: 3.3813 | Validation loss: 3.3642\n",
      "Step 111/500 | 222 tensors in memory | 4098.38 ms/step - Train Loss: 3.3675 | Validation loss: 3.3908\n",
      "Step 112/500 | 222 tensors in memory | 4094.01 ms/step - Train Loss: 3.3183 | Validation loss: 3.3427\n",
      "Step 113/500 | 222 tensors in memory | 4093.62 ms/step - Train Loss: 3.3042 | Validation loss: 3.3596\n",
      "Step 114/500 | 222 tensors in memory | 4091.79 ms/step - Train Loss: 3.3159 | Validation loss: 3.3341\n",
      "Step 115/500 | 222 tensors in memory | 4088.94 ms/step - Train Loss: 3.3176 | Validation loss: 3.2984\n",
      "Step 116/500 | 222 tensors in memory | 4085.56 ms/step - Train Loss: 3.3057 | Validation loss: 3.3333\n",
      "Step 117/500 | 222 tensors in memory | 4085.87 ms/step - Train Loss: 3.2996 | Validation loss: 3.3076\n",
      "Step 118/500 | 222 tensors in memory | 4083.09 ms/step - Train Loss: 3.2920 | Validation loss: 3.3040\n",
      "Step 119/500 | 222 tensors in memory | 4083.05 ms/step - Train Loss: 3.2566 | Validation loss: 3.3111\n",
      "Step 120/500 | 222 tensors in memory | 4081.96 ms/step - Train Loss: 3.2827 | Validation loss: 3.2963\n",
      "Step 121/500 | 222 tensors in memory | 4080.65 ms/step - Train Loss: 3.2925 | Validation loss: 3.2438\n",
      "Step 122/500 | 222 tensors in memory | 4083.22 ms/step - Train Loss: 3.2961 | Validation loss: 3.3263\n",
      "Step 123/500 | 222 tensors in memory | 4080.89 ms/step - Train Loss: 3.2796 | Validation loss: 3.2861\n",
      "Step 124/500 | 222 tensors in memory | 4079.45 ms/step - Train Loss: 3.2931 | Validation loss: 3.3066\n",
      "Step 125/500 | 222 tensors in memory | 4077.94 ms/step - Train Loss: 3.2444 | Validation loss: 3.2993\n",
      "Step 126/500 | 222 tensors in memory | 4077.70 ms/step - Train Loss: 3.2195 | Validation loss: 3.2105\n",
      "Step 127/500 | 222 tensors in memory | 4075.43 ms/step - Train Loss: 3.2329 | Validation loss: 3.2864\n",
      "Step 128/500 | 222 tensors in memory | 4073.88 ms/step - Train Loss: 3.2688 | Validation loss: 3.2603\n",
      "Step 129/500 | 222 tensors in memory | 4070.53 ms/step - Train Loss: 3.2467 | Validation loss: 3.2304\n",
      "Step 130/500 | 222 tensors in memory | 4067.29 ms/step - Train Loss: 3.2141 | Validation loss: 3.2511\n",
      "Step 131/500 | 222 tensors in memory | 4069.47 ms/step - Train Loss: 3.2417 | Validation loss: 3.2702\n",
      "Step 132/500 | 222 tensors in memory | 4068.76 ms/step - Train Loss: 3.2210 | Validation loss: 3.2309\n",
      "Step 133/500 | 222 tensors in memory | 4067.90 ms/step - Train Loss: 3.2094 | Validation loss: 3.2473\n",
      "Step 134/500 | 222 tensors in memory | 4066.54 ms/step - Train Loss: 3.2422 | Validation loss: 3.2163\n",
      "Step 135/500 | 222 tensors in memory | 4064.99 ms/step - Train Loss: 3.1924 | Validation loss: 3.2192\n",
      "Step 136/500 | 222 tensors in memory | 4064.77 ms/step - Train Loss: 3.2477 | Validation loss: 3.2544\n",
      "Step 137/500 | 222 tensors in memory | 4066.02 ms/step - Train Loss: 3.2065 | Validation loss: 3.2497\n",
      "Step 138/500 | 222 tensors in memory | 4064.59 ms/step - Train Loss: 3.1848 | Validation loss: 3.2423\n",
      "Step 139/500 | 222 tensors in memory | 4063.23 ms/step - Train Loss: 3.1879 | Validation loss: 3.2227\n",
      "Step 140/500 | 222 tensors in memory | 4061.47 ms/step - Train Loss: 3.1652 | Validation loss: 3.1978\n",
      "Step 141/500 | 222 tensors in memory | 4060.31 ms/step - Train Loss: 3.1529 | Validation loss: 3.2152\n",
      "Step 142/500 | 222 tensors in memory | 4057.88 ms/step - Train Loss: 3.1984 | Validation loss: 3.1812\n",
      "Step 143/500 | 222 tensors in memory | 4057.00 ms/step - Train Loss: 3.1897 | Validation loss: 3.2356\n",
      "Step 144/500 | 222 tensors in memory | 4055.41 ms/step - Train Loss: 3.1853 | Validation loss: 3.2079\n",
      "Step 145/500 | 222 tensors in memory | 4053.39 ms/step - Train Loss: 3.1666 | Validation loss: 3.1564\n",
      "Step 146/500 | 222 tensors in memory | 4051.94 ms/step - Train Loss: 3.1970 | Validation loss: 3.1967\n",
      "Step 147/500 | 222 tensors in memory | 4050.26 ms/step - Train Loss: 3.2236 | Validation loss: 3.2197\n",
      "Step 148/500 | 222 tensors in memory | 4047.95 ms/step - Train Loss: 3.1541 | Validation loss: 3.2362\n",
      "Step 149/500 | 222 tensors in memory | 4045.70 ms/step - Train Loss: 3.1485 | Validation loss: 3.2089\n",
      "Step 150/500 | 222 tensors in memory | 4045.89 ms/step - Train Loss: 3.1389 | Validation loss: 3.2180\n",
      "Step 151/500 | 222 tensors in memory | 4044.67 ms/step - Train Loss: 3.2145 | Validation loss: 3.1906\n",
      "Step 152/500 | 222 tensors in memory | 4043.70 ms/step - Train Loss: 3.1780 | Validation loss: 3.1957\n",
      "Step 153/500 | 222 tensors in memory | 4042.20 ms/step - Train Loss: 3.1712 | Validation loss: 3.1694\n",
      "Step 154/500 | 222 tensors in memory | 4040.95 ms/step - Train Loss: 3.1705 | Validation loss: 3.1975\n",
      "Step 155/500 | 222 tensors in memory | 4040.23 ms/step - Train Loss: 3.2054 | Validation loss: 3.1914\n",
      "Step 156/500 | 222 tensors in memory | 4038.26 ms/step - Train Loss: 3.1430 | Validation loss: 3.2071\n",
      "Step 157/500 | 222 tensors in memory | 4036.29 ms/step - Train Loss: 3.1720 | Validation loss: 3.2304\n",
      "Step 158/500 | 222 tensors in memory | 4034.51 ms/step - Train Loss: 3.1663 | Validation loss: 3.1742\n",
      "Step 159/500 | 222 tensors in memory | 4031.83 ms/step - Train Loss: 3.1279 | Validation loss: 3.1901\n",
      "Step 160/500 | 222 tensors in memory | 4032.67 ms/step - Train Loss: 3.1685 | Validation loss: 3.1879\n",
      "Step 161/500 | 222 tensors in memory | 4031.91 ms/step - Train Loss: 3.1598 | Validation loss: 3.1412\n",
      "Step 162/500 | 222 tensors in memory | 4031.00 ms/step - Train Loss: 3.1849 | Validation loss: 3.1783\n",
      "Step 163/500 | 222 tensors in memory | 4031.24 ms/step - Train Loss: 3.1526 | Validation loss: 3.1832\n",
      "Step 164/500 | 222 tensors in memory | 4030.55 ms/step - Train Loss: 3.1351 | Validation loss: 3.1604\n",
      "Step 165/500 | 222 tensors in memory | 4028.83 ms/step - Train Loss: 3.1389 | Validation loss: 3.1763\n",
      "Step 166/500 | 222 tensors in memory | 4025.98 ms/step - Train Loss: 3.1848 | Validation loss: 3.1787\n",
      "Step 167/500 | 222 tensors in memory | 4023.61 ms/step - Train Loss: 3.1521 | Validation loss: 3.1823\n",
      "Step 168/500 | 222 tensors in memory | 4021.53 ms/step - Train Loss: 3.1674 | Validation loss: 3.1578\n",
      "Step 169/500 | 222 tensors in memory | 4019.06 ms/step - Train Loss: 3.0809 | Validation loss: 3.1865\n",
      "Step 170/500 | 222 tensors in memory | 4016.71 ms/step - Train Loss: 3.0922 | Validation loss: 3.1644\n",
      "Step 171/500 | 222 tensors in memory | 4016.59 ms/step - Train Loss: 3.1163 | Validation loss: 3.1618\n",
      "Step 172/500 | 222 tensors in memory | 4014.99 ms/step - Train Loss: 3.1112 | Validation loss: 3.1670\n",
      "Step 173/500 | 222 tensors in memory | 4013.74 ms/step - Train Loss: 3.1603 | Validation loss: 3.1215\n",
      "Step 174/500 | 222 tensors in memory | 4013.22 ms/step - Train Loss: 3.1808 | Validation loss: 3.0974\n",
      "Step 175/500 | 222 tensors in memory | 4011.43 ms/step - Train Loss: 3.0972 | Validation loss: 3.1310\n",
      "Step 176/500 | 222 tensors in memory | 4009.31 ms/step - Train Loss: 3.1470 | Validation loss: 3.1616\n",
      "Step 177/500 | 222 tensors in memory | 4007.27 ms/step - Train Loss: 3.1321 | Validation loss: 3.1622\n",
      "Step 178/500 | 222 tensors in memory | 4004.24 ms/step - Train Loss: 3.1099 | Validation loss: 3.1581\n",
      "Step 179/500 | 222 tensors in memory | 4011.72 ms/step - Train Loss: 3.1011 | Validation loss: 3.1154\n",
      "Step 180/500 | 222 tensors in memory | 4012.78 ms/step - Train Loss: 3.1047 | Validation loss: 3.1656\n",
      "Step 181/500 | 222 tensors in memory | 4011.79 ms/step - Train Loss: 3.1293 | Validation loss: 3.1353\n",
      "Step 182/500 | 222 tensors in memory | 4010.97 ms/step - Train Loss: 3.1098 | Validation loss: 3.1401\n",
      "Step 183/500 | 222 tensors in memory | 4010.76 ms/step - Train Loss: 3.1307 | Validation loss: 3.1266\n",
      "Step 184/500 | 222 tensors in memory | 4010.28 ms/step - Train Loss: 3.1146 | Validation loss: 3.1556\n",
      "Step 185/500 | 222 tensors in memory | 4010.68 ms/step - Train Loss: 3.1395 | Validation loss: 3.1156\n",
      "Step 186/500 | 222 tensors in memory | 4011.73 ms/step - Train Loss: 3.1205 | Validation loss: 3.1977\n",
      "Step 187/500 | 222 tensors in memory | 4010.17 ms/step - Train Loss: 3.1245 | Validation loss: 3.1104\n",
      "Step 188/500 | 222 tensors in memory | 4010.15 ms/step - Train Loss: 3.0940 | Validation loss: 3.1912\n",
      "Step 189/500 | 222 tensors in memory | 4008.58 ms/step - Train Loss: 3.0949 | Validation loss: 3.1527\n",
      "Step 190/500 | 222 tensors in memory | 4007.60 ms/step - Train Loss: 3.1294 | Validation loss: 3.1314\n",
      "Step 191/500 | 222 tensors in memory | 4009.01 ms/step - Train Loss: 3.1652 | Validation loss: 3.1021\n",
      "Step 192/500 | 222 tensors in memory | 4008.32 ms/step - Train Loss: 3.0963 | Validation loss: 3.1631\n",
      "Step 193/500 | 222 tensors in memory | 4008.23 ms/step - Train Loss: 3.0815 | Validation loss: 3.1696\n",
      "Step 194/500 | 222 tensors in memory | 4007.94 ms/step - Train Loss: 3.1412 | Validation loss: 3.1068\n",
      "Step 195/500 | 222 tensors in memory | 4007.20 ms/step - Train Loss: 3.0635 | Validation loss: 3.1032\n",
      "Step 196/500 | 222 tensors in memory | 4006.74 ms/step - Train Loss: 3.0912 | Validation loss: 3.1504\n",
      "Step 197/500 | 222 tensors in memory | 4006.62 ms/step - Train Loss: 3.0439 | Validation loss: 3.1337\n",
      "Step 198/500 | 222 tensors in memory | 4007.26 ms/step - Train Loss: 3.0945 | Validation loss: 3.1040\n",
      "Step 199/500 | 222 tensors in memory | 4006.75 ms/step - Train Loss: 3.0635 | Validation loss: 3.1281\n",
      "Step 200/500 | 222 tensors in memory | 4006.00 ms/step - Train Loss: 3.1142 | Validation loss: 3.1285\n",
      "Step 201/500 | 222 tensors in memory | 4005.35 ms/step - Train Loss: 3.1131 | Validation loss: 3.1434\n",
      "Step 202/500 | 222 tensors in memory | 4004.46 ms/step - Train Loss: 3.0977 | Validation loss: 3.1463\n",
      "Step 203/500 | 222 tensors in memory | 4002.94 ms/step - Train Loss: 3.0382 | Validation loss: 3.1420\n",
      "Step 204/500 | 222 tensors in memory | 4004.00 ms/step - Train Loss: 3.0572 | Validation loss: 3.1058\n",
      "Step 205/500 | 222 tensors in memory | 4004.93 ms/step - Train Loss: 3.1103 | Validation loss: 3.1402\n",
      "Step 206/500 | 222 tensors in memory | 4004.20 ms/step - Train Loss: 3.0791 | Validation loss: 3.1467\n",
      "Step 207/500 | 222 tensors in memory | 4004.34 ms/step - Train Loss: 3.1035 | Validation loss: 3.1281\n",
      "Step 208/500 | 222 tensors in memory | 4006.00 ms/step - Train Loss: 3.0459 | Validation loss: 3.1128\n",
      "Step 209/500 | 222 tensors in memory | 4005.91 ms/step - Train Loss: 3.0851 | Validation loss: 3.1063\n",
      "Step 210/500 | 222 tensors in memory | 4005.82 ms/step - Train Loss: 3.0909 | Validation loss: 3.1633\n",
      "Step 211/500 | 222 tensors in memory | 4005.43 ms/step - Train Loss: 3.0742 | Validation loss: 3.1066\n",
      "Step 212/500 | 222 tensors in memory | 4004.95 ms/step - Train Loss: 3.0613 | Validation loss: 3.1015\n",
      "Step 213/500 | 222 tensors in memory | 4003.83 ms/step - Train Loss: 3.0577 | Validation loss: 3.0899\n",
      "Step 214/500 | 222 tensors in memory | 4002.52 ms/step - Train Loss: 3.0693 | Validation loss: 3.1137\n",
      "Step 215/500 | 222 tensors in memory | 4003.22 ms/step - Train Loss: 3.1138 | Validation loss: 3.1265\n",
      "Step 216/500 | 222 tensors in memory | 4003.44 ms/step - Train Loss: 3.0678 | Validation loss: 3.1111\n",
      "Step 217/500 | 222 tensors in memory | 4003.62 ms/step - Train Loss: 3.0723 | Validation loss: 3.1154\n",
      "Step 218/500 | 222 tensors in memory | 4003.91 ms/step - Train Loss: 3.0069 | Validation loss: 3.1002\n",
      "Step 219/500 | 222 tensors in memory | 4002.49 ms/step - Train Loss: 3.0808 | Validation loss: 3.1306\n",
      "Step 220/500 | 222 tensors in memory | 4001.40 ms/step - Train Loss: 3.0971 | Validation loss: 3.1110\n",
      "Step 221/500 | 222 tensors in memory | 4000.00 ms/step - Train Loss: 3.0738 | Validation loss: 3.0974\n",
      "Step 222/500 | 222 tensors in memory | 3998.13 ms/step - Train Loss: 3.0475 | Validation loss: 3.1032\n",
      "Step 223/500 | 222 tensors in memory | 3997.00 ms/step - Train Loss: 3.0351 | Validation loss: 3.1080\n",
      "Step 224/500 | 222 tensors in memory | 3995.50 ms/step - Train Loss: 3.0667 | Validation loss: 3.1199\n",
      "Step 225/500 | 222 tensors in memory | 3996.11 ms/step - Train Loss: 3.0491 | Validation loss: 3.1030\n",
      "Step 226/500 | 222 tensors in memory | 3996.71 ms/step - Train Loss: 3.0692 | Validation loss: 3.1043\n",
      "Step 227/500 | 222 tensors in memory | 3995.35 ms/step - Train Loss: 3.0716 | Validation loss: 3.0739\n",
      "Step 228/500 | 222 tensors in memory | 3995.30 ms/step - Train Loss: 3.0313 | Validation loss: 3.0629\n",
      "Step 229/500 | 222 tensors in memory | 3993.98 ms/step - Train Loss: 3.0466 | Validation loss: 3.0852\n",
      "Step 230/500 | 222 tensors in memory | 3992.08 ms/step - Train Loss: 3.0512 | Validation loss: 3.1125\n",
      "Step 231/500 | 222 tensors in memory | 3993.05 ms/step - Train Loss: 3.0517 | Validation loss: 3.0783\n",
      "Step 232/500 | 222 tensors in memory | 3994.14 ms/step - Train Loss: 3.0273 | Validation loss: 3.0981\n",
      "Step 233/500 | 222 tensors in memory | 3994.91 ms/step - Train Loss: 3.0894 | Validation loss: 3.0760\n",
      "Step 234/500 | 222 tensors in memory | 3994.97 ms/step - Train Loss: 3.0422 | Validation loss: 3.1098\n",
      "Step 235/500 | 222 tensors in memory | 3994.93 ms/step - Train Loss: 3.1029 | Validation loss: 3.1226\n",
      "Step 236/500 | 222 tensors in memory | 3994.01 ms/step - Train Loss: 3.0210 | Validation loss: 3.0795\n",
      "Step 237/500 | 222 tensors in memory | 3994.70 ms/step - Train Loss: 3.0704 | Validation loss: 3.1095\n",
      "Step 238/500 | 222 tensors in memory | 3994.04 ms/step - Train Loss: 3.0619 | Validation loss: 3.1378\n",
      "Step 239/500 | 222 tensors in memory | 3992.67 ms/step - Train Loss: 3.0636 | Validation loss: 3.1054\n",
      "Step 240/500 | 222 tensors in memory | 3992.31 ms/step - Train Loss: 3.0708 | Validation loss: 3.0884\n",
      "Step 241/500 | 222 tensors in memory | 3991.79 ms/step - Train Loss: 3.0430 | Validation loss: 3.0720\n",
      "Step 242/500 | 222 tensors in memory | 3990.30 ms/step - Train Loss: 3.0615 | Validation loss: 3.0782\n",
      "Step 243/500 | 222 tensors in memory | 3990.69 ms/step - Train Loss: 3.0798 | Validation loss: 3.1081\n",
      "Step 244/500 | 222 tensors in memory | 3989.25 ms/step - Train Loss: 3.0627 | Validation loss: 3.0755\n",
      "Step 245/500 | 222 tensors in memory | 3988.29 ms/step - Train Loss: 3.0292 | Validation loss: 3.0808\n",
      "Step 246/500 | 222 tensors in memory | 3986.59 ms/step - Train Loss: 3.0348 | Validation loss: 3.1286\n",
      "Step 247/500 | 222 tensors in memory | 3984.87 ms/step - Train Loss: 3.0784 | Validation loss: 3.1105\n",
      "Step 248/500 | 222 tensors in memory | 3983.68 ms/step - Train Loss: 3.0602 | Validation loss: 3.0962\n",
      "Step 249/500 | 222 tensors in memory | 3983.23 ms/step - Train Loss: 3.0873 | Validation loss: 3.0880\n",
      "Step 250/500 | 222 tensors in memory | 3982.72 ms/step - Train Loss: 3.0652 | Validation loss: 3.0951\n",
      "Step 251/500 | 222 tensors in memory | 3980.97 ms/step - Train Loss: 3.0167 | Validation loss: 3.0901\n",
      "Step 252/500 | 222 tensors in memory | 3979.60 ms/step - Train Loss: 3.0216 | Validation loss: 3.1045\n",
      "Step 253/500 | 222 tensors in memory | 3977.84 ms/step - Train Loss: 3.0425 | Validation loss: 3.0644\n",
      "Step 254/500 | 222 tensors in memory | 3976.58 ms/step - Train Loss: 3.0179 | Validation loss: 3.0868\n",
      "Step 255/500 | 222 tensors in memory | 3975.26 ms/step - Train Loss: 3.0826 | Validation loss: 3.1006\n",
      "Step 256/500 | 222 tensors in memory | 3975.24 ms/step - Train Loss: 3.0383 | Validation loss: 3.0606\n",
      "Step 257/500 | 222 tensors in memory | 3973.70 ms/step - Train Loss: 2.9988 | Validation loss: 3.0793\n",
      "Step 258/500 | 222 tensors in memory | 3971.89 ms/step - Train Loss: 2.9990 | Validation loss: 3.0577\n",
      "Step 259/500 | 222 tensors in memory | 3970.96 ms/step - Train Loss: 3.0237 | Validation loss: 3.0709\n",
      "Step 260/500 | 222 tensors in memory | 3968.80 ms/step - Train Loss: 3.0332 | Validation loss: 3.0468\n",
      "Step 261/500 | 222 tensors in memory | 3968.66 ms/step - Train Loss: 3.0742 | Validation loss: 3.0449\n",
      "Step 262/500 | 222 tensors in memory | 3966.89 ms/step - Train Loss: 3.0373 | Validation loss: 3.0556\n",
      "Step 263/500 | 222 tensors in memory | 3966.11 ms/step - Train Loss: 3.0237 | Validation loss: 3.0698\n",
      "Step 264/500 | 222 tensors in memory | 3964.94 ms/step - Train Loss: 3.0400 | Validation loss: 3.0680\n",
      "Step 265/500 | 222 tensors in memory | 3963.44 ms/step - Train Loss: 3.0570 | Validation loss: 3.0488\n",
      "Step 266/500 | 222 tensors in memory | 3961.76 ms/step - Train Loss: 3.0806 | Validation loss: 3.0376\n",
      "Step 267/500 | 222 tensors in memory | 3960.34 ms/step - Train Loss: 3.0184 | Validation loss: 3.0536\n",
      "Step 268/500 | 222 tensors in memory | 3958.83 ms/step - Train Loss: 3.0274 | Validation loss: 3.0455\n",
      "Step 269/500 | 222 tensors in memory | 3957.53 ms/step - Train Loss: 3.0089 | Validation loss: 3.0705\n",
      "Step 270/500 | 222 tensors in memory | 3956.33 ms/step - Train Loss: 3.0357 | Validation loss: 3.0894\n",
      "Step 271/500 | 222 tensors in memory | 3956.63 ms/step - Train Loss: 3.0718 | Validation loss: 3.1070\n",
      "Step 272/500 | 222 tensors in memory | 3955.59 ms/step - Train Loss: 3.0166 | Validation loss: 3.0703\n",
      "Step 273/500 | 222 tensors in memory | 3954.37 ms/step - Train Loss: 3.0074 | Validation loss: 3.0681\n",
      "Step 274/500 | 222 tensors in memory | 3953.74 ms/step - Train Loss: 3.0432 | Validation loss: 3.1127\n",
      "Step 275/500 | 222 tensors in memory | 3952.51 ms/step - Train Loss: 3.0141 | Validation loss: 3.0859\n",
      "Step 276/500 | 222 tensors in memory | 3951.30 ms/step - Train Loss: 3.0159 | Validation loss: 3.0623\n",
      "Step 277/500 | 222 tensors in memory | 3949.29 ms/step - Train Loss: 3.0583 | Validation loss: 3.0698\n",
      "Step 278/500 | 222 tensors in memory | 3947.42 ms/step - Train Loss: 3.0468 | Validation loss: 3.0915\n",
      "Step 279/500 | 222 tensors in memory | 3945.83 ms/step - Train Loss: 2.9970 | Validation loss: 3.0765\n",
      "Step 280/500 | 222 tensors in memory | 3944.81 ms/step - Train Loss: 3.0285 | Validation loss: 3.0578\n",
      "Step 281/500 | 222 tensors in memory | 3943.36 ms/step - Train Loss: 3.0039 | Validation loss: 3.0471\n",
      "Step 282/500 | 222 tensors in memory | 3941.76 ms/step - Train Loss: 3.0435 | Validation loss: 3.0642\n",
      "Step 283/500 | 222 tensors in memory | 3942.44 ms/step - Train Loss: 3.0351 | Validation loss: 3.0426\n",
      "Step 284/500 | 222 tensors in memory | 3943.88 ms/step - Train Loss: 3.0272 | Validation loss: 3.1013\n",
      "Step 285/500 | 222 tensors in memory | 3944.28 ms/step - Train Loss: 2.9915 | Validation loss: 3.0694\n",
      "Step 286/500 | 222 tensors in memory | 3944.30 ms/step - Train Loss: 2.9740 | Validation loss: 3.0885\n",
      "Step 287/500 | 222 tensors in memory | 3945.41 ms/step - Train Loss: 3.0731 | Validation loss: 3.0672\n",
      "Step 288/500 | 222 tensors in memory | 3945.39 ms/step - Train Loss: 3.0398 | Validation loss: 3.0610\n",
      "Step 289/500 | 222 tensors in memory | 3944.64 ms/step - Train Loss: 3.0591 | Validation loss: 3.1144\n",
      "Step 290/500 | 222 tensors in memory | 3944.36 ms/step - Train Loss: 3.0031 | Validation loss: 3.0584\n",
      "Step 291/500 | 222 tensors in memory | 3944.11 ms/step - Train Loss: 3.0211 | Validation loss: 3.0420\n",
      "Step 292/500 | 222 tensors in memory | 3943.46 ms/step - Train Loss: 2.9999 | Validation loss: 3.0837\n",
      "Step 293/500 | 222 tensors in memory | 3943.91 ms/step - Train Loss: 3.0561 | Validation loss: 3.0575\n",
      "Step 294/500 | 222 tensors in memory | 3942.91 ms/step - Train Loss: 3.0314 | Validation loss: 3.0387\n",
      "Step 295/500 | 222 tensors in memory | 3942.01 ms/step - Train Loss: 3.0497 | Validation loss: 3.0668\n",
      "Step 296/500 | 222 tensors in memory | 3941.35 ms/step - Train Loss: 3.0496 | Validation loss: 3.0511\n",
      "Step 297/500 | 222 tensors in memory | 3941.38 ms/step - Train Loss: 3.0335 | Validation loss: 3.0826\n",
      "Step 298/500 | 222 tensors in memory | 3940.44 ms/step - Train Loss: 3.0464 | Validation loss: 3.0452\n",
      "Step 299/500 | 222 tensors in memory | 3939.31 ms/step - Train Loss: 2.9957 | Validation loss: 3.0691\n",
      "Step 300/500 | 222 tensors in memory | 3939.65 ms/step - Train Loss: 2.9942 | Validation loss: 3.0508\n",
      "Step 301/500 | 222 tensors in memory | 3938.99 ms/step - Train Loss: 3.0200 | Validation loss: 3.0934\n",
      "Step 302/500 | 222 tensors in memory | 3938.88 ms/step - Train Loss: 3.0021 | Validation loss: 3.0534\n",
      "Step 303/500 | 222 tensors in memory | 3938.04 ms/step - Train Loss: 3.0096 | Validation loss: 3.0930\n",
      "Step 304/500 | 222 tensors in memory | 3936.85 ms/step - Train Loss: 3.0083 | Validation loss: 3.0779\n",
      "Step 305/500 | 222 tensors in memory | 3936.09 ms/step - Train Loss: 2.9969 | Validation loss: 3.0657\n",
      "Step 306/500 | 222 tensors in memory | 3935.45 ms/step - Train Loss: 2.9876 | Validation loss: 3.0520\n",
      "Step 307/500 | 222 tensors in memory | 3935.28 ms/step - Train Loss: 2.9902 | Validation loss: 3.0675\n",
      "Step 308/500 | 222 tensors in memory | 3934.23 ms/step - Train Loss: 3.0020 | Validation loss: 3.0726\n",
      "Step 309/500 | 222 tensors in memory | 3933.17 ms/step - Train Loss: 2.9989 | Validation loss: 3.0808\n",
      "Step 310/500 | 222 tensors in memory | 3931.94 ms/step - Train Loss: 2.9667 | Validation loss: 3.1022\n",
      "Step 311/500 | 222 tensors in memory | 3930.58 ms/step - Train Loss: 3.0312 | Validation loss: 3.0603\n",
      "Step 312/500 | 222 tensors in memory | 3930.40 ms/step - Train Loss: 2.9661 | Validation loss: 3.0421\n",
      "Step 313/500 | 222 tensors in memory | 3929.61 ms/step - Train Loss: 3.0420 | Validation loss: 3.0420\n",
      "Step 314/500 | 222 tensors in memory | 3928.59 ms/step - Train Loss: 3.0110 | Validation loss: 3.0905\n",
      "Step 315/500 | 222 tensors in memory | 3927.84 ms/step - Train Loss: 3.0331 | Validation loss: 3.0438\n",
      "Step 316/500 | 222 tensors in memory | 3927.31 ms/step - Train Loss: 2.9934 | Validation loss: 3.0358\n",
      "Step 317/500 | 222 tensors in memory | 3926.45 ms/step - Train Loss: 2.9988 | Validation loss: 3.0657\n",
      "Step 318/500 | 222 tensors in memory | 3925.86 ms/step - Train Loss: 2.9870 | Validation loss: 3.0266\n",
      "Step 319/500 | 222 tensors in memory | 3925.87 ms/step - Train Loss: 2.9992 | Validation loss: 3.0707\n",
      "Step 320/500 | 222 tensors in memory | 3924.76 ms/step - Train Loss: 3.0276 | Validation loss: 3.0335\n",
      "Step 321/500 | 222 tensors in memory | 3924.25 ms/step - Train Loss: 2.9761 | Validation loss: 3.0734\n",
      "Step 322/500 | 222 tensors in memory | 3923.16 ms/step - Train Loss: 3.0154 | Validation loss: 3.0567\n",
      "Step 323/500 | 222 tensors in memory | 3922.02 ms/step - Train Loss: 3.0126 | Validation loss: 3.0247\n",
      "Step 324/500 | 222 tensors in memory | 3920.91 ms/step - Train Loss: 2.9694 | Validation loss: 3.0014\n",
      "Step 325/500 | 222 tensors in memory | 3920.18 ms/step - Train Loss: 3.0091 | Validation loss: 3.0480\n",
      "Step 326/500 | 222 tensors in memory | 3919.86 ms/step - Train Loss: 2.9630 | Validation loss: 3.0555\n",
      "Step 327/500 | 222 tensors in memory | 3918.98 ms/step - Train Loss: 3.0143 | Validation loss: 3.1015\n",
      "Step 328/500 | 222 tensors in memory | 3918.22 ms/step - Train Loss: 3.0507 | Validation loss: 3.0447\n",
      "Step 329/500 | 222 tensors in memory | 3917.59 ms/step - Train Loss: 3.0093 | Validation loss: 3.0380\n",
      "Step 330/500 | 222 tensors in memory | 3916.70 ms/step - Train Loss: 2.9591 | Validation loss: 3.0453\n",
      "Step 331/500 | 222 tensors in memory | 3916.39 ms/step - Train Loss: 3.0006 | Validation loss: 3.1016\n",
      "Step 332/500 | 222 tensors in memory | 3915.52 ms/step - Train Loss: 2.9675 | Validation loss: 3.0546\n",
      "Step 333/500 | 222 tensors in memory | 3915.65 ms/step - Train Loss: 2.9853 | Validation loss: 3.0471\n",
      "Step 334/500 | 222 tensors in memory | 3914.91 ms/step - Train Loss: 2.9717 | Validation loss: 3.0275\n",
      "Step 335/500 | 222 tensors in memory | 3914.36 ms/step - Train Loss: 2.9704 | Validation loss: 3.0383\n",
      "Step 336/500 | 222 tensors in memory | 3913.32 ms/step - Train Loss: 2.9865 | Validation loss: 3.0314\n",
      "Step 337/500 | 222 tensors in memory | 3912.08 ms/step - Train Loss: 2.9782 | Validation loss: 3.0249\n",
      "Step 338/500 | 222 tensors in memory | 3910.91 ms/step - Train Loss: 3.0011 | Validation loss: 3.0233\n",
      "Step 339/500 | 222 tensors in memory | 3909.90 ms/step - Train Loss: 2.9483 | Validation loss: 3.0366\n",
      "Step 340/500 | 222 tensors in memory | 3908.87 ms/step - Train Loss: 2.9712 | Validation loss: 3.0496\n",
      "Step 341/500 | 222 tensors in memory | 3907.99 ms/step - Train Loss: 2.9526 | Validation loss: 3.0126\n",
      "Step 342/500 | 222 tensors in memory | 3907.40 ms/step - Train Loss: 3.0049 | Validation loss: 3.0755\n",
      "Step 343/500 | 222 tensors in memory | 3906.54 ms/step - Train Loss: 2.9458 | Validation loss: 3.0263\n",
      "Step 344/500 | 222 tensors in memory | 3905.98 ms/step - Train Loss: 2.9876 | Validation loss: 3.0260\n",
      "Step 345/500 | 222 tensors in memory | 3905.02 ms/step - Train Loss: 2.9609 | Validation loss: 3.0225\n",
      "Step 346/500 | 222 tensors in memory | 3904.06 ms/step - Train Loss: 2.9701 | Validation loss: 3.0109\n",
      "Step 347/500 | 222 tensors in memory | 3902.92 ms/step - Train Loss: 2.9584 | Validation loss: 3.0401\n",
      "Step 348/500 | 222 tensors in memory | 3901.71 ms/step - Train Loss: 2.9819 | Validation loss: 3.0268\n",
      "Step 349/500 | 222 tensors in memory | 3902.15 ms/step - Train Loss: 2.9632 | Validation loss: 3.0445\n",
      "Step 350/500 | 222 tensors in memory | 3902.15 ms/step - Train Loss: 3.0050 | Validation loss: 3.0452\n",
      "Step 351/500 | 222 tensors in memory | 3901.62 ms/step - Train Loss: 2.9381 | Validation loss: 2.9919\n",
      "Step 352/500 | 222 tensors in memory | 3900.69 ms/step - Train Loss: 2.9857 | Validation loss: 3.0221\n",
      "Step 353/500 | 222 tensors in memory | 3899.83 ms/step - Train Loss: 2.9709 | Validation loss: 3.0059\n",
      "Step 354/500 | 222 tensors in memory | 3899.56 ms/step - Train Loss: 3.0053 | Validation loss: 3.0151\n",
      "Step 355/500 | 222 tensors in memory | 3898.55 ms/step - Train Loss: 2.9482 | Validation loss: 3.0492\n",
      "Step 356/500 | 222 tensors in memory | 3897.56 ms/step - Train Loss: 2.9728 | Validation loss: 2.9709\n",
      "Step 357/500 | 222 tensors in memory | 3896.60 ms/step - Train Loss: 2.9284 | Validation loss: 3.0081\n",
      "Step 358/500 | 222 tensors in memory | 3895.56 ms/step - Train Loss: 2.9776 | Validation loss: 2.9894\n",
      "Step 359/500 | 222 tensors in memory | 3894.44 ms/step - Train Loss: 2.9496 | Validation loss: 3.1020\n",
      "Step 360/500 | 222 tensors in memory | 3893.54 ms/step - Train Loss: 2.8882 | Validation loss: 3.0428\n",
      "Step 361/500 | 222 tensors in memory | 3893.17 ms/step - Train Loss: 2.9174 | Validation loss: 3.0492\n",
      "Step 362/500 | 222 tensors in memory | 3897.25 ms/step - Train Loss: 2.9375 | Validation loss: 3.0463\n",
      "Step 363/500 | 222 tensors in memory | 3898.49 ms/step - Train Loss: 2.9289 | Validation loss: 3.0231\n",
      "Step 364/500 | 222 tensors in memory | 3898.76 ms/step - Train Loss: 2.9387 | Validation loss: 3.0424\n",
      "Step 365/500 | 222 tensors in memory | 3898.91 ms/step - Train Loss: 2.9449 | Validation loss: 3.0360\n",
      "Step 366/500 | 222 tensors in memory | 3898.99 ms/step - Train Loss: 2.9150 | Validation loss: 3.0194\n",
      "Step 367/500 | 222 tensors in memory | 3900.04 ms/step - Train Loss: 2.9319 | Validation loss: 2.9702\n",
      "Step 368/500 | 222 tensors in memory | 3900.49 ms/step - Train Loss: 2.9557 | Validation loss: 3.0318\n",
      "Step 369/500 | 222 tensors in memory | 3900.13 ms/step - Train Loss: 2.9230 | Validation loss: 3.0130\n",
      "Step 370/500 | 222 tensors in memory | 3899.21 ms/step - Train Loss: 2.9072 | Validation loss: 3.0313\n",
      "Step 371/500 | 222 tensors in memory | 3898.88 ms/step - Train Loss: 2.9317 | Validation loss: 2.9980\n",
      "Step 372/500 | 222 tensors in memory | 3898.57 ms/step - Train Loss: 2.9447 | Validation loss: 3.0126\n",
      "Step 373/500 | 222 tensors in memory | 3898.51 ms/step - Train Loss: 2.9520 | Validation loss: 2.9819\n",
      "Step 374/500 | 222 tensors in memory | 3898.29 ms/step - Train Loss: 2.9143 | Validation loss: 3.0146\n",
      "Step 375/500 | 222 tensors in memory | 3897.50 ms/step - Train Loss: 2.9304 | Validation loss: 3.0214\n",
      "Step 376/500 | 222 tensors in memory | 3896.65 ms/step - Train Loss: 2.9234 | Validation loss: 2.9876\n",
      "Step 377/500 | 222 tensors in memory | 3895.96 ms/step - Train Loss: 2.9404 | Validation loss: 2.9893\n",
      "Step 378/500 | 222 tensors in memory | 3896.10 ms/step - Train Loss: 2.9013 | Validation loss: 2.9748\n",
      "Step 379/500 | 222 tensors in memory | 3896.23 ms/step - Train Loss: 2.9409 | Validation loss: 3.0517\n",
      "Step 380/500 | 222 tensors in memory | 3895.92 ms/step - Train Loss: 2.9093 | Validation loss: 2.9935\n",
      "Step 381/500 | 222 tensors in memory | 3895.32 ms/step - Train Loss: 2.8924 | Validation loss: 3.0544\n",
      "Step 382/500 | 222 tensors in memory | 3894.85 ms/step - Train Loss: 2.8861 | Validation loss: 2.9956\n",
      "Step 383/500 | 222 tensors in memory | 3894.32 ms/step - Train Loss: 2.9128 | Validation loss: 2.9999\n",
      "Step 384/500 | 222 tensors in memory | 3893.86 ms/step - Train Loss: 2.9112 | Validation loss: 3.0378\n",
      "Step 385/500 | 222 tensors in memory | 3893.39 ms/step - Train Loss: 2.9110 | Validation loss: 3.0113\n",
      "Step 386/500 | 222 tensors in memory | 3893.28 ms/step - Train Loss: 2.9197 | Validation loss: 2.9926\n",
      "Step 387/500 | 222 tensors in memory | 3893.21 ms/step - Train Loss: 2.9044 | Validation loss: 2.9561\n",
      "Step 388/500 | 222 tensors in memory | 3892.94 ms/step - Train Loss: 2.9032 | Validation loss: 3.0100\n",
      "Step 389/500 | 222 tensors in memory | 3892.83 ms/step - Train Loss: 2.9269 | Validation loss: 2.9839\n",
      "Step 390/500 | 222 tensors in memory | 3893.04 ms/step - Train Loss: 2.9474 | Validation loss: 2.9973\n",
      "Step 391/500 | 222 tensors in memory | 3892.21 ms/step - Train Loss: 2.8974 | Validation loss: 2.9897\n",
      "Step 392/500 | 222 tensors in memory | 3891.49 ms/step - Train Loss: 2.9219 | Validation loss: 2.9873\n",
      "Step 393/500 | 222 tensors in memory | 3890.82 ms/step - Train Loss: 2.9028 | Validation loss: 2.9893\n",
      "Step 394/500 | 222 tensors in memory | 3890.02 ms/step - Train Loss: 2.8684 | Validation loss: 3.0126\n",
      "Step 395/500 | 222 tensors in memory | 3888.74 ms/step - Train Loss: 2.8783 | Validation loss: 2.9528\n",
      "Step 396/500 | 222 tensors in memory | 3887.53 ms/step - Train Loss: 2.8883 | Validation loss: 2.9654\n",
      "Step 397/500 | 222 tensors in memory | 3887.21 ms/step - Train Loss: 2.8609 | Validation loss: 2.9889\n",
      "Step 398/500 | 222 tensors in memory | 3886.51 ms/step - Train Loss: 2.8728 | Validation loss: 2.9792\n",
      "Step 399/500 | 222 tensors in memory | 3886.38 ms/step - Train Loss: 2.8560 | Validation loss: 3.0034\n",
      "Step 400/500 | 222 tensors in memory | 3885.14 ms/step - Train Loss: 2.9013 | Validation loss: 2.9730\n",
      "Step 401/500 | 222 tensors in memory | 3884.01 ms/step - Train Loss: 2.8698 | Validation loss: 3.0336\n",
      "Step 402/500 | 222 tensors in memory | 3882.73 ms/step - Train Loss: 2.8587 | Validation loss: 2.9706\n",
      "Step 403/500 | 222 tensors in memory | 3881.56 ms/step - Train Loss: 2.8994 | Validation loss: 2.9832\n",
      "Step 404/500 | 222 tensors in memory | 3880.22 ms/step - Train Loss: 2.8827 | Validation loss: 2.9690\n",
      "Step 405/500 | 222 tensors in memory | 3878.97 ms/step - Train Loss: 2.8636 | Validation loss: 2.9910\n",
      "Step 406/500 | 222 tensors in memory | 3877.83 ms/step - Train Loss: 2.9024 | Validation loss: 3.0015\n",
      "Step 407/500 | 222 tensors in memory | 3876.49 ms/step - Train Loss: 2.8975 | Validation loss: 2.9814\n",
      "Step 408/500 | 222 tensors in memory | 3875.16 ms/step - Train Loss: 2.9196 | Validation loss: 2.9826\n",
      "Step 409/500 | 222 tensors in memory | 3873.93 ms/step - Train Loss: 2.8664 | Validation loss: 2.9385\n",
      "Step 410/500 | 222 tensors in memory | 3872.65 ms/step - Train Loss: 2.8343 | Validation loss: 2.9426\n",
      "Step 411/500 | 222 tensors in memory | 3871.36 ms/step - Train Loss: 2.8599 | Validation loss: 2.9592\n",
      "Step 412/500 | 222 tensors in memory | 3870.26 ms/step - Train Loss: 2.8900 | Validation loss: 2.9676\n",
      "Step 413/500 | 222 tensors in memory | 3869.06 ms/step - Train Loss: 2.8379 | Validation loss: 2.9729\n",
      "Step 414/500 | 222 tensors in memory | 3867.73 ms/step - Train Loss: 2.8879 | Validation loss: 2.9352\n",
      "Step 415/500 | 222 tensors in memory | 3866.41 ms/step - Train Loss: 2.8708 | Validation loss: 2.9447\n",
      "Step 416/500 | 222 tensors in memory | 3865.78 ms/step - Train Loss: 2.9018 | Validation loss: 2.9847\n",
      "Step 417/500 | 222 tensors in memory | 3864.70 ms/step - Train Loss: 2.8578 | Validation loss: 2.9861\n",
      "Step 418/500 | 222 tensors in memory | 3863.84 ms/step - Train Loss: 2.8805 | Validation loss: 2.9548\n",
      "Step 419/500 | 222 tensors in memory | 3862.77 ms/step - Train Loss: 2.8584 | Validation loss: 2.9413\n",
      "Step 420/500 | 222 tensors in memory | 3861.72 ms/step - Train Loss: 2.8163 | Validation loss: 2.9628\n",
      "Step 421/500 | 222 tensors in memory | 3860.47 ms/step - Train Loss: 2.8478 | Validation loss: 2.9644\n",
      "Step 422/500 | 222 tensors in memory | 3859.40 ms/step - Train Loss: 2.8250 | Validation loss: 2.9548\n",
      "Step 423/500 | 222 tensors in memory | 3858.13 ms/step - Train Loss: 2.8843 | Validation loss: 2.9734\n",
      "Step 424/500 | 222 tensors in memory | 3856.84 ms/step - Train Loss: 2.8342 | Validation loss: 2.9395\n",
      "Step 425/500 | 222 tensors in memory | 3855.77 ms/step - Train Loss: 2.8197 | Validation loss: 2.9267\n",
      "Step 426/500 | 222 tensors in memory | 3855.36 ms/step - Train Loss: 2.8871 | Validation loss: 2.9306\n",
      "Step 427/500 | 222 tensors in memory | 3855.14 ms/step - Train Loss: 2.8433 | Validation loss: 2.9241\n",
      "Step 428/500 | 222 tensors in memory | 3856.37 ms/step - Train Loss: 2.8217 | Validation loss: 2.9475\n",
      "Step 429/500 | 222 tensors in memory | 3855.78 ms/step - Train Loss: 2.8496 | Validation loss: 2.9458\n",
      "Step 430/500 | 222 tensors in memory | 3855.14 ms/step - Train Loss: 2.7909 | Validation loss: 2.9748\n",
      "Step 431/500 | 222 tensors in memory | 3854.28 ms/step - Train Loss: 2.8002 | Validation loss: 2.9628\n",
      "Step 432/500 | 222 tensors in memory | 3853.27 ms/step - Train Loss: 2.8165 | Validation loss: 2.8985\n",
      "Step 433/500 | 222 tensors in memory | 3852.12 ms/step - Train Loss: 2.8479 | Validation loss: 2.9196\n",
      "Step 434/500 | 222 tensors in memory | 3851.05 ms/step - Train Loss: 2.8167 | Validation loss: 2.9323\n",
      "Step 435/500 | 222 tensors in memory | 3849.92 ms/step - Train Loss: 2.8699 | Validation loss: 2.9207\n",
      "Step 436/500 | 222 tensors in memory | 3849.19 ms/step - Train Loss: 2.8311 | Validation loss: 2.8995\n",
      "Step 437/500 | 222 tensors in memory | 3848.09 ms/step - Train Loss: 2.8424 | Validation loss: 2.9138\n",
      "Step 438/500 | 222 tensors in memory | 3847.15 ms/step - Train Loss: 2.8016 | Validation loss: 2.8717\n",
      "Step 439/500 | 222 tensors in memory | 3847.05 ms/step - Train Loss: 2.7886 | Validation loss: 2.9362\n",
      "Step 440/500 | 222 tensors in memory | 3845.86 ms/step - Train Loss: 2.8054 | Validation loss: 2.9085\n",
      "Step 441/500 | 222 tensors in memory | 3844.72 ms/step - Train Loss: 2.8213 | Validation loss: 2.9168\n",
      "Step 442/500 | 222 tensors in memory | 3843.65 ms/step - Train Loss: 2.8320 | Validation loss: 2.9431\n",
      "Step 443/500 | 222 tensors in memory | 3842.58 ms/step - Train Loss: 2.7912 | Validation loss: 2.8689\n",
      "Step 444/500 | 222 tensors in memory | 3841.41 ms/step - Train Loss: 2.7646 | Validation loss: 2.8763\n",
      "Step 445/500 | 222 tensors in memory | 3840.35 ms/step - Train Loss: 2.7875 | Validation loss: 2.9305\n",
      "Step 446/500 | 222 tensors in memory | 3839.36 ms/step - Train Loss: 2.8003 | Validation loss: 2.9182\n",
      "Step 447/500 | 222 tensors in memory | 3838.26 ms/step - Train Loss: 2.7872 | Validation loss: 2.9050\n",
      "Step 448/500 | 222 tensors in memory | 3838.23 ms/step - Train Loss: 2.7918 | Validation loss: 2.8820\n",
      "Step 449/500 | 222 tensors in memory | 3837.58 ms/step - Train Loss: 2.7844 | Validation loss: 2.8971\n",
      "Step 450/500 | 222 tensors in memory | 3836.82 ms/step - Train Loss: 2.8263 | Validation loss: 2.9504\n",
      "Step 451/500 | 222 tensors in memory | 3835.71 ms/step - Train Loss: 2.7467 | Validation loss: 2.9599\n",
      "Step 452/500 | 222 tensors in memory | 3838.54 ms/step - Train Loss: 2.7750 | Validation loss: 2.9334\n",
      "Step 453/500 | 222 tensors in memory | 3838.89 ms/step - Train Loss: 2.7490 | Validation loss: 2.9376\n",
      "Step 454/500 | 222 tensors in memory | 3839.00 ms/step - Train Loss: 2.7625 | Validation loss: 2.9008\n",
      "Step 455/500 | 222 tensors in memory | 3838.62 ms/step - Train Loss: 2.7392 | Validation loss: 2.9015\n",
      "Step 456/500 | 222 tensors in memory | 3838.23 ms/step - Train Loss: 2.7978 | Validation loss: 2.8953\n",
      "Step 457/500 | 222 tensors in memory | 3839.35 ms/step - Train Loss: 2.7591 | Validation loss: 2.8878\n",
      "Step 458/500 | 222 tensors in memory | 3839.88 ms/step - Train Loss: 2.7637 | Validation loss: 2.9172\n",
      "Step 459/500 | 222 tensors in memory | 3839.83 ms/step - Train Loss: 2.7791 | Validation loss: 2.8428\n",
      "Step 460/500 | 222 tensors in memory | 3839.91 ms/step - Train Loss: 2.7046 | Validation loss: 2.8465\n",
      "Step 461/500 | 222 tensors in memory | 3839.94 ms/step - Train Loss: 2.7734 | Validation loss: 2.8941\n",
      "Step 462/500 | 222 tensors in memory | 3840.33 ms/step - Train Loss: 2.7715 | Validation loss: 2.9558\n",
      "Step 463/500 | 222 tensors in memory | 3839.90 ms/step - Train Loss: 2.7564 | Validation loss: 2.8822\n",
      "Step 464/500 | 222 tensors in memory | 3839.38 ms/step - Train Loss: 2.7880 | Validation loss: 2.9383\n",
      "Step 465/500 | 222 tensors in memory | 3839.35 ms/step - Train Loss: 2.7802 | Validation loss: 2.8506\n",
      "Step 466/500 | 222 tensors in memory | 3841.59 ms/step - Train Loss: 2.7696 | Validation loss: 2.8677\n",
      "Step 467/500 | 222 tensors in memory | 3842.22 ms/step - Train Loss: 2.7544 | Validation loss: 2.8459\n",
      "Step 468/500 | 222 tensors in memory | 3841.99 ms/step - Train Loss: 2.7041 | Validation loss: 2.9379\n",
      "Step 469/500 | 222 tensors in memory | 3841.76 ms/step - Train Loss: 2.7487 | Validation loss: 2.9189\n",
      "Step 470/500 | 222 tensors in memory | 3841.46 ms/step - Train Loss: 2.6994 | Validation loss: 2.8731\n",
      "Step 471/500 | 222 tensors in memory | 3841.16 ms/step - Train Loss: 2.7550 | Validation loss: 2.8875\n",
      "Step 472/500 | 222 tensors in memory | 3841.35 ms/step - Train Loss: 2.7418 | Validation loss: 2.8389\n",
      "Step 473/500 | 222 tensors in memory | 3840.88 ms/step - Train Loss: 2.7171 | Validation loss: 2.8603\n",
      "Step 474/500 | 222 tensors in memory | 3840.44 ms/step - Train Loss: 2.7528 | Validation loss: 2.8829\n",
      "Step 475/500 | 222 tensors in memory | 3840.16 ms/step - Train Loss: 2.7331 | Validation loss: 2.8595\n",
      "Step 476/500 | 222 tensors in memory | 3840.09 ms/step - Train Loss: 2.6749 | Validation loss: 2.8437\n",
      "Step 477/500 | 222 tensors in memory | 3840.51 ms/step - Train Loss: 2.7277 | Validation loss: 2.8771\n",
      "Step 478/500 | 222 tensors in memory | 3840.18 ms/step - Train Loss: 2.7192 | Validation loss: 2.8372\n",
      "Step 479/500 | 222 tensors in memory | 3839.73 ms/step - Train Loss: 2.7080 | Validation loss: 2.8180\n",
      "Step 480/500 | 222 tensors in memory | 3839.00 ms/step - Train Loss: 2.7006 | Validation loss: 2.8621\n",
      "Step 481/500 | 222 tensors in memory | 3838.51 ms/step - Train Loss: 2.7255 | Validation loss: 2.8796\n",
      "Step 482/500 | 222 tensors in memory | 3837.92 ms/step - Train Loss: 2.6930 | Validation loss: 2.8700\n",
      "Step 483/500 | 222 tensors in memory | 3838.14 ms/step - Train Loss: 2.7217 | Validation loss: 2.8492\n",
      "Step 484/500 | 222 tensors in memory | 3839.49 ms/step - Train Loss: 2.6856 | Validation loss: 2.8662\n",
      "Step 485/500 | 222 tensors in memory | 3840.99 ms/step - Train Loss: 2.6858 | Validation loss: 2.8315\n",
      "Step 486/500 | 222 tensors in memory | 3842.03 ms/step - Train Loss: 2.7544 | Validation loss: 2.8521\n",
      "Step 487/500 | 222 tensors in memory | 3843.40 ms/step - Train Loss: 2.6910 | Validation loss: 2.8436\n",
      "Step 488/500 | 222 tensors in memory | 3843.72 ms/step - Train Loss: 2.6755 | Validation loss: 2.8591\n",
      "Step 489/500 | 222 tensors in memory | 3843.44 ms/step - Train Loss: 2.7079 | Validation loss: 2.8350\n",
      "Step 490/500 | 222 tensors in memory | 3843.18 ms/step - Train Loss: 2.6711 | Validation loss: 2.8548\n",
      "Step 491/500 | 222 tensors in memory | 3843.23 ms/step - Train Loss: 2.6517 | Validation loss: 2.8496\n",
      "Step 492/500 | 222 tensors in memory | 3843.26 ms/step - Train Loss: 2.6612 | Validation loss: 2.8073\n",
      "Step 493/500 | 222 tensors in memory | 3842.84 ms/step - Train Loss: 2.6329 | Validation loss: 2.7826\n",
      "Step 494/500 | 222 tensors in memory | 3842.99 ms/step - Train Loss: 2.6795 | Validation loss: 2.7893\n",
      "Step 495/500 | 222 tensors in memory | 3842.82 ms/step - Train Loss: 2.6737 | Validation loss: 2.8100\n",
      "Step 496/500 | 222 tensors in memory | 3842.75 ms/step - Train Loss: 2.7303 | Validation loss: 2.8201\n",
      "Step 497/500 | 222 tensors in memory | 3842.56 ms/step - Train Loss: 2.6441 | Validation loss: 2.8455\n",
      "Step 498/500 | 222 tensors in memory | 3842.35 ms/step - Train Loss: 2.6765 | Validation loss: 2.8230\n",
      "Step 499/500 | 222 tensors in memory | 3842.09 ms/step - Train Loss: 2.6598 | Validation loss: 2.8243\n",
      "Step 500/500 | 222 tensors in memory | 3841.67 ms/step - Train Loss: 2.6588 | Validation loss: 2.8234\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = language_model.fit(\n",
    "    data_loader = data_loader,\n",
    "    steps = training_steps,\n",
    "    lr = learning_rate,\n",
    "    batch_size = batch_size,\n",
    "    eval_iters = eval_iters,\n",
    "    grad_accumulation_steps = grad_accumulation_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "language_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation loss\n",
    "data_analysis.plot_history(history[\"loss\"].data, history[\"val_loss\"].data, \"Training and Validation Loss\", \"Eval iter\", \"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inva finseguralle\n",
      "Santo di vien Stasta male,\n",
      "come luce ed io io con que' quabbere,\n",
      "Cadcome stringerbo che 'l si 'l enno more.\n",
      "Ora e ora me si per monta si far questa acqua,\n",
      "e finto ancor era ride,\n",
      "semelle tristo, assi,\n",
      "che rai le naturribile e chi perco;\n",
      "quell'animo di fu e parti.\n",
      "folsi crise omo?' crebbe trapparu veduro\n",
      "cui,"
     ]
    }
   ],
   "source": [
    "# Generate some text context from the trained model\n",
    "context = Tensor(np.zeros((1, 1), dtype=np.int32))\n",
    "\n",
    "# Iterate over the tokens generated by the transformer\n",
    "for token in language_model.generate(context, max_new_tokens=200, stream=True):\n",
    "    # Decode the token\n",
    "    decoded_token = tokenizer.decode([token.data.squeeze().tolist()])\n",
    "\n",
    "    # Print the decoded token\n",
    "    print(decoded_token, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
