{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Add the path to the custom library to the system path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import custom modules\n",
    "from src import Tensor\n",
    "from src.core.utils import data_analysis, context_manager\n",
    "from src.architectures.transformer import Tokenizer, Transformer, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dataset_path = os.path.join(os.getcwd(), 'dataset', 'divina_commedia.txt')\n",
    "tokenizer_path = os.path.join(os.getcwd(), 'checkpoints', 'tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "dropout = 0.2 # The dropout rate\n",
    "train_val_split = 0.9 # 90% of the data will be used for training, 10% for validation\n",
    "batch_size = 32 # The number of samples to use for each batch\n",
    "grad_accumulation_steps = 1 # The number of steps to accumulate gradients before updating the model\n",
    "sequence_length = 256 # The size of the sequence length (the context window)\n",
    "learning_rate = 1e-3 # The learning rate for the optimizer\n",
    "training_steps = 500 # The number of steps to train the model for\n",
    "n_embed = 384 # The size of the token embeddings (the dimensionality of the embeddings)\n",
    "eval_iters = 1 # The number of iterations to evaluate the model\n",
    "n_attention_heads = 6 # The number of attention heads in the multi-head attention mechanism\n",
    "n_decoder_blocks = 6 # The number of transformer'decoder blocks in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_file(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load a text file from the specified path.\n",
    "    \n",
    "    Parameters:\n",
    "    - path (str): The path to the text file.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The contents of the text file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f'The file \"{path}\" does not exist.')\n",
    "    \n",
    "    # Read the file\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Load the state of the tokenizer\n",
    "tokenizer.load(tokenizer_path)\n",
    "\n",
    "# Extract the vocabulary size\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "text = load_txt_file(dataset_path)\n",
    "\n",
    "# Encode the text using the tokenizer\n",
    "encoded_text = tokenizer.encode(text)\n",
    "\n",
    "# Convert the data to a tensor\n",
    "data = Tensor(np.array(encoded_text), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the data loader\n",
    "data_loader = DataLoader(\n",
    "    data = data, \n",
    "    train_val_split = train_val_split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the language model\n",
    "language_model = Transformer(\n",
    "    name = \"Language Model\",\n",
    "    vocab_size = vocab_size,\n",
    "    n_embed = n_embed,\n",
    "    n_attention_heads = n_attention_heads,\n",
    "    sequence_length = sequence_length,\n",
    "    n_decoder_blocks = n_decoder_blocks,\n",
    "    dropout = dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model with a first batch to initialize the weights\n",
    "# This is not necessary, but it is useful to know the input size\n",
    "\n",
    "# Disable gradient computation\n",
    "with context_manager.no_grad():\n",
    "    # Set the model in evaluation mode\n",
    "    language_model.eval()\n",
    "    \n",
    "    # Get a batch of data\n",
    "    x, _ = data_loader.get_batch(\n",
    "        batch_size = batch_size,\n",
    "        sequence_length = sequence_length\n",
    "    )\n",
    "    \n",
    "    # Call the model with a batch of data to initialize it\n",
    "    language_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model (Transformer) [output_shape=(32, 256, 1024), params=11526400]\n",
      "└── language_model.decoder (Decoder) [output_shape=(32, 256, 1024), params=11526400]\n",
      "    ├── decoder.embedding (Embedding) [output_shape=(32, 256, 384), params=393216]\n",
      "    ├── decoder.positional_embedding (Embedding) [output_shape=(256, 384), params=98304]\n",
      "    ├── decoder.decoder_blocks[0].decoder_block (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │   ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │   │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   └── decoder.decoder_blocks[0].decoder_block.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │       ├── multi_head_attention.heads[0].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[1].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[2].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[3].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[4].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[5].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │       └── decoder.decoder_blocks[0].decoder_block.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    ├── decoder.decoder_blocks[1].decoder_block (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │   ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │   │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   └── decoder.decoder_blocks[1].decoder_block.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │       ├── multi_head_attention.heads[0].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[1].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[2].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[3].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[4].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[5].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │       └── decoder.decoder_blocks[1].decoder_block.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    ├── decoder.decoder_blocks[2].decoder_block (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │   ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │   │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   └── decoder.decoder_blocks[2].decoder_block.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │       ├── multi_head_attention.heads[0].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[1].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[2].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[3].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[4].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[5].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │       └── decoder.decoder_blocks[2].decoder_block.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    ├── decoder.decoder_blocks[3].decoder_block (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │   ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │   │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   └── decoder.decoder_blocks[3].decoder_block.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │       ├── multi_head_attention.heads[0].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[1].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[2].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[3].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[4].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[5].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │       └── decoder.decoder_blocks[3].decoder_block.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    ├── decoder.decoder_blocks[4].decoder_block (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │   ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │   │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   └── decoder.decoder_blocks[4].decoder_block.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │       ├── multi_head_attention.heads[0].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[1].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[2].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[3].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[4].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[5].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │       └── decoder.decoder_blocks[4].decoder_block.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    ├── decoder.decoder_blocks[5].decoder_block (DecoderBlock) [output_shape=(32, 256, 384), params=1773312]\n",
      "    │   ├── decoder_block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   ├── decoder_block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "    │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │   │   ├── decoder_block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "    │   │   └── decoder_block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "    │   ├── decoder_block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    │   └── decoder.decoder_blocks[5].decoder_block.attention_heads (MultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "    │       ├── multi_head_attention.heads[0].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[1].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[2].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[3].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[4].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.heads[5].single_head_attention (SingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "    │       │   ├── single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   ├── single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "    │       │   └── single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "    │       ├── multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "    │       └── decoder.decoder_blocks[5].decoder_block.attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "    ├── decoder.layer_norm (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "    └── decoder.output_layer (Dense) [output_shape=(32, 256, 1024), params=394240]\n"
     ]
    }
   ],
   "source": [
    "# Display the model summary in tree format.\n",
    "# This is useful since the whole model is composed of submodules,\n",
    "# therefore, the model summary will be displayed recursively\n",
    "language_model.summary(recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/500 | 217 tensors in memory | 4252.17 ms/step - Train Loss: 7.0919 | Validation loss: 7.0939\n",
      "Step 2/500 | 222 tensors in memory | 4482.75 ms/step - Train Loss: 6.5550 | Validation loss: 6.6181\n",
      "Step 3/500 | 222 tensors in memory | 4458.01 ms/step - Train Loss: 5.3031 | Validation loss: 5.3529\n",
      "Step 4/500 | 222 tensors in memory | 4319.11 ms/step - Train Loss: 5.0908 | Validation loss: 5.0768\n",
      "Step 5/500 | 222 tensors in memory | 4449.71 ms/step - Train Loss: 4.7592 | Validation loss: 4.7599\n",
      "Step 6/500 | 222 tensors in memory | 4589.25 ms/step - Train Loss: 4.5209 | Validation loss: 4.5368\n",
      "Step 7/500 | 222 tensors in memory | 4696.06 ms/step - Train Loss: 4.4646 | Validation loss: 4.4661\n",
      "Step 8/500 | 222 tensors in memory | 4684.63 ms/step - Train Loss: 4.4055 | Validation loss: 4.4000\n",
      "Step 9/500 | 222 tensors in memory | 4652.98 ms/step - Train Loss: 4.3448 | Validation loss: 4.3672\n",
      "Step 10/500 | 222 tensors in memory | 4670.37 ms/step - Train Loss: 4.2924 | Validation loss: 4.2887\n",
      "Step 11/500 | 222 tensors in memory | 4634.73 ms/step - Train Loss: 4.2874 | Validation loss: 4.2980\n",
      "Step 12/500 | 222 tensors in memory | 4648.83 ms/step - Train Loss: 4.2604 | Validation loss: 4.2812\n",
      "Step 13/500 | 222 tensors in memory | 4670.07 ms/step - Train Loss: 4.2727 | Validation loss: 4.2286\n",
      "Step 14/500 | 222 tensors in memory | 4683.36 ms/step - Train Loss: 4.2384 | Validation loss: 4.2767\n",
      "Step 15/500 | 222 tensors in memory | 4714.40 ms/step - Train Loss: 4.2334 | Validation loss: 4.2076\n",
      "Step 16/500 | 222 tensors in memory | 4725.59 ms/step - Train Loss: 4.1969 | Validation loss: 4.2009\n",
      "Step 17/500 | 222 tensors in memory | 4719.24 ms/step - Train Loss: 4.1656 | Validation loss: 4.2095\n",
      "Step 18/500 | 222 tensors in memory | 4716.69 ms/step - Train Loss: 4.1186 | Validation loss: 4.1616\n",
      "Step 19/500 | 222 tensors in memory | 4722.59 ms/step - Train Loss: 4.1141 | Validation loss: 4.1308\n",
      "Step 20/500 | 222 tensors in memory | 4750.14 ms/step - Train Loss: 4.0978 | Validation loss: 4.1152\n",
      "Step 21/500 | 222 tensors in memory | 4757.97 ms/step - Train Loss: 4.0384 | Validation loss: 4.0771\n",
      "Step 22/500 | 222 tensors in memory | 4756.10 ms/step - Train Loss: 4.0331 | Validation loss: 4.0827\n",
      "Step 23/500 | 222 tensors in memory | 4756.38 ms/step - Train Loss: 4.0436 | Validation loss: 4.0843\n",
      "Step 24/500 | 222 tensors in memory | 4753.85 ms/step - Train Loss: 4.0217 | Validation loss: 4.0448\n",
      "Step 25/500 | 222 tensors in memory | 4761.91 ms/step - Train Loss: 3.9970 | Validation loss: 4.0211\n",
      "Step 26/500 | 222 tensors in memory | 4773.65 ms/step - Train Loss: 3.9628 | Validation loss: 3.9652\n",
      "Step 27/500 | 222 tensors in memory | 4782.34 ms/step - Train Loss: 3.9814 | Validation loss: 3.9361\n",
      "Step 28/500 | 222 tensors in memory | 4796.21 ms/step - Train Loss: 3.9235 | Validation loss: 4.0084\n",
      "Step 29/500 | 222 tensors in memory | 4802.67 ms/step - Train Loss: 3.9463 | Validation loss: 3.9346\n",
      "Step 30/500 | 222 tensors in memory | 4796.49 ms/step - Train Loss: 3.9053 | Validation loss: 3.8842\n",
      "Step 31/500 | 222 tensors in memory | 4791.10 ms/step - Train Loss: 3.9121 | Validation loss: 3.9004\n",
      "Step 32/500 | 222 tensors in memory | 4791.36 ms/step - Train Loss: 3.8844 | Validation loss: 3.8549\n",
      "Step 33/500 | 222 tensors in memory | 4784.60 ms/step - Train Loss: 3.8608 | Validation loss: 3.8516\n",
      "Step 34/500 | 222 tensors in memory | 4764.83 ms/step - Train Loss: 3.8559 | Validation loss: 3.8739\n",
      "Step 35/500 | 222 tensors in memory | 4756.22 ms/step - Train Loss: 3.8382 | Validation loss: 3.8147\n",
      "Step 36/500 | 222 tensors in memory | 4753.10 ms/step - Train Loss: 3.8868 | Validation loss: 3.8156\n",
      "Step 37/500 | 222 tensors in memory | 4743.29 ms/step - Train Loss: 3.8183 | Validation loss: 3.8527\n",
      "Step 38/500 | 222 tensors in memory | 4740.35 ms/step - Train Loss: 3.8241 | Validation loss: 3.8122\n",
      "Step 39/500 | 222 tensors in memory | 4729.45 ms/step - Train Loss: 3.8047 | Validation loss: 3.8159\n",
      "Step 40/500 | 222 tensors in memory | 4732.92 ms/step - Train Loss: 3.8569 | Validation loss: 3.8004\n",
      "Step 41/500 | 222 tensors in memory | 4720.32 ms/step - Train Loss: 3.8528 | Validation loss: 3.7923\n",
      "Step 42/500 | 222 tensors in memory | 4710.27 ms/step - Train Loss: 3.7571 | Validation loss: 3.7867\n",
      "Step 43/500 | 222 tensors in memory | 4716.84 ms/step - Train Loss: 3.7795 | Validation loss: 3.8078\n",
      "Step 44/500 | 222 tensors in memory | 4725.94 ms/step - Train Loss: 3.7683 | Validation loss: 3.7883\n",
      "Step 45/500 | 222 tensors in memory | 4727.62 ms/step - Train Loss: 3.7759 | Validation loss: 3.7920\n",
      "Step 46/500 | 222 tensors in memory | 4730.03 ms/step - Train Loss: 3.7610 | Validation loss: 3.7693\n",
      "Step 47/500 | 222 tensors in memory | 4717.48 ms/step - Train Loss: 3.7445 | Validation loss: 3.7265\n",
      "Step 48/500 | 222 tensors in memory | 4695.36 ms/step - Train Loss: 3.7434 | Validation loss: 3.7394\n",
      "Step 49/500 | 222 tensors in memory | 4674.33 ms/step - Train Loss: 3.7477 | Validation loss: 3.7552\n",
      "Step 50/500 | 222 tensors in memory | 4655.38 ms/step - Train Loss: 3.6915 | Validation loss: 3.7333\n",
      "Step 51/500 | 222 tensors in memory | 4646.63 ms/step - Train Loss: 3.7298 | Validation loss: 3.7389\n",
      "Step 52/500 | 222 tensors in memory | 4638.20 ms/step - Train Loss: 3.7032 | Validation loss: 3.7334\n",
      "Step 53/500 | 222 tensors in memory | 4622.16 ms/step - Train Loss: 3.7017 | Validation loss: 3.7174\n",
      "Step 54/500 | 222 tensors in memory | 4603.43 ms/step - Train Loss: 3.6967 | Validation loss: 3.7079\n",
      "Step 55/500 | 222 tensors in memory | 4592.03 ms/step - Train Loss: 3.6865 | Validation loss: 3.6893\n",
      "Step 56/500 | 222 tensors in memory | 4579.01 ms/step - Train Loss: 3.7190 | Validation loss: 3.6695\n",
      "Step 57/500 | 222 tensors in memory | 4568.68 ms/step - Train Loss: 3.6788 | Validation loss: 3.6660\n",
      "Step 58/500 | 222 tensors in memory | 4570.05 ms/step - Train Loss: 3.6453 | Validation loss: 3.6313\n",
      "Step 59/500 | 222 tensors in memory | 4575.61 ms/step - Train Loss: 3.6552 | Validation loss: 3.6498\n",
      "Step 60/500 | 222 tensors in memory | 4571.93 ms/step - Train Loss: 3.6526 | Validation loss: 3.6599\n",
      "Step 61/500 | 222 tensors in memory | 4571.14 ms/step - Train Loss: 3.6044 | Validation loss: 3.6302\n",
      "Step 62/500 | 222 tensors in memory | 4571.18 ms/step - Train Loss: 3.6459 | Validation loss: 3.6425\n",
      "Step 63/500 | 222 tensors in memory | 4569.68 ms/step - Train Loss: 3.6292 | Validation loss: 3.5855\n",
      "Step 64/500 | 222 tensors in memory | 4576.80 ms/step - Train Loss: 3.6170 | Validation loss: 3.6373\n",
      "Step 65/500 | 222 tensors in memory | 4578.46 ms/step - Train Loss: 3.6826 | Validation loss: 3.6607\n",
      "Step 66/500 | 222 tensors in memory | 4578.12 ms/step - Train Loss: 3.6357 | Validation loss: 3.5909\n",
      "Step 67/500 | 222 tensors in memory | 4576.78 ms/step - Train Loss: 3.6342 | Validation loss: 3.5943\n",
      "Step 68/500 | 222 tensors in memory | 4572.22 ms/step - Train Loss: 3.5971 | Validation loss: 3.5699\n",
      "Step 69/500 | 222 tensors in memory | 4570.43 ms/step - Train Loss: 3.5646 | Validation loss: 3.5933\n",
      "Step 70/500 | 222 tensors in memory | 4568.56 ms/step - Train Loss: 3.5239 | Validation loss: 3.5677\n",
      "Step 71/500 | 222 tensors in memory | 4563.60 ms/step - Train Loss: 3.5323 | Validation loss: 3.5533\n",
      "Step 72/500 | 222 tensors in memory | 4556.12 ms/step - Train Loss: 3.5493 | Validation loss: 3.5809\n",
      "Step 73/500 | 222 tensors in memory | 4546.56 ms/step - Train Loss: 3.5366 | Validation loss: 3.5725\n",
      "Step 74/500 | 222 tensors in memory | 4558.33 ms/step - Train Loss: 3.5146 | Validation loss: 3.5514\n",
      "Step 75/500 | 222 tensors in memory | 4565.08 ms/step - Train Loss: 3.5500 | Validation loss: 3.5223\n",
      "Step 76/500 | 222 tensors in memory | 4569.38 ms/step - Train Loss: 3.5463 | Validation loss: 3.5117\n",
      "Step 77/500 | 222 tensors in memory | 4570.28 ms/step - Train Loss: 3.4984 | Validation loss: 3.4774\n",
      "Step 78/500 | 222 tensors in memory | 4573.51 ms/step - Train Loss: 3.5042 | Validation loss: 3.5167\n",
      "Step 79/500 | 222 tensors in memory | 4574.61 ms/step - Train Loss: 3.4988 | Validation loss: 3.5226\n",
      "Step 80/500 | 222 tensors in memory | 4576.85 ms/step - Train Loss: 3.5220 | Validation loss: 3.4655\n",
      "Step 81/500 | 222 tensors in memory | 4569.08 ms/step - Train Loss: 3.4936 | Validation loss: 3.5126\n",
      "Step 82/500 | 222 tensors in memory | 4571.88 ms/step - Train Loss: 3.4816 | Validation loss: 3.4679\n",
      "Step 83/500 | 222 tensors in memory | 4566.46 ms/step - Train Loss: 3.4607 | Validation loss: 3.4528\n",
      "Step 84/500 | 222 tensors in memory | 4556.21 ms/step - Train Loss: 3.5098 | Validation loss: 3.4372\n",
      "Step 85/500 | 222 tensors in memory | 4545.37 ms/step - Train Loss: 3.4551 | Validation loss: 3.4573\n",
      "Step 86/500 | 222 tensors in memory | 4538.94 ms/step - Train Loss: 3.4495 | Validation loss: 3.4720\n",
      "Step 87/500 | 222 tensors in memory | 4531.34 ms/step - Train Loss: 3.4588 | Validation loss: 3.4217\n",
      "Step 88/500 | 222 tensors in memory | 4524.39 ms/step - Train Loss: 3.3815 | Validation loss: 3.4989\n",
      "Step 89/500 | 222 tensors in memory | 4518.33 ms/step - Train Loss: 3.4310 | Validation loss: 3.4648\n",
      "Step 90/500 | 222 tensors in memory | 4507.68 ms/step - Train Loss: 3.4063 | Validation loss: 3.4040\n",
      "Step 91/500 | 222 tensors in memory | 4499.69 ms/step - Train Loss: 3.4337 | Validation loss: 3.4323\n",
      "Step 92/500 | 222 tensors in memory | 4504.28 ms/step - Train Loss: 3.3946 | Validation loss: 3.3646\n",
      "Step 93/500 | 222 tensors in memory | 4502.03 ms/step - Train Loss: 3.4028 | Validation loss: 3.3737\n",
      "Step 94/500 | 222 tensors in memory | 4497.80 ms/step - Train Loss: 3.3804 | Validation loss: 3.4058\n",
      "Step 95/500 | 222 tensors in memory | 4501.87 ms/step - Train Loss: 3.4030 | Validation loss: 3.3646\n",
      "Step 96/500 | 222 tensors in memory | 4504.88 ms/step - Train Loss: 3.3933 | Validation loss: 3.4109\n",
      "Step 97/500 | 222 tensors in memory | 4510.58 ms/step - Train Loss: 3.3248 | Validation loss: 3.3758\n",
      "Step 98/500 | 222 tensors in memory | 4512.87 ms/step - Train Loss: 3.3806 | Validation loss: 3.3801\n",
      "Step 99/500 | 222 tensors in memory | 4511.75 ms/step - Train Loss: 3.3367 | Validation loss: 3.4180\n",
      "Step 100/500 | 222 tensors in memory | 4510.19 ms/step - Train Loss: 3.3487 | Validation loss: 3.3211\n",
      "Step 101/500 | 222 tensors in memory | 4504.89 ms/step - Train Loss: 3.3445 | Validation loss: 3.4082\n",
      "Step 102/500 | 222 tensors in memory | 4496.22 ms/step - Train Loss: 3.3544 | Validation loss: 3.3621\n",
      "Step 103/500 | 222 tensors in memory | 4498.33 ms/step - Train Loss: 3.3321 | Validation loss: 3.3206\n",
      "Step 104/500 | 222 tensors in memory | 4498.80 ms/step - Train Loss: 3.3129 | Validation loss: 3.3379\n",
      "Step 105/500 | 222 tensors in memory | 4496.98 ms/step - Train Loss: 3.3128 | Validation loss: 3.3211\n",
      "Step 106/500 | 222 tensors in memory | 4495.94 ms/step - Train Loss: 3.3072 | Validation loss: 3.3607\n",
      "Step 107/500 | 222 tensors in memory | 4494.09 ms/step - Train Loss: 3.2807 | Validation loss: 3.3486\n",
      "Step 108/500 | 222 tensors in memory | 4487.67 ms/step - Train Loss: 3.3044 | Validation loss: 3.3117\n",
      "Step 109/500 | 222 tensors in memory | 4483.81 ms/step - Train Loss: 3.3143 | Validation loss: 3.3432\n",
      "Step 110/500 | 222 tensors in memory | 4477.19 ms/step - Train Loss: 3.3398 | Validation loss: 3.3280\n",
      "Step 111/500 | 222 tensors in memory | 4477.93 ms/step - Train Loss: 3.3371 | Validation loss: 3.3197\n",
      "Step 112/500 | 222 tensors in memory | 4476.67 ms/step - Train Loss: 3.3190 | Validation loss: 3.3783\n",
      "Step 113/500 | 222 tensors in memory | 4476.73 ms/step - Train Loss: 3.3035 | Validation loss: 3.3390\n",
      "Step 114/500 | 222 tensors in memory | 4479.21 ms/step - Train Loss: 3.2978 | Validation loss: 3.3182\n",
      "Step 115/500 | 222 tensors in memory | 4477.73 ms/step - Train Loss: 3.3092 | Validation loss: 3.2894\n",
      "Step 116/500 | 222 tensors in memory | 4472.28 ms/step - Train Loss: 3.3219 | Validation loss: 3.2618\n",
      "Step 117/500 | 222 tensors in memory | 4466.03 ms/step - Train Loss: 3.2828 | Validation loss: 3.3378\n",
      "Step 118/500 | 222 tensors in memory | 4463.04 ms/step - Train Loss: 3.2967 | Validation loss: 3.2944\n",
      "Step 119/500 | 222 tensors in memory | 4459.93 ms/step - Train Loss: 3.2383 | Validation loss: 3.2893\n",
      "Step 120/500 | 222 tensors in memory | 4460.38 ms/step - Train Loss: 3.2931 | Validation loss: 3.2594\n",
      "Step 121/500 | 222 tensors in memory | 4457.26 ms/step - Train Loss: 3.2891 | Validation loss: 3.2784\n",
      "Step 122/500 | 222 tensors in memory | 4458.92 ms/step - Train Loss: 3.2931 | Validation loss: 3.2791\n",
      "Step 123/500 | 222 tensors in memory | 4458.02 ms/step - Train Loss: 3.2523 | Validation loss: 3.3301\n",
      "Step 124/500 | 222 tensors in memory | 4463.83 ms/step - Train Loss: 3.2234 | Validation loss: 3.2540\n",
      "Step 125/500 | 222 tensors in memory | 4466.73 ms/step - Train Loss: 3.2751 | Validation loss: 3.2844\n",
      "Step 126/500 | 222 tensors in memory | 4466.61 ms/step - Train Loss: 3.2399 | Validation loss: 3.2907\n",
      "Step 127/500 | 222 tensors in memory | 4464.35 ms/step - Train Loss: 3.2310 | Validation loss: 3.2678\n",
      "Step 128/500 | 222 tensors in memory | 4461.23 ms/step - Train Loss: 3.2780 | Validation loss: 3.2805\n",
      "Step 129/500 | 222 tensors in memory | 4454.65 ms/step - Train Loss: 3.2388 | Validation loss: 3.2196\n",
      "Step 130/500 | 222 tensors in memory | 4451.45 ms/step - Train Loss: 3.2166 | Validation loss: 3.2254\n",
      "Step 131/500 | 222 tensors in memory | 4447.90 ms/step - Train Loss: 3.2450 | Validation loss: 3.2672\n",
      "Step 132/500 | 222 tensors in memory | 4444.21 ms/step - Train Loss: 3.2110 | Validation loss: 3.2904\n",
      "Step 133/500 | 222 tensors in memory | 4441.88 ms/step - Train Loss: 3.2210 | Validation loss: 3.2746\n",
      "Step 134/500 | 222 tensors in memory | 4442.69 ms/step - Train Loss: 3.2141 | Validation loss: 3.2542\n",
      "Step 135/500 | 222 tensors in memory | 4443.55 ms/step - Train Loss: 3.2456 | Validation loss: 3.2170\n",
      "Step 136/500 | 222 tensors in memory | 4446.05 ms/step - Train Loss: 3.2036 | Validation loss: 3.2282\n",
      "Step 137/500 | 222 tensors in memory | 4441.72 ms/step - Train Loss: 3.2504 | Validation loss: 3.2424\n",
      "Step 138/500 | 222 tensors in memory | 4436.67 ms/step - Train Loss: 3.1989 | Validation loss: 3.2263\n",
      "Step 139/500 | 222 tensors in memory | 4430.45 ms/step - Train Loss: 3.2288 | Validation loss: 3.2078\n",
      "Step 140/500 | 222 tensors in memory | 4423.37 ms/step - Train Loss: 3.1880 | Validation loss: 3.2101\n",
      "Step 141/500 | 222 tensors in memory | 4420.93 ms/step - Train Loss: 3.1853 | Validation loss: 3.2129\n",
      "Step 142/500 | 222 tensors in memory | 4415.72 ms/step - Train Loss: 3.1382 | Validation loss: 3.2510\n",
      "Step 143/500 | 222 tensors in memory | 4410.03 ms/step - Train Loss: 3.1929 | Validation loss: 3.1990\n",
      "Step 144/500 | 222 tensors in memory | 4405.97 ms/step - Train Loss: 3.1973 | Validation loss: 3.2275\n",
      "Step 145/500 | 222 tensors in memory | 4402.79 ms/step - Train Loss: 3.1775 | Validation loss: 3.2205\n",
      "Step 146/500 | 222 tensors in memory | 4400.15 ms/step - Train Loss: 3.1828 | Validation loss: 3.2049\n",
      "Step 147/500 | 222 tensors in memory | 4395.92 ms/step - Train Loss: 3.1455 | Validation loss: 3.1899\n",
      "Step 148/500 | 222 tensors in memory | 4395.44 ms/step - Train Loss: 3.1776 | Validation loss: 3.2526\n",
      "Step 149/500 | 222 tensors in memory | 4394.84 ms/step - Train Loss: 3.1448 | Validation loss: 3.2370\n",
      "Step 150/500 | 222 tensors in memory | 4392.57 ms/step - Train Loss: 3.1287 | Validation loss: 3.2068\n",
      "Step 151/500 | 222 tensors in memory | 4391.52 ms/step - Train Loss: 3.1941 | Validation loss: 3.2153\n",
      "Step 152/500 | 222 tensors in memory | 4389.18 ms/step - Train Loss: 3.1407 | Validation loss: 3.2040\n",
      "Step 153/500 | 222 tensors in memory | 4384.33 ms/step - Train Loss: 3.1421 | Validation loss: 3.1731\n",
      "Step 154/500 | 222 tensors in memory | 4379.16 ms/step - Train Loss: 3.1598 | Validation loss: 3.2270\n",
      "Step 155/500 | 222 tensors in memory | 4379.73 ms/step - Train Loss: 3.1173 | Validation loss: 3.1726\n",
      "Step 156/500 | 222 tensors in memory | 4376.13 ms/step - Train Loss: 3.1466 | Validation loss: 3.1917\n",
      "Step 157/500 | 222 tensors in memory | 4371.45 ms/step - Train Loss: 3.1325 | Validation loss: 3.2353\n",
      "Step 158/500 | 222 tensors in memory | 4369.40 ms/step - Train Loss: 3.1212 | Validation loss: 3.1493\n",
      "Step 159/500 | 222 tensors in memory | 4366.37 ms/step - Train Loss: 3.1595 | Validation loss: 3.1945\n",
      "Step 160/500 | 222 tensors in memory | 4362.39 ms/step - Train Loss: 3.1700 | Validation loss: 3.1581\n",
      "Step 161/500 | 222 tensors in memory | 4358.00 ms/step - Train Loss: 3.1508 | Validation loss: 3.1611\n",
      "Step 162/500 | 222 tensors in memory | 4355.34 ms/step - Train Loss: 3.0811 | Validation loss: 3.1209\n",
      "Step 163/500 | 222 tensors in memory | 4352.80 ms/step - Train Loss: 3.1747 | Validation loss: 3.1956\n",
      "Step 164/500 | 222 tensors in memory | 4348.64 ms/step - Train Loss: 3.1276 | Validation loss: 3.1669\n",
      "Step 165/500 | 222 tensors in memory | 4347.49 ms/step - Train Loss: 3.1605 | Validation loss: 3.1597\n",
      "Step 166/500 | 222 tensors in memory | 4344.84 ms/step - Train Loss: 3.1342 | Validation loss: 3.1269\n",
      "Step 167/500 | 222 tensors in memory | 4341.30 ms/step - Train Loss: 3.1313 | Validation loss: 3.1776\n",
      "Step 168/500 | 222 tensors in memory | 4337.92 ms/step - Train Loss: 3.1493 | Validation loss: 3.1495\n",
      "Step 169/500 | 222 tensors in memory | 4334.03 ms/step - Train Loss: 3.1225 | Validation loss: 3.1398\n",
      "Step 170/500 | 222 tensors in memory | 4334.51 ms/step - Train Loss: 3.1155 | Validation loss: 3.1400\n",
      "Step 171/500 | 222 tensors in memory | 4337.65 ms/step - Train Loss: 3.1249 | Validation loss: 3.1280\n",
      "Step 172/500 | 222 tensors in memory | 4337.63 ms/step - Train Loss: 3.1370 | Validation loss: 3.1765\n",
      "Step 173/500 | 222 tensors in memory | 4338.34 ms/step - Train Loss: 3.1170 | Validation loss: 3.1860\n",
      "Step 174/500 | 222 tensors in memory | 4337.43 ms/step - Train Loss: 3.1100 | Validation loss: 3.1508\n",
      "Step 175/500 | 222 tensors in memory | 4338.76 ms/step - Train Loss: 3.1176 | Validation loss: 3.1640\n",
      "Step 176/500 | 222 tensors in memory | 4342.28 ms/step - Train Loss: 3.1621 | Validation loss: 3.1373\n",
      "Step 177/500 | 222 tensors in memory | 4342.12 ms/step - Train Loss: 3.0915 | Validation loss: 3.1113\n",
      "Step 178/500 | 222 tensors in memory | 4340.83 ms/step - Train Loss: 3.1469 | Validation loss: 3.1215\n",
      "Step 179/500 | 222 tensors in memory | 4339.00 ms/step - Train Loss: 3.1335 | Validation loss: 3.1291\n",
      "Step 180/500 | 222 tensors in memory | 4337.65 ms/step - Train Loss: 3.0713 | Validation loss: 3.1406\n",
      "Step 181/500 | 222 tensors in memory | 4337.00 ms/step - Train Loss: 3.1026 | Validation loss: 3.0880\n",
      "Step 182/500 | 222 tensors in memory | 4335.55 ms/step - Train Loss: 3.1438 | Validation loss: 3.1433\n",
      "Step 183/500 | 222 tensors in memory | 4334.65 ms/step - Train Loss: 3.1157 | Validation loss: 3.1682\n",
      "Step 184/500 | 222 tensors in memory | 4333.53 ms/step - Train Loss: 3.0853 | Validation loss: 3.1711\n",
      "Step 185/500 | 222 tensors in memory | 4332.56 ms/step - Train Loss: 3.0645 | Validation loss: 3.1279\n",
      "Step 186/500 | 222 tensors in memory | 4330.08 ms/step - Train Loss: 3.0753 | Validation loss: 3.1619\n",
      "Step 187/500 | 222 tensors in memory | 4329.46 ms/step - Train Loss: 3.0986 | Validation loss: 3.1388\n",
      "Step 188/500 | 222 tensors in memory | 4328.54 ms/step - Train Loss: 3.0998 | Validation loss: 3.1109\n",
      "Step 189/500 | 222 tensors in memory | 4327.25 ms/step - Train Loss: 3.1116 | Validation loss: 3.1458\n",
      "Step 190/500 | 222 tensors in memory | 4327.03 ms/step - Train Loss: 3.1161 | Validation loss: 3.1438\n",
      "Step 191/500 | 222 tensors in memory | 4327.52 ms/step - Train Loss: 3.1007 | Validation loss: 3.1268\n",
      "Step 192/500 | 222 tensors in memory | 4326.98 ms/step - Train Loss: 3.0897 | Validation loss: 3.1052\n",
      "Step 193/500 | 222 tensors in memory | 4325.82 ms/step - Train Loss: 3.0766 | Validation loss: 3.1267\n",
      "Step 194/500 | 222 tensors in memory | 4324.40 ms/step - Train Loss: 3.0904 | Validation loss: 3.1023\n",
      "Step 195/500 | 222 tensors in memory | 4323.33 ms/step - Train Loss: 3.1074 | Validation loss: 3.1572\n",
      "Step 196/500 | 222 tensors in memory | 4322.53 ms/step - Train Loss: 3.1085 | Validation loss: 3.1022\n",
      "Step 197/500 | 222 tensors in memory | 4321.36 ms/step - Train Loss: 3.0935 | Validation loss: 3.1309\n",
      "Step 198/500 | 222 tensors in memory | 4323.95 ms/step - Train Loss: 3.1243 | Validation loss: 3.1243\n",
      "Step 199/500 | 222 tensors in memory | 4323.01 ms/step - Train Loss: 3.0907 | Validation loss: 3.1254\n",
      "Step 200/500 | 222 tensors in memory | 4321.82 ms/step - Train Loss: 3.0919 | Validation loss: 3.1272\n",
      "Step 201/500 | 222 tensors in memory | 4322.95 ms/step - Train Loss: 3.0957 | Validation loss: 3.1273\n",
      "Step 202/500 | 222 tensors in memory | 4324.01 ms/step - Train Loss: 3.0789 | Validation loss: 3.1154\n",
      "Step 203/500 | 222 tensors in memory | 4323.51 ms/step - Train Loss: 3.1167 | Validation loss: 3.1384\n",
      "Step 204/500 | 222 tensors in memory | 4324.80 ms/step - Train Loss: 3.0707 | Validation loss: 3.1152\n",
      "Step 205/500 | 222 tensors in memory | 4323.74 ms/step - Train Loss: 3.1210 | Validation loss: 3.0947\n",
      "Step 206/500 | 222 tensors in memory | 4322.20 ms/step - Train Loss: 3.0929 | Validation loss: 3.1008\n",
      "Step 207/500 | 222 tensors in memory | 4319.82 ms/step - Train Loss: 3.0665 | Validation loss: 3.1021\n",
      "Step 208/500 | 222 tensors in memory | 4318.86 ms/step - Train Loss: 3.1144 | Validation loss: 3.0682\n",
      "Step 209/500 | 222 tensors in memory | 4318.86 ms/step - Train Loss: 3.0595 | Validation loss: 3.0937\n",
      "Step 210/500 | 222 tensors in memory | 4318.11 ms/step - Train Loss: 3.0946 | Validation loss: 3.1028\n",
      "Step 211/500 | 222 tensors in memory | 4317.92 ms/step - Train Loss: 3.0839 | Validation loss: 3.1243\n",
      "Step 212/500 | 222 tensors in memory | 4322.79 ms/step - Train Loss: 3.0089 | Validation loss: 3.0937\n",
      "Step 213/500 | 222 tensors in memory | 4322.01 ms/step - Train Loss: 3.0661 | Validation loss: 3.1314\n",
      "Step 214/500 | 222 tensors in memory | 4321.34 ms/step - Train Loss: 3.0528 | Validation loss: 3.1214\n",
      "Step 215/500 | 222 tensors in memory | 4319.91 ms/step - Train Loss: 3.0892 | Validation loss: 3.0806\n",
      "Step 216/500 | 222 tensors in memory | 4317.78 ms/step - Train Loss: 3.0435 | Validation loss: 3.1092\n",
      "Step 217/500 | 222 tensors in memory | 4315.79 ms/step - Train Loss: 3.0431 | Validation loss: 3.1161\n",
      "Step 218/500 | 222 tensors in memory | 4313.70 ms/step - Train Loss: 3.0616 | Validation loss: 3.1298\n",
      "Step 219/500 | 222 tensors in memory | 4311.94 ms/step - Train Loss: 3.0349 | Validation loss: 3.0851\n",
      "Step 220/500 | 222 tensors in memory | 4310.92 ms/step - Train Loss: 3.0704 | Validation loss: 3.0740\n",
      "Step 221/500 | 222 tensors in memory | 4309.57 ms/step - Train Loss: 3.0611 | Validation loss: 3.1170\n",
      "Step 222/500 | 222 tensors in memory | 4308.26 ms/step - Train Loss: 3.0953 | Validation loss: 3.1244\n",
      "Step 223/500 | 222 tensors in memory | 4308.32 ms/step - Train Loss: 3.0460 | Validation loss: 3.1116\n",
      "Step 224/500 | 222 tensors in memory | 4306.23 ms/step - Train Loss: 3.0345 | Validation loss: 3.1034\n",
      "Step 225/500 | 222 tensors in memory | 4305.10 ms/step - Train Loss: 3.0303 | Validation loss: 3.1088\n",
      "Step 226/500 | 222 tensors in memory | 4304.11 ms/step - Train Loss: 3.0523 | Validation loss: 3.1334\n",
      "Step 227/500 | 222 tensors in memory | 4302.71 ms/step - Train Loss: 3.0738 | Validation loss: 3.1012\n",
      "Step 228/500 | 222 tensors in memory | 4303.59 ms/step - Train Loss: 3.0531 | Validation loss: 3.0945\n",
      "Step 229/500 | 222 tensors in memory | 4304.13 ms/step - Train Loss: 3.0537 | Validation loss: 3.0986\n",
      "Step 230/500 | 222 tensors in memory | 4303.51 ms/step - Train Loss: 3.0346 | Validation loss: 3.0892\n",
      "Step 231/500 | 222 tensors in memory | 4303.71 ms/step - Train Loss: 3.0103 | Validation loss: 3.1023\n",
      "Step 232/500 | 222 tensors in memory | 4302.45 ms/step - Train Loss: 3.0681 | Validation loss: 3.0829\n",
      "Step 233/500 | 222 tensors in memory | 4299.69 ms/step - Train Loss: 3.1039 | Validation loss: 3.0962\n",
      "Step 234/500 | 222 tensors in memory | 4297.39 ms/step - Train Loss: 3.0717 | Validation loss: 3.1192\n",
      "Step 235/500 | 222 tensors in memory | 4296.79 ms/step - Train Loss: 3.1052 | Validation loss: 3.1165\n",
      "Step 236/500 | 222 tensors in memory | 4295.53 ms/step - Train Loss: 3.0279 | Validation loss: 3.1092\n",
      "Step 237/500 | 222 tensors in memory | 4294.95 ms/step - Train Loss: 3.0442 | Validation loss: 3.0798\n",
      "Step 238/500 | 222 tensors in memory | 4292.68 ms/step - Train Loss: 3.0647 | Validation loss: 3.0942\n",
      "Step 239/500 | 222 tensors in memory | 4290.64 ms/step - Train Loss: 3.0885 | Validation loss: 3.1169\n",
      "Step 240/500 | 222 tensors in memory | 4288.22 ms/step - Train Loss: 3.0445 | Validation loss: 3.0731\n",
      "Step 241/500 | 222 tensors in memory | 4285.85 ms/step - Train Loss: 3.0418 | Validation loss: 3.0909\n",
      "Step 242/500 | 222 tensors in memory | 4285.12 ms/step - Train Loss: 3.0462 | Validation loss: 3.1074\n",
      "Step 243/500 | 222 tensors in memory | 4285.34 ms/step - Train Loss: 3.0609 | Validation loss: 3.0849\n",
      "Step 244/500 | 222 tensors in memory | 4283.28 ms/step - Train Loss: 3.0371 | Validation loss: 3.0815\n",
      "Step 245/500 | 222 tensors in memory | 4280.12 ms/step - Train Loss: 3.0158 | Validation loss: 3.0665\n",
      "Step 246/500 | 222 tensors in memory | 4277.07 ms/step - Train Loss: 3.0260 | Validation loss: 3.0746\n",
      "Step 247/500 | 222 tensors in memory | 4276.29 ms/step - Train Loss: 3.0800 | Validation loss: 3.1265\n",
      "Step 248/500 | 222 tensors in memory | 4275.08 ms/step - Train Loss: 3.0281 | Validation loss: 3.0965\n",
      "Step 249/500 | 222 tensors in memory | 4272.88 ms/step - Train Loss: 3.0429 | Validation loss: 3.1049\n",
      "Step 250/500 | 222 tensors in memory | 4270.85 ms/step - Train Loss: 3.0623 | Validation loss: 3.1170\n",
      "Step 251/500 | 222 tensors in memory | 4269.60 ms/step - Train Loss: 3.0851 | Validation loss: 3.0718\n",
      "Step 252/500 | 222 tensors in memory | 4267.82 ms/step - Train Loss: 3.0114 | Validation loss: 3.1135\n",
      "Step 253/500 | 222 tensors in memory | 4266.19 ms/step - Train Loss: 3.0642 | Validation loss: 3.0683\n",
      "Step 254/500 | 222 tensors in memory | 4263.95 ms/step - Train Loss: 3.0566 | Validation loss: 3.0635\n",
      "Step 255/500 | 222 tensors in memory | 4261.87 ms/step - Train Loss: 3.0288 | Validation loss: 3.0632\n",
      "Step 256/500 | 222 tensors in memory | 4260.31 ms/step - Train Loss: 3.0686 | Validation loss: 3.0597\n",
      "Step 257/500 | 222 tensors in memory | 4258.77 ms/step - Train Loss: 3.0459 | Validation loss: 3.1285\n",
      "Step 258/500 | 222 tensors in memory | 4257.05 ms/step - Train Loss: 3.1162 | Validation loss: 3.0428\n",
      "Step 259/500 | 222 tensors in memory | 4258.90 ms/step - Train Loss: 3.0908 | Validation loss: 3.0431\n",
      "Step 260/500 | 222 tensors in memory | 4258.55 ms/step - Train Loss: 3.0207 | Validation loss: 3.0838\n",
      "Step 261/500 | 222 tensors in memory | 4257.75 ms/step - Train Loss: 3.0170 | Validation loss: 3.0720\n",
      "Step 262/500 | 222 tensors in memory | 4256.77 ms/step - Train Loss: 3.1000 | Validation loss: 3.1138\n",
      "Step 263/500 | 222 tensors in memory | 4258.06 ms/step - Train Loss: 3.0200 | Validation loss: 3.0777\n",
      "Step 264/500 | 222 tensors in memory | 4257.31 ms/step - Train Loss: 3.0268 | Validation loss: 3.0720\n",
      "Step 265/500 | 222 tensors in memory | 4255.93 ms/step - Train Loss: 3.0578 | Validation loss: 3.0576\n",
      "Step 266/500 | 222 tensors in memory | 4254.58 ms/step - Train Loss: 3.0273 | Validation loss: 3.0966\n",
      "Step 267/500 | 222 tensors in memory | 4254.67 ms/step - Train Loss: 3.0506 | Validation loss: 3.0704\n",
      "Step 268/500 | 222 tensors in memory | 4252.61 ms/step - Train Loss: 3.0632 | Validation loss: 3.0587\n",
      "Step 269/500 | 222 tensors in memory | 4252.53 ms/step - Train Loss: 3.0243 | Validation loss: 3.0726\n",
      "Step 270/500 | 222 tensors in memory | 4250.94 ms/step - Train Loss: 3.0585 | Validation loss: 3.0620\n",
      "Step 271/500 | 222 tensors in memory | 4249.48 ms/step - Train Loss: 3.0302 | Validation loss: 3.0731\n",
      "Step 272/500 | 222 tensors in memory | 4247.55 ms/step - Train Loss: 3.0438 | Validation loss: 3.0821\n",
      "Step 273/500 | 222 tensors in memory | 4246.02 ms/step - Train Loss: 3.0387 | Validation loss: 2.9954\n",
      "Step 274/500 | 222 tensors in memory | 4243.87 ms/step - Train Loss: 3.0247 | Validation loss: 3.0630\n",
      "Step 275/500 | 222 tensors in memory | 4241.49 ms/step - Train Loss: 3.0504 | Validation loss: 3.0976\n",
      "Step 276/500 | 222 tensors in memory | 4240.70 ms/step - Train Loss: 3.0344 | Validation loss: 3.0973\n",
      "Step 277/500 | 222 tensors in memory | 4239.69 ms/step - Train Loss: 3.0620 | Validation loss: 3.0543\n",
      "Step 278/500 | 222 tensors in memory | 4240.67 ms/step - Train Loss: 2.9847 | Validation loss: 3.0516\n",
      "Step 279/500 | 222 tensors in memory | 4239.14 ms/step - Train Loss: 3.0388 | Validation loss: 3.0923\n",
      "Step 280/500 | 222 tensors in memory | 4237.88 ms/step - Train Loss: 3.0292 | Validation loss: 3.0996\n",
      "Step 281/500 | 222 tensors in memory | 4236.81 ms/step - Train Loss: 3.0249 | Validation loss: 3.0676\n",
      "Step 282/500 | 222 tensors in memory | 4236.89 ms/step - Train Loss: 3.0172 | Validation loss: 3.0993\n",
      "Step 283/500 | 222 tensors in memory | 4235.47 ms/step - Train Loss: 3.0209 | Validation loss: 3.0561\n",
      "Step 284/500 | 222 tensors in memory | 4234.29 ms/step - Train Loss: 3.0128 | Validation loss: 3.0507\n",
      "Step 285/500 | 222 tensors in memory | 4233.27 ms/step - Train Loss: 3.0055 | Validation loss: 3.0179\n",
      "Step 286/500 | 222 tensors in memory | 4234.01 ms/step - Train Loss: 3.0257 | Validation loss: 3.0248\n",
      "Step 287/500 | 222 tensors in memory | 4234.18 ms/step - Train Loss: 3.0214 | Validation loss: 3.0388\n",
      "Step 288/500 | 222 tensors in memory | 4232.99 ms/step - Train Loss: 3.0137 | Validation loss: 3.1087\n",
      "Step 289/500 | 222 tensors in memory | 4231.85 ms/step - Train Loss: 3.0251 | Validation loss: 3.1142\n",
      "Step 290/500 | 222 tensors in memory | 4231.92 ms/step - Train Loss: 3.0198 | Validation loss: 3.0627\n",
      "Step 291/500 | 222 tensors in memory | 4230.93 ms/step - Train Loss: 3.0192 | Validation loss: 3.0673\n",
      "Step 292/500 | 222 tensors in memory | 4230.09 ms/step - Train Loss: 3.0431 | Validation loss: 3.0876\n",
      "Step 293/500 | 222 tensors in memory | 4228.58 ms/step - Train Loss: 3.0086 | Validation loss: 3.0850\n",
      "Step 294/500 | 222 tensors in memory | 4227.23 ms/step - Train Loss: 3.0567 | Validation loss: 3.0751\n",
      "Step 295/500 | 222 tensors in memory | 4226.66 ms/step - Train Loss: 2.9668 | Validation loss: 3.0745\n",
      "Step 296/500 | 222 tensors in memory | 4226.00 ms/step - Train Loss: 3.0736 | Validation loss: 3.0839\n",
      "Step 297/500 | 222 tensors in memory | 4224.88 ms/step - Train Loss: 3.0279 | Validation loss: 3.1022\n",
      "Step 298/500 | 222 tensors in memory | 4223.03 ms/step - Train Loss: 3.0034 | Validation loss: 3.0726\n",
      "Step 299/500 | 222 tensors in memory | 4222.87 ms/step - Train Loss: 2.9807 | Validation loss: 3.0676\n",
      "Step 300/500 | 222 tensors in memory | 4221.33 ms/step - Train Loss: 3.0410 | Validation loss: 3.0490\n",
      "Step 301/500 | 222 tensors in memory | 4219.94 ms/step - Train Loss: 3.0544 | Validation loss: 3.0632\n",
      "Step 302/500 | 222 tensors in memory | 4218.85 ms/step - Train Loss: 2.9751 | Validation loss: 3.0947\n",
      "Step 303/500 | 222 tensors in memory | 4219.23 ms/step - Train Loss: 2.9866 | Validation loss: 3.0627\n",
      "Step 304/500 | 222 tensors in memory | 4218.03 ms/step - Train Loss: 3.0212 | Validation loss: 3.0804\n",
      "Step 305/500 | 222 tensors in memory | 4216.98 ms/step - Train Loss: 3.0305 | Validation loss: 3.0475\n",
      "Step 306/500 | 222 tensors in memory | 4216.04 ms/step - Train Loss: 2.9601 | Validation loss: 3.0515\n",
      "Step 307/500 | 222 tensors in memory | 4216.06 ms/step - Train Loss: 2.9882 | Validation loss: 3.0754\n",
      "Step 308/500 | 222 tensors in memory | 4215.16 ms/step - Train Loss: 3.0351 | Validation loss: 3.0971\n",
      "Step 309/500 | 222 tensors in memory | 4214.40 ms/step - Train Loss: 2.9804 | Validation loss: 3.0903\n",
      "Step 310/500 | 222 tensors in memory | 4213.14 ms/step - Train Loss: 2.9879 | Validation loss: 3.0329\n",
      "Step 311/500 | 222 tensors in memory | 4212.33 ms/step - Train Loss: 2.9826 | Validation loss: 3.0599\n",
      "Step 312/500 | 222 tensors in memory | 4210.86 ms/step - Train Loss: 2.9667 | Validation loss: 3.0963\n",
      "Step 313/500 | 222 tensors in memory | 4209.39 ms/step - Train Loss: 2.9951 | Validation loss: 3.0385\n",
      "Step 314/500 | 222 tensors in memory | 4209.23 ms/step - Train Loss: 2.9904 | Validation loss: 3.0728\n",
      "Step 315/500 | 222 tensors in memory | 4208.45 ms/step - Train Loss: 3.0268 | Validation loss: 3.0590\n",
      "Step 316/500 | 222 tensors in memory | 4207.28 ms/step - Train Loss: 3.0038 | Validation loss: 3.0617\n",
      "Step 317/500 | 222 tensors in memory | 4205.76 ms/step - Train Loss: 2.9758 | Validation loss: 3.0492\n",
      "Step 318/500 | 222 tensors in memory | 4206.95 ms/step - Train Loss: 3.0184 | Validation loss: 3.0306\n",
      "Step 319/500 | 222 tensors in memory | 4206.58 ms/step - Train Loss: 3.0095 | Validation loss: 3.0762\n",
      "Step 320/500 | 222 tensors in memory | 4206.40 ms/step - Train Loss: 2.9761 | Validation loss: 3.0602\n",
      "Step 321/500 | 222 tensors in memory | 4206.56 ms/step - Train Loss: 3.0273 | Validation loss: 3.0173\n",
      "Step 322/500 | 222 tensors in memory | 4205.28 ms/step - Train Loss: 2.9901 | Validation loss: 3.0605\n",
      "Step 323/500 | 222 tensors in memory | 4204.60 ms/step - Train Loss: 3.0414 | Validation loss: 3.0329\n",
      "Step 324/500 | 222 tensors in memory | 4203.69 ms/step - Train Loss: 2.9918 | Validation loss: 3.0723\n",
      "Step 325/500 | 222 tensors in memory | 4202.61 ms/step - Train Loss: 3.0271 | Validation loss: 3.0794\n",
      "Step 326/500 | 222 tensors in memory | 4201.40 ms/step - Train Loss: 3.0255 | Validation loss: 3.0532\n",
      "Step 327/500 | 222 tensors in memory | 4199.94 ms/step - Train Loss: 3.0089 | Validation loss: 3.0625\n",
      "Step 328/500 | 222 tensors in memory | 4198.76 ms/step - Train Loss: 2.9666 | Validation loss: 3.0614\n",
      "Step 329/500 | 222 tensors in memory | 4196.75 ms/step - Train Loss: 2.9915 | Validation loss: 3.0311\n",
      "Step 330/500 | 222 tensors in memory | 4195.24 ms/step - Train Loss: 3.0121 | Validation loss: 3.0388\n",
      "Step 331/500 | 222 tensors in memory | 4194.30 ms/step - Train Loss: 2.9971 | Validation loss: 3.0192\n",
      "Step 332/500 | 222 tensors in memory | 4194.04 ms/step - Train Loss: 2.9835 | Validation loss: 3.0714\n",
      "Step 333/500 | 222 tensors in memory | 4192.57 ms/step - Train Loss: 2.9949 | Validation loss: 3.0898\n",
      "Step 334/500 | 222 tensors in memory | 4191.50 ms/step - Train Loss: 3.0087 | Validation loss: 3.0746\n",
      "Step 335/500 | 222 tensors in memory | 4190.39 ms/step - Train Loss: 3.0022 | Validation loss: 3.0725\n",
      "Step 336/500 | 222 tensors in memory | 4188.66 ms/step - Train Loss: 2.9648 | Validation loss: 3.0607\n",
      "Step 337/500 | 222 tensors in memory | 4186.90 ms/step - Train Loss: 2.9943 | Validation loss: 3.1017\n",
      "Step 338/500 | 222 tensors in memory | 4188.00 ms/step - Train Loss: 2.9858 | Validation loss: 3.0052\n",
      "Step 339/500 | 222 tensors in memory | 4187.63 ms/step - Train Loss: 2.9756 | Validation loss: 3.0452\n",
      "Step 340/500 | 222 tensors in memory | 4187.39 ms/step - Train Loss: 2.9837 | Validation loss: 3.1074\n",
      "Step 341/500 | 222 tensors in memory | 4186.76 ms/step - Train Loss: 3.0146 | Validation loss: 3.0246\n",
      "Step 342/500 | 222 tensors in memory | 4185.30 ms/step - Train Loss: 3.0226 | Validation loss: 3.0080\n",
      "Step 343/500 | 222 tensors in memory | 4184.50 ms/step - Train Loss: 2.9875 | Validation loss: 3.0786\n",
      "Step 344/500 | 222 tensors in memory | 4184.30 ms/step - Train Loss: 2.9541 | Validation loss: 3.0208\n",
      "Step 345/500 | 222 tensors in memory | 4183.51 ms/step - Train Loss: 2.9941 | Validation loss: 3.0522\n",
      "Step 346/500 | 222 tensors in memory | 4183.10 ms/step - Train Loss: 2.9428 | Validation loss: 3.0564\n",
      "Step 347/500 | 222 tensors in memory | 4182.13 ms/step - Train Loss: 2.9321 | Validation loss: 3.0398\n",
      "Step 348/500 | 222 tensors in memory | 4182.85 ms/step - Train Loss: 2.9966 | Validation loss: 3.0361\n",
      "Step 349/500 | 222 tensors in memory | 4182.27 ms/step - Train Loss: 2.9905 | Validation loss: 3.0105\n",
      "Step 350/500 | 222 tensors in memory | 4183.79 ms/step - Train Loss: 2.9659 | Validation loss: 3.0429\n",
      "Step 351/500 | 222 tensors in memory | 4183.62 ms/step - Train Loss: 2.9501 | Validation loss: 3.0231\n",
      "Step 352/500 | 222 tensors in memory | 4182.79 ms/step - Train Loss: 3.0327 | Validation loss: 2.9773\n",
      "Step 353/500 | 222 tensors in memory | 4181.66 ms/step - Train Loss: 2.9815 | Validation loss: 3.0202\n",
      "Step 354/500 | 222 tensors in memory | 4180.63 ms/step - Train Loss: 2.9879 | Validation loss: 3.0382\n",
      "Step 355/500 | 222 tensors in memory | 4179.67 ms/step - Train Loss: 2.9584 | Validation loss: 3.0842\n",
      "Step 356/500 | 222 tensors in memory | 4178.59 ms/step - Train Loss: 2.9249 | Validation loss: 2.9884\n",
      "Step 357/500 | 222 tensors in memory | 4177.44 ms/step - Train Loss: 2.9463 | Validation loss: 3.0342\n",
      "Step 358/500 | 222 tensors in memory | 4175.56 ms/step - Train Loss: 2.9506 | Validation loss: 3.0193\n",
      "Step 359/500 | 222 tensors in memory | 4174.48 ms/step - Train Loss: 2.9468 | Validation loss: 3.0423\n",
      "Step 360/500 | 222 tensors in memory | 4172.72 ms/step - Train Loss: 2.9263 | Validation loss: 3.0053\n",
      "Step 361/500 | 222 tensors in memory | 4171.72 ms/step - Train Loss: 2.9787 | Validation loss: 3.0340\n",
      "Step 362/500 | 222 tensors in memory | 4170.93 ms/step - Train Loss: 2.9487 | Validation loss: 3.0617\n",
      "Step 363/500 | 222 tensors in memory | 4170.33 ms/step - Train Loss: 2.9784 | Validation loss: 3.0257\n",
      "Step 364/500 | 222 tensors in memory | 4170.32 ms/step - Train Loss: 2.9761 | Validation loss: 2.9867\n",
      "Step 365/500 | 222 tensors in memory | 4168.94 ms/step - Train Loss: 2.9560 | Validation loss: 3.0302\n",
      "Step 366/500 | 222 tensors in memory | 4168.38 ms/step - Train Loss: 2.9797 | Validation loss: 3.0380\n",
      "Step 367/500 | 222 tensors in memory | 4170.32 ms/step - Train Loss: 2.9593 | Validation loss: 3.0303\n",
      "Step 368/500 | 222 tensors in memory | 4170.13 ms/step - Train Loss: 2.9294 | Validation loss: 3.0558\n",
      "Step 369/500 | 222 tensors in memory | 4169.81 ms/step - Train Loss: 2.9540 | Validation loss: 3.0137\n",
      "Step 370/500 | 222 tensors in memory | 4169.09 ms/step - Train Loss: 2.9331 | Validation loss: 3.0465\n",
      "Step 371/500 | 222 tensors in memory | 4168.42 ms/step - Train Loss: 2.9709 | Validation loss: 3.0776\n",
      "Step 372/500 | 222 tensors in memory | 4167.72 ms/step - Train Loss: 2.9746 | Validation loss: 3.0211\n",
      "Step 373/500 | 222 tensors in memory | 4168.34 ms/step - Train Loss: 2.9668 | Validation loss: 3.0438\n",
      "Step 374/500 | 222 tensors in memory | 4168.62 ms/step - Train Loss: 2.9115 | Validation loss: 3.0124\n",
      "Step 375/500 | 222 tensors in memory | 4169.11 ms/step - Train Loss: 2.9388 | Validation loss: 3.0191\n",
      "Step 376/500 | 222 tensors in memory | 4169.96 ms/step - Train Loss: 2.9522 | Validation loss: 2.9979\n",
      "Step 377/500 | 222 tensors in memory | 4169.95 ms/step - Train Loss: 2.9404 | Validation loss: 2.9936\n",
      "Step 378/500 | 222 tensors in memory | 4169.98 ms/step - Train Loss: 2.9545 | Validation loss: 3.0707\n",
      "Step 379/500 | 222 tensors in memory | 4169.83 ms/step - Train Loss: 2.9329 | Validation loss: 3.0143\n",
      "Step 380/500 | 222 tensors in memory | 4170.26 ms/step - Train Loss: 2.9395 | Validation loss: 3.0115\n",
      "Step 381/500 | 222 tensors in memory | 4170.22 ms/step - Train Loss: 2.9559 | Validation loss: 3.0356\n",
      "Step 382/500 | 222 tensors in memory | 4169.93 ms/step - Train Loss: 2.9320 | Validation loss: 3.0630\n",
      "Step 383/500 | 222 tensors in memory | 4170.36 ms/step - Train Loss: 2.9285 | Validation loss: 3.0012\n",
      "Step 384/500 | 222 tensors in memory | 4170.45 ms/step - Train Loss: 2.9441 | Validation loss: 3.0298\n",
      "Step 385/500 | 222 tensors in memory | 4170.51 ms/step - Train Loss: 2.9336 | Validation loss: 3.0094\n",
      "Step 386/500 | 222 tensors in memory | 4169.37 ms/step - Train Loss: 2.9515 | Validation loss: 3.0128\n",
      "Step 387/500 | 222 tensors in memory | 4168.74 ms/step - Train Loss: 2.9771 | Validation loss: 3.0457\n",
      "Step 388/500 | 222 tensors in memory | 4169.11 ms/step - Train Loss: 2.9036 | Validation loss: 2.9980\n",
      "Step 389/500 | 222 tensors in memory | 4168.26 ms/step - Train Loss: 2.8846 | Validation loss: 3.0065\n",
      "Step 390/500 | 222 tensors in memory | 4167.19 ms/step - Train Loss: 2.9147 | Validation loss: 3.0207\n",
      "Step 391/500 | 222 tensors in memory | 4166.90 ms/step - Train Loss: 2.9189 | Validation loss: 3.0208\n",
      "Step 392/500 | 222 tensors in memory | 4166.36 ms/step - Train Loss: 2.9088 | Validation loss: 2.9976\n",
      "Step 393/500 | 222 tensors in memory | 4165.17 ms/step - Train Loss: 2.9448 | Validation loss: 3.0241\n",
      "Step 394/500 | 222 tensors in memory | 4165.53 ms/step - Train Loss: 2.8892 | Validation loss: 2.9818\n",
      "Step 395/500 | 222 tensors in memory | 4165.09 ms/step - Train Loss: 2.9197 | Validation loss: 2.9913\n",
      "Step 396/500 | 222 tensors in memory | 4166.27 ms/step - Train Loss: 2.9290 | Validation loss: 3.0213\n",
      "Step 397/500 | 222 tensors in memory | 4166.22 ms/step - Train Loss: 2.9239 | Validation loss: 3.0806\n",
      "Step 398/500 | 222 tensors in memory | 4165.50 ms/step - Train Loss: 2.9358 | Validation loss: 3.0127\n",
      "Step 399/500 | 222 tensors in memory | 4165.29 ms/step - Train Loss: 2.8887 | Validation loss: 2.9894\n",
      "Step 400/500 | 222 tensors in memory | 4165.03 ms/step - Train Loss: 2.9396 | Validation loss: 3.0079\n",
      "Step 401/500 | 222 tensors in memory | 4164.58 ms/step - Train Loss: 2.8932 | Validation loss: 3.0191\n",
      "Step 402/500 | 222 tensors in memory | 4163.90 ms/step - Train Loss: 2.9375 | Validation loss: 3.0496\n",
      "Step 403/500 | 222 tensors in memory | 4163.25 ms/step - Train Loss: 2.9201 | Validation loss: 2.9833\n",
      "Step 404/500 | 222 tensors in memory | 4163.23 ms/step - Train Loss: 2.8775 | Validation loss: 3.0132\n",
      "Step 405/500 | 222 tensors in memory | 4163.26 ms/step - Train Loss: 2.8829 | Validation loss: 2.9866\n",
      "Step 406/500 | 222 tensors in memory | 4162.53 ms/step - Train Loss: 2.8631 | Validation loss: 2.9910\n",
      "Step 407/500 | 222 tensors in memory | 4161.55 ms/step - Train Loss: 2.8976 | Validation loss: 3.0017\n",
      "Step 408/500 | 222 tensors in memory | 4161.39 ms/step - Train Loss: 2.9383 | Validation loss: 2.9598\n",
      "Step 409/500 | 222 tensors in memory | 4160.62 ms/step - Train Loss: 2.9209 | Validation loss: 2.9688\n",
      "Step 410/500 | 222 tensors in memory | 4159.39 ms/step - Train Loss: 2.8689 | Validation loss: 2.9898\n",
      "Step 411/500 | 222 tensors in memory | 4158.48 ms/step - Train Loss: 2.8757 | Validation loss: 3.0511\n",
      "Step 412/500 | 222 tensors in memory | 4157.85 ms/step - Train Loss: 2.8883 | Validation loss: 3.0192\n",
      "Step 413/500 | 222 tensors in memory | 4156.80 ms/step - Train Loss: 2.8968 | Validation loss: 3.0027\n",
      "Step 414/500 | 222 tensors in memory | 4157.76 ms/step - Train Loss: 2.8862 | Validation loss: 3.0248\n",
      "Step 415/500 | 222 tensors in memory | 4157.36 ms/step - Train Loss: 2.9252 | Validation loss: 3.0100\n",
      "Step 416/500 | 222 tensors in memory | 4157.58 ms/step - Train Loss: 2.8674 | Validation loss: 3.0016\n",
      "Step 417/500 | 222 tensors in memory | 4156.90 ms/step - Train Loss: 2.9151 | Validation loss: 2.9822\n",
      "Step 418/500 | 222 tensors in memory | 4156.30 ms/step - Train Loss: 2.8642 | Validation loss: 2.9992\n",
      "Step 419/500 | 222 tensors in memory | 4155.81 ms/step - Train Loss: 2.8685 | Validation loss: 2.9909\n",
      "Step 420/500 | 222 tensors in memory | 4155.85 ms/step - Train Loss: 2.8940 | Validation loss: 2.9828\n",
      "Step 421/500 | 222 tensors in memory | 4155.52 ms/step - Train Loss: 2.8672 | Validation loss: 2.9848\n",
      "Step 422/500 | 222 tensors in memory | 4155.77 ms/step - Train Loss: 2.8709 | Validation loss: 2.9809\n",
      "Step 423/500 | 222 tensors in memory | 4155.74 ms/step - Train Loss: 2.9008 | Validation loss: 2.9728\n",
      "Step 424/500 | 222 tensors in memory | 4155.14 ms/step - Train Loss: 2.8766 | Validation loss: 2.9995\n",
      "Step 425/500 | 222 tensors in memory | 4154.90 ms/step - Train Loss: 2.8383 | Validation loss: 3.0091\n",
      "Step 426/500 | 222 tensors in memory | 4154.46 ms/step - Train Loss: 2.8754 | Validation loss: 3.0075\n",
      "Step 427/500 | 222 tensors in memory | 4153.73 ms/step - Train Loss: 2.8747 | Validation loss: 2.9805\n",
      "Step 428/500 | 222 tensors in memory | 4152.68 ms/step - Train Loss: 2.8756 | Validation loss: 2.9574\n",
      "Step 429/500 | 222 tensors in memory | 4152.38 ms/step - Train Loss: 2.8513 | Validation loss: 2.9623\n",
      "Step 430/500 | 222 tensors in memory | 4151.71 ms/step - Train Loss: 2.8662 | Validation loss: 2.9958\n",
      "Step 431/500 | 222 tensors in memory | 4151.14 ms/step - Train Loss: 2.8658 | Validation loss: 3.0162\n",
      "Step 432/500 | 222 tensors in memory | 4150.96 ms/step - Train Loss: 2.8173 | Validation loss: 2.9515\n",
      "Step 433/500 | 222 tensors in memory | 4150.52 ms/step - Train Loss: 2.8493 | Validation loss: 2.9374\n",
      "Step 434/500 | 222 tensors in memory | 4149.65 ms/step - Train Loss: 2.8956 | Validation loss: 2.9802\n",
      "Step 435/500 | 222 tensors in memory | 4148.59 ms/step - Train Loss: 2.8681 | Validation loss: 2.9860\n",
      "Step 436/500 | 222 tensors in memory | 4148.37 ms/step - Train Loss: 2.8550 | Validation loss: 2.9825\n",
      "Step 437/500 | 222 tensors in memory | 4148.02 ms/step - Train Loss: 2.8318 | Validation loss: 3.0191\n",
      "Step 438/500 | 222 tensors in memory | 4149.08 ms/step - Train Loss: 2.8339 | Validation loss: 2.9405\n",
      "Step 439/500 | 222 tensors in memory | 4148.92 ms/step - Train Loss: 2.8309 | Validation loss: 2.9764\n",
      "Step 440/500 | 222 tensors in memory | 4148.53 ms/step - Train Loss: 2.8608 | Validation loss: 2.9593\n",
      "Step 441/500 | 222 tensors in memory | 4148.11 ms/step - Train Loss: 2.8497 | Validation loss: 2.9679\n",
      "Step 442/500 | 222 tensors in memory | 4148.00 ms/step - Train Loss: 2.8597 | Validation loss: 2.9764\n",
      "Step 443/500 | 222 tensors in memory | 4148.01 ms/step - Train Loss: 2.8755 | Validation loss: 2.9095\n",
      "Step 444/500 | 222 tensors in memory | 4147.52 ms/step - Train Loss: 2.8506 | Validation loss: 2.9902\n",
      "Step 445/500 | 222 tensors in memory | 4146.82 ms/step - Train Loss: 2.8580 | Validation loss: 2.9762\n",
      "Step 446/500 | 222 tensors in memory | 4146.01 ms/step - Train Loss: 2.8525 | Validation loss: 2.9608\n",
      "Step 447/500 | 222 tensors in memory | 4145.89 ms/step - Train Loss: 2.8171 | Validation loss: 2.9327\n",
      "Step 448/500 | 222 tensors in memory | 4145.21 ms/step - Train Loss: 2.8227 | Validation loss: 2.9763\n",
      "Step 449/500 | 222 tensors in memory | 4144.60 ms/step - Train Loss: 2.8338 | Validation loss: 2.9522\n",
      "Step 450/500 | 222 tensors in memory | 4144.22 ms/step - Train Loss: 2.7703 | Validation loss: 3.0040\n",
      "Step 451/500 | 222 tensors in memory | 4144.02 ms/step - Train Loss: 2.8444 | Validation loss: 2.9554\n",
      "Step 452/500 | 222 tensors in memory | 4143.71 ms/step - Train Loss: 2.8213 | Validation loss: 2.9547\n",
      "Step 453/500 | 222 tensors in memory | 4143.07 ms/step - Train Loss: 2.8686 | Validation loss: 2.9458\n",
      "Step 454/500 | 222 tensors in memory | 4143.09 ms/step - Train Loss: 2.8277 | Validation loss: 2.9751\n",
      "Step 455/500 | 222 tensors in memory | 4144.16 ms/step - Train Loss: 2.8066 | Validation loss: 2.9985\n",
      "Step 456/500 | 222 tensors in memory | 4143.74 ms/step - Train Loss: 2.7903 | Validation loss: 2.9515\n",
      "Step 457/500 | 222 tensors in memory | 4142.81 ms/step - Train Loss: 2.8097 | Validation loss: 2.9447\n",
      "Step 458/500 | 222 tensors in memory | 4143.47 ms/step - Train Loss: 2.9053 | Validation loss: 2.9545\n",
      "Step 459/500 | 222 tensors in memory | 4143.29 ms/step - Train Loss: 2.8358 | Validation loss: 2.9612\n",
      "Step 460/500 | 222 tensors in memory | 4144.13 ms/step - Train Loss: 2.7885 | Validation loss: 2.9308\n",
      "Step 461/500 | 222 tensors in memory | 4144.10 ms/step - Train Loss: 2.7873 | Validation loss: 2.9620\n",
      "Step 462/500 | 222 tensors in memory | 4144.30 ms/step - Train Loss: 2.8284 | Validation loss: 2.9528\n",
      "Step 463/500 | 222 tensors in memory | 4144.00 ms/step - Train Loss: 2.8135 | Validation loss: 2.9724\n",
      "Step 464/500 | 222 tensors in memory | 4143.12 ms/step - Train Loss: 2.8121 | Validation loss: 2.9582\n",
      "Step 465/500 | 222 tensors in memory | 4142.42 ms/step - Train Loss: 2.8009 | Validation loss: 2.9297\n",
      "Step 466/500 | 222 tensors in memory | 4141.89 ms/step - Train Loss: 2.7994 | Validation loss: 2.9578\n",
      "Step 467/500 | 222 tensors in memory | 4141.81 ms/step - Train Loss: 2.8182 | Validation loss: 2.9673\n",
      "Step 468/500 | 222 tensors in memory | 4141.17 ms/step - Train Loss: 2.7807 | Validation loss: 2.9181\n",
      "Step 469/500 | 222 tensors in memory | 4140.95 ms/step - Train Loss: 2.7872 | Validation loss: 2.9720\n",
      "Step 470/500 | 222 tensors in memory | 4140.55 ms/step - Train Loss: 2.7730 | Validation loss: 2.9064\n",
      "Step 471/500 | 222 tensors in memory | 4140.57 ms/step - Train Loss: 2.7840 | Validation loss: 2.9942\n",
      "Step 472/500 | 222 tensors in memory | 4140.07 ms/step - Train Loss: 2.8272 | Validation loss: 2.9634\n",
      "Step 473/500 | 222 tensors in memory | 4139.29 ms/step - Train Loss: 2.7844 | Validation loss: 2.9486\n",
      "Step 474/500 | 222 tensors in memory | 4138.57 ms/step - Train Loss: 2.7923 | Validation loss: 2.9576\n",
      "Step 475/500 | 222 tensors in memory | 4138.41 ms/step - Train Loss: 2.8141 | Validation loss: 2.9176\n",
      "Step 476/500 | 222 tensors in memory | 4138.83 ms/step - Train Loss: 2.8260 | Validation loss: 2.9332\n",
      "Step 477/500 | 222 tensors in memory | 4138.18 ms/step - Train Loss: 2.8243 | Validation loss: 2.9542\n",
      "Step 478/500 | 222 tensors in memory | 4138.84 ms/step - Train Loss: 2.7858 | Validation loss: 2.9568\n",
      "Step 479/500 | 222 tensors in memory | 4139.07 ms/step - Train Loss: 2.8057 | Validation loss: 2.9175\n",
      "Step 480/500 | 222 tensors in memory | 4139.10 ms/step - Train Loss: 2.7772 | Validation loss: 2.9148\n",
      "Step 481/500 | 222 tensors in memory | 4138.93 ms/step - Train Loss: 2.7951 | Validation loss: 2.8900\n",
      "Step 482/500 | 222 tensors in memory | 4139.30 ms/step - Train Loss: 2.8145 | Validation loss: 2.9181\n",
      "Step 483/500 | 222 tensors in memory | 4139.13 ms/step - Train Loss: 2.7818 | Validation loss: 2.9250\n",
      "Step 484/500 | 222 tensors in memory | 4138.67 ms/step - Train Loss: 2.7814 | Validation loss: 2.9465\n",
      "Step 485/500 | 222 tensors in memory | 4138.18 ms/step - Train Loss: 2.8127 | Validation loss: 2.9645\n",
      "Step 486/500 | 222 tensors in memory | 4137.36 ms/step - Train Loss: 2.7699 | Validation loss: 2.9124\n",
      "Step 487/500 | 222 tensors in memory | 4136.66 ms/step - Train Loss: 2.7812 | Validation loss: 2.9197\n",
      "Step 488/500 | 222 tensors in memory | 4135.81 ms/step - Train Loss: 2.7386 | Validation loss: 2.9051\n",
      "Step 489/500 | 222 tensors in memory | 4135.29 ms/step - Train Loss: 2.7894 | Validation loss: 2.9723\n",
      "Step 490/500 | 222 tensors in memory | 4135.25 ms/step - Train Loss: 2.7780 | Validation loss: 2.9326\n",
      "Step 491/500 | 222 tensors in memory | 4134.68 ms/step - Train Loss: 2.7195 | Validation loss: 2.9366\n",
      "Step 492/500 | 222 tensors in memory | 4134.53 ms/step - Train Loss: 2.7918 | Validation loss: 2.8787\n",
      "Step 493/500 | 222 tensors in memory | 4133.81 ms/step - Train Loss: 2.7452 | Validation loss: 2.9278\n",
      "Step 494/500 | 222 tensors in memory | 4133.01 ms/step - Train Loss: 2.7413 | Validation loss: 2.9024\n",
      "Step 495/500 | 222 tensors in memory | 4132.37 ms/step - Train Loss: 2.7936 | Validation loss: 2.9607\n",
      "Step 496/500 | 222 tensors in memory | 4132.62 ms/step - Train Loss: 2.7593 | Validation loss: 2.9588\n",
      "Step 497/500 | 222 tensors in memory | 4132.40 ms/step - Train Loss: 2.7396 | Validation loss: 2.8986\n",
      "Step 498/500 | 222 tensors in memory | 4131.79 ms/step - Train Loss: 2.7390 | Validation loss: 2.9687\n",
      "Step 499/500 | 222 tensors in memory | 4131.60 ms/step - Train Loss: 2.7238 | Validation loss: 2.9233\n",
      "Step 500/500 | 222 tensors in memory | 4131.33 ms/step - Train Loss: 2.7669 | Validation loss: 2.9315\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = language_model.fit(\n",
    "    data_loader = data_loader,\n",
    "    steps = training_steps,\n",
    "    lr = learning_rate,\n",
    "    batch_size = batch_size,\n",
    "    eval_iters = eval_iters,\n",
    "    grad_accumulation_steps = grad_accumulation_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation loss\n",
    "data_analysis.plot_history(history[\"loss\"].data, history[\"val_loss\"].data, \"Training and Validation Loss\", \"Eval iter\", \"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fiamppe:\n",
      "Belli di ba a pubiloco� scende\n",
      "la nidel senntesse cupilla diil vo\n",
      "te, e vando maggia suol fia\n",
      "li mi sanza nostre, e che fuorgo\n",
      "sive nel torreo mo, neltuo, sotto stregge;\n",
      "\n",
      "\n",
      " mente cade cammin sannon fesso,\n",
      "or quella biesse per sposa\n",
      "che; si e fede che si li trar, chi a,\n",
      "quanto lungo letinon non soave; drizzavante "
     ]
    }
   ],
   "source": [
    "# Generate some text context from the trained model\n",
    "context = Tensor(np.zeros((1, 1), dtype=np.int32))\n",
    "\n",
    "# Iterate over the tokens generated by the transformer\n",
    "for token in language_model.generate(context, max_new_tokens=200, stream=True):\n",
    "    # Decode the token\n",
    "    decoded_token = tokenizer.decode([token.data.squeeze().tolist()])\n",
    "\n",
    "    # Print the decoded token\n",
    "    print(decoded_token, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
