{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Add the path to the custom library to the system path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import custom modules\n",
    "from src import Tensor, loss_functions, optimizers, metrics\n",
    "from src.architectures.transformer import Tokenizer, DecoderTransformer\n",
    "from src.core.utils import data_analysis, data_processing, context_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dataset_path = os.path.join(os.getcwd(), 'dataset', 'divina_commedia.txt')\n",
    "tokenizer_path = os.path.join(os.getcwd(), 'checkpoints', 'tokenizer.json')\n",
    "model_path = os.path.join(os.getcwd(), 'checkpoints', 'language_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "dropout = 0.3 # The dropout rate\n",
    "train_test_split_pct = 0.1 # 90% of the data will be used for training, 10% for testing\n",
    "train_valid_split_pct = 0.1 # 90% of the training data will be used for training, 10% for validation\n",
    "batch_size = 32 # The number of samples to use for each batch\n",
    "grad_accumulation_steps = 1 # The number of steps to accumulate gradients before updating the model\n",
    "sequence_length = 256 # The size of the sequence length (the context window)\n",
    "learning_rate = 1e-3 # The learning rate for the optimizer\n",
    "weight_decay = 1e-2 # The weight decay for the optimizer\n",
    "epochs = 1 # The number of epochs to train the model for\n",
    "n_embed = 384 # The size of the token embeddings (the dimensionality of the embeddings)\n",
    "n_attention_heads = 6 # The number of attention heads in the multi-head attention mechanism\n",
    "n_decoder_blocks = 6 # The number of transformer'decoder blocks in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_file(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load a text file from the specified path.\n",
    "    \n",
    "    Parameters:\n",
    "    - path (str): The path to the text file.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The contents of the text file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f'The file \"{path}\" does not exist.')\n",
    "    \n",
    "    # Read the file\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Load the state of the tokenizer\n",
    "tokenizer.load(tokenizer_path)\n",
    "\n",
    "# Extract the vocabulary size\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "text = load_txt_file(dataset_path)\n",
    "\n",
    "# Encode the text using the tokenizer\n",
    "encoded_text = tokenizer.encode(text)\n",
    "\n",
    "# Convert the data to a tensor\n",
    "data = np.array(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(input_data: np.ndarray, seq_length: int) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Build sequences\n",
    "    \n",
    "    Parameters:\n",
    "    - input_data: np.ndarray, input features\n",
    "    - target_data: np.ndarray, target values (aligned with input_data)\n",
    "    - seq_length: int, length of input sequences\n",
    "    \n",
    "    Returns:\n",
    "    - tuple[Tensor, Tensor], input sequences and targets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize lists to hold sequences and targets\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Iterate over the input data to create sequences\n",
    "    for i in range(seq_length, len(input_data)):\n",
    "        # Append the sequence of input and the corresponding target\n",
    "        X.append(input_data[i-seq_length:i])\n",
    "        y.append(input_data[i-seq_length+1:i+1])\n",
    "    \n",
    "    # Convert the lists to numpy arrays and return as Tensors\n",
    "    return Tensor(np.array(X, dtype=np.float32)), Tensor(np.array(y, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequences from the encoded text data\n",
    "X, y = build_sequences(data, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (143160, 256) (143160, 256)\n",
      "Validation set: (15906, 256) (15906, 256)\n",
      "Testing set: (17674, 256) (17674, 256)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = data_processing.split_data((X, y), train_test_split_pct, shuffle=True)[0]\n",
    "X_train, X_valid, y_train, y_valid = data_processing.split_data((X_train, y_train), train_valid_split_pct, shuffle=True)[0]\n",
    "\n",
    "# Print the dataset information\n",
    "print('Training set:', X_train.shape(), y_train.shape())\n",
    "print('Validation set:', X_valid.shape(), y_valid.shape())\n",
    "print('Testing set:', X_test.shape(), y_test.shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the language model\n",
    "language_model = DecoderTransformer(\n",
    "    name = \"Language Model\",\n",
    "    input_dim = vocab_size,\n",
    "    sequence_length = sequence_length,\n",
    "    n_embed = n_embed,\n",
    "    return_sequence = True,\n",
    "    n_attention_heads = n_attention_heads,\n",
    "    n_decoder_blocks = n_decoder_blocks,\n",
    "    dropout = dropout\n",
    ")\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = loss_functions.CrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model with a first batch to initialize the weights\n",
    "# This is not necessary, but it is useful to know the input size\n",
    "\n",
    "# Disable gradient computation\n",
    "with context_manager.no_grad():\n",
    "    # Set the model in evaluation mode\n",
    "    language_model.eval()\n",
    "    \n",
    "    # Call the model with a batch of data to initialize it\n",
    "    language_model(X_train[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model (Transformer) [output_shape=(32, 256, 1024), params=11526400]\n",
      "└── language_model.modules (ModuleList) [output_shape=(32, 256, 1024), params=11526400]\n",
      "    └── module_list.0 (EncoderDecoderTransformer) [output_shape=(32, 256, 1024), params=11526400]\n",
      "        └── encoder_decoder_transformer.decoder (Decoder) [output_shape=(32, 256, 1024), params=11526400]\n",
      "            ├── decoder.input_proj (Embedding) [output_shape=(32, 256, 384), params=393216]\n",
      "            ├── decoder.positional_embedding (Embedding) [output_shape=(256, 384), params=98304]\n",
      "            ├── decoder.decoder_blocks (ModuleList) [output_shape=?, params=10639872]\n",
      "            │   ├── module_list.0 (Block) [output_shape=(32, 256, 384), params=1773312]\n",
      "            │   │   ├── block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │   │   ├── block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "            │   │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │   │   │   ├── block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "            │   │   │   └── block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "            │   │   ├── block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │   │   └── module_list.0.self_attention_heads (SelfMultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "            │   │       ├── self_multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "            │   │       │   ├── module_list.0 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.1 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.2 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.3 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.4 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   └── module_list.5 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │       ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       ├── self_multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │   │       └── module_list.0.self_attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "            │   ├── module_list.1 (Block) [output_shape=(32, 256, 384), params=1773312]\n",
      "            │   │   ├── block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │   │   ├── block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "            │   │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │   │   │   ├── block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "            │   │   │   └── block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "            │   │   ├── block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │   │   └── module_list.1.self_attention_heads (SelfMultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "            │   │       ├── self_multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "            │   │       │   ├── module_list.0 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.1 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.2 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.3 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.4 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   └── module_list.5 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │       ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       ├── self_multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │   │       └── module_list.1.self_attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "            │   ├── module_list.2 (Block) [output_shape=(32, 256, 384), params=1773312]\n",
      "            │   │   ├── block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │   │   ├── block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "            │   │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │   │   │   ├── block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "            │   │   │   └── block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "            │   │   ├── block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │   │   └── module_list.2.self_attention_heads (SelfMultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "            │   │       ├── self_multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "            │   │       │   ├── module_list.0 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.1 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.2 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.3 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.4 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   └── module_list.5 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │       ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       ├── self_multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │   │       └── module_list.2.self_attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "            │   ├── module_list.3 (Block) [output_shape=(32, 256, 384), params=1773312]\n",
      "            │   │   ├── block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │   │   ├── block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "            │   │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │   │   │   ├── block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "            │   │   │   └── block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "            │   │   ├── block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │   │   └── module_list.3.self_attention_heads (SelfMultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "            │   │       ├── self_multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "            │   │       │   ├── module_list.0 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.1 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.2 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.3 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.4 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   └── module_list.5 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │       ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       ├── self_multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │   │       └── module_list.3.self_attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "            │   ├── module_list.4 (Block) [output_shape=(32, 256, 384), params=1773312]\n",
      "            │   │   ├── block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │   │   ├── block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "            │   │   │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │   │   │   ├── block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "            │   │   │   └── block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "            │   │   ├── block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │   │   └── module_list.4.self_attention_heads (SelfMultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "            │   │       ├── self_multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "            │   │       │   ├── module_list.0 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.1 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.2 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.3 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   ├── module_list.4 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       │   └── module_list.5 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │   │       │       ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │   │       │       └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │   │       ├── self_multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │   │       └── module_list.4.self_attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "            │   └── module_list.5 (Block) [output_shape=(32, 256, 384), params=1773312]\n",
      "            │       ├── block.layer_norm_1 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │       ├── block.mlp (MLP) [output_shape=(32, 256, 384), params=1181568]\n",
      "            │       │   ├── mlp.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │       │   ├── block.mlp.input_dense (Dense) [output_shape=(32, 256, 1536), params=591360]\n",
      "            │       │   └── block.mlp.output_dense (Dense) [output_shape=(32, 256, 384), params=590208]\n",
      "            │       ├── block.layer_norm_2 (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            │       └── module_list.5.self_attention_heads (SelfMultiHeadAttention) [output_shape=(32, 256, 384), params=590208]\n",
      "            │           ├── self_multi_head_attention.heads (ModuleList) [output_shape=?, params=442368]\n",
      "            │           │   ├── module_list.0 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │           │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │           │   ├── module_list.1 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │           │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │           │   ├── module_list.2 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │           │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │           │   ├── module_list.3 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │           │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │           │   ├── module_list.4 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │           │   │   ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │   │   └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │           │   └── module_list.5 (SelfSingleHeadAttention) [output_shape=(32, 256, 64), params=73728]\n",
      "            │           │       ├── self_single_head_attention.key (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │       ├── self_single_head_attention.query (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │       ├── self_single_head_attention.value (Dense) [output_shape=(32, 256, 64), params=24576]\n",
      "            │           │       └── self_single_head_attention.dropout (Dropout) [output_shape=(32, 256, 256), params=0]\n",
      "            │           ├── self_multi_head_attention.dropout (Dropout) [output_shape=(32, 256, 384), params=0]\n",
      "            │           └── module_list.5.self_attention_heads.output_linear (Dense) [output_shape=(32, 256, 384), params=147840]\n",
      "            ├── decoder.layer_norm (LayerNormalization) [output_shape=(32, 256, 384), params=768]\n",
      "            └── decoder.output_layer (Dense) [output_shape=(32, 256, 1024), params=394240]\n"
     ]
    }
   ],
   "source": [
    "# Display the model summary in tree format.\n",
    "# This is useful since the whole model is composed of submodules,\n",
    "# therefore, the model summary will be displayed recursively\n",
    "language_model.summary(recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 (4.78%) | 1149 tensors in memory | 6035.89 ms/step --> loss: 3.08039"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mlanguage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_accumulation_steps\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/architectures/sequential/sequential.py:123\u001b[0m, in \u001b[0;36mSequential.fit\u001b[0;34m(self, X_train, y_train, X_valid, y_valid, optimizer, loss_fn, batch_size, gradient_accumulation_steps, epochs, metrics, callbacks, shuffle)\u001b[0m\n\u001b[1;32m    120\u001b[0m y_training_batch \u001b[38;5;241m=\u001b[39m Y_train_shuffled[training_step \u001b[38;5;241m*\u001b[39m batch_size:(training_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Forward pass: Compute the output of the model\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m training_batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_training_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Compute the loss of the model\u001b[39;00m\n\u001b[1;32m    126\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m loss_fn(y_training_batch, training_batch_output)\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:203\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape()\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/architectures/sequential/sequential.py:267\u001b[0m, in \u001b[0;36mSequential._forward\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    264\u001b[0m batch_out \u001b[38;5;241m=\u001b[39m x[step \u001b[38;5;241m*\u001b[39m batch_size:(step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size] \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Forward pass: Compute the output of the model\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m batch_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Append the output of the batch\u001b[39;00m\n\u001b[1;32m    270\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(batch_out)\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:203\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape()\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/modules_list.py:148\u001b[0m, in \u001b[0;36mModuleList._forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Iterate over the modules\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Perform the forward pass\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Return the output tensor\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:203\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape()\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/architectures/transformer/transformer.py:129\u001b[0m, in \u001b[0;36m_forward\u001b[0;34m(self, decoder_input, encoder_input)\u001b[0m\n\u001b[1;32m    117\u001b[0m         # Initialize the superclass\n\u001b[1;32m    118\u001b[0m         super().__init__(\n\u001b[1;32m    119\u001b[0m             sequence_length = sequence_length,\n\u001b[1;32m    120\u001b[0m             return_sequence = return_sequence,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m             *args, **kwargs\n\u001b[1;32m    125\u001b[0m         )\n\u001b[1;32m    128\u001b[0m class EncoderDecoderTransformer(Module):\n\u001b[0;32m--> 129\u001b[0m \n\u001b[1;32m    130\u001b[0m     ### Magic methods ###\n\u001b[1;32m    132\u001b[0m     def __init__(\n\u001b[1;32m    133\u001b[0m         self,\n\u001b[1;32m    134\u001b[0m         decoder_input_dim: int,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m         *args, **kwargs\n\u001b[1;32m    149\u001b[0m     ) -> None:\n\u001b[1;32m    150\u001b[0m         \"\"\"\n\u001b[1;32m    151\u001b[0m         Initialize the encoder-decoder transformer model.\n\u001b[1;32m    152\u001b[0m         \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m         - data_type (Literal[\"discrete\", \"continuous\"]): The type of input data, either \"discrete\" for text or \"continuous\" for other types\n\u001b[1;32m    176\u001b[0m         \"\"\"\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:91\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mMethod to call the forward method of the module\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m- Tensor: Output of the module after the forward pass\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Call the forward method\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:203\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape()\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/architectures/transformer/decoder.py:124\u001b[0m, in \u001b[0;36mDecoder._forward\u001b[0;34m(self, x, encoder_output)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Apply the decoder blocks\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_blocks:\n\u001b[0;32m--> 124\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, S, E) -> (B, S, E)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Apply the output layer to get the logits\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(embeddings))\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:91\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mMethod to call the forward method of the module\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m- Tensor: Output of the module after the forward pass\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Call the forward method\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:203\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape()\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/architectures/transformer/block.py:82\u001b[0m, in \u001b[0;36mBlock._forward\u001b[0;34m(self, x, encoder_output)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03mForward pass of the transformer block.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m- ValueError: If cross-attention is used but encoder_output is not provided.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Dimensions are:\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# - B: batch size\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# - S: sequence length\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# - E: embedding size (embedding dimension of the original data)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Apply the multi-attention mechanism with skip connections\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m out \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, S, E) + (B, S, E) -> (B, S, E)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Check if cross-attention has to be applied\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cross_attention:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Ensure that encoder_output is provided\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:91\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mMethod to call the forward method of the module\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m- Tensor: Output of the module after the forward pass\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Call the forward method\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:203\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape()\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/architectures/transformer/self_attention.py:173\u001b[0m, in \u001b[0;36mSelfMultiHeadAttention._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mForward pass of the MultiHeadAttention module\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m- Tensor: The output tensor\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Dimensions are:\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# - B: batch size\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# - S: sequence length\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Apply each head to the embeddings\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m out \u001b[38;5;241m=\u001b[39m concat(\u001b[43m[\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, S, E) -> (B, S, H * n_heads)\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Apply the output linear layer to project the embeddings back to the original size\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_linear(out))\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/architectures/transformer/self_attention.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mForward pass of the MultiHeadAttention module\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m- Tensor: The output tensor\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Dimensions are:\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# - B: batch size\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# - S: sequence length\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Apply each head to the embeddings\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m out \u001b[38;5;241m=\u001b[39m concat([\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, S, E) -> (B, S, H * n_heads)\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Apply the output linear layer to project the embeddings back to the original size\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_linear(out))\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:91\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mMethod to call the forward method of the module\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m- Tensor: Output of the module after the forward pass\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Call the forward method\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:203\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape()\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/architectures/transformer/self_attention.py:84\u001b[0m, in \u001b[0;36mSelfSingleHeadAttention._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39msoftmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Apply dropout to the attention weights\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, S, S) -> (B, S, S)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Compute the contextual embeddings by applying the attention weights to the value embeddings\u001b[39;00m\n\u001b[1;32m     87\u001b[0m contextual_embeddings \u001b[38;5;241m=\u001b[39m attention_weights \u001b[38;5;241m@\u001b[39m v \u001b[38;5;66;03m# (B, S, S) @ (B, S, H) -> (B, S, H)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:91\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03mMethod to call the forward method of the module\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m- Tensor: Output of the module after the forward pass\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Call the forward method\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/core/module.py:203\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_init(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m### Step 2: Forward pass, to be implemented in the child class ###\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Call the forward method of the module\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m### Step 3: Update the output shape ###\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Save the input and  output shape of the module\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_shape \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape()\n",
      "File \u001b[0;32m~/Documents/Machine-Learning-from-scratch/Deep Learning/Transformer/../src/layers/dropout.py:41\u001b[0m, in \u001b[0;36mDropout._forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mForward pass of the dropout layer.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m- Tensor: The output tensor.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Generate a random mask\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     mask \u001b[38;5;241m=\u001b[39m Tensor(\n\u001b[0;32m---> 41\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate, \n\u001b[1;32m     42\u001b[0m         requires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     43\u001b[0m         is_parameter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Scale the output during training\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m mask \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrate)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = language_model.fit(\n",
    "    X_train = X_train,\n",
    "    y_train = y_train,\n",
    "    X_valid = X_valid,\n",
    "    y_valid = y_valid,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = loss_fn,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    metrics = [metrics.accuracy],\n",
    "    gradient_accumulation_steps = grad_accumulation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "language_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation loss\n",
    "data_analysis.plot_history(\n",
    "    train_loss = language_model.history[\"loss\"], \n",
    "    valid_loss = language_model.history[\"val_loss\"], \n",
    "    title = \"Training and Validation Loss\", \n",
    "    xlabel = \"Eval iter\",\n",
    "    ylabel = \"Loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient computation\n",
    "with context_manager.no_grad():\n",
    "    # Set the model in evaluation mode\n",
    "    language_model.eval()\n",
    "    \n",
    "    # Compute the predictions\n",
    "    predictions = language_model(X_test)\n",
    "\n",
    "# Apply the argmax function to the predictions\n",
    "predictions = Tensor(np.argmax(predictions.data, axis=1), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy\n",
    "accuracy = metrics.accuracy(y_test, predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy.data:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ristscentua li�sotto �vi\u0002 �noi  due � \\ui tt�modi lui che  i.\n",
      "convien ha 0� g��spdel in aliluce \u0018e,\n",
      "ava parve orl'uunacqua Per ava orlo d'un �questo, e tutti del e;\n",
      "Etto qdo,\n",
      "mmo bera, e segntutte  aperveder  do,\n",
      " 3uezia \u000e�ranaltro ssmia e�quellta.\n",
      " tutto 'o\n",
      "che �gente  nel �cum'��alcun \u0013sempre ��chi �}fossa,\n",
      "e quellvolagno,\n",
      "e cer9\u001ce;\n",
      "ch'a  mo  \u0007\u001dcono nostrstellcompvin��$el ggio mondo �ombco\n",
      "amenTu}quelle cor fa fac ciascl'anigspi finalla fachcomp#quanQe \n",
      "�Vlui �\u001cprima altre avan afand'��e � far Poli che si uscider��veder \u0001masol ?\n",
      "asi "
     ]
    }
   ],
   "source": [
    "# Generate some text context from the trained model\n",
    "context = Tensor(np.zeros((1, 1), dtype=np.int32))\n",
    "\n",
    "# Iterate over the tokens generated by the transformer\n",
    "for token in language_model.autoregressive_generation(x=context, num_steps=200, stream=True):\n",
    "    # Decode the token\n",
    "    decoded_token = tokenizer.decode([token.data.squeeze().tolist()])\n",
    "\n",
    "    # Print the decoded token\n",
    "    print(decoded_token, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
