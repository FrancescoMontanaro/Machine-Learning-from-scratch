{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from src.torch.utils import device\n",
    "from src.utils import load_txt_file\n",
    "from src.tokenizer import Tokenizer\n",
    "from src.torch.data_loader import DataLoader\n",
    "from src.torch.transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dataset_path = os.path.join(os.getcwd(), 'dataset', 'input.txt')\n",
    "tokenizer_path = os.path.join(os.getcwd(), 'checkpoints', 'tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "train_val_split = 0.9 # 90% of the data will be used for training, 10% for validation\n",
    "batch_size = 64 # The number of samples to use for each batch\n",
    "block_size = 256 # The size of the sequence length (the context window)\n",
    "learning_rate = 1e-3 # The learning rate for the optimizer\n",
    "epochs = 500 # The number of epochs to train the model for\n",
    "n_embed = 384 # The size of the token embeddings (the dimensionality of the embeddings)\n",
    "eval_iters = 10 # The number of iterations to evaluate the model\n",
    "num_attention_heads = 6 # The number of attention heads in the multi-head attention mechanism\n",
    "num_transformer_blocks = 6 # The number of transformer blocks in the model\n",
    "dropout = 0.2 # The dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(1337);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Load the state of the tokenizer\n",
    "tokenizer.load_state(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "text = load_txt_file(dataset_path)\n",
    "\n",
    "# Encode the text using the tokenizer\n",
    "encoded_text = tokenizer.encode(text)\n",
    "\n",
    "# Convert the data to a tensor\n",
    "data = torch.tensor(encoded_text, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the data handler\n",
    "data_handler = DataLoader(\n",
    "    data = data, \n",
    "    train_val_split = train_val_split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to device: mps\n"
     ]
    }
   ],
   "source": [
    "# Create the language model\n",
    "language_model = Transformer(\n",
    "    vocab_size = tokenizer.vocab_size, # type: ignore\n",
    "    n_embed = n_embed,\n",
    "    n_heads = num_attention_heads,\n",
    "    block_size = block_size,\n",
    "    n_transformer_blocks = num_transformer_blocks,\n",
    "    dropout = dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 - Train Loss: 7.1033, Val Loss: 7.0970\n",
      "Epoch 2/500 - Train Loss: 6.3920, Val Loss: 6.4655\n",
      "Epoch 3/500 - Train Loss: 6.0905, Val Loss: 6.0725\n",
      "Epoch 4/500 - Train Loss: 5.7916, Val Loss: 5.8165\n",
      "Epoch 5/500 - Train Loss: 5.5507, Val Loss: 5.5572\n",
      "Epoch 6/500 - Train Loss: 5.3630, Val Loss: 5.3522\n",
      "Epoch 7/500 - Train Loss: 5.2500, Val Loss: 5.2770\n",
      "Epoch 8/500 - Train Loss: 5.2101, Val Loss: 5.2534\n",
      "Epoch 9/500 - Train Loss: 5.2063, Val Loss: 5.2511\n",
      "Epoch 10/500 - Train Loss: 5.1816, Val Loss: 5.2306\n",
      "Epoch 11/500 - Train Loss: 5.1752, Val Loss: 5.2044\n",
      "Epoch 12/500 - Train Loss: 5.1322, Val Loss: 5.1698\n",
      "Epoch 13/500 - Train Loss: 5.1153, Val Loss: 5.1428\n",
      "Epoch 14/500 - Train Loss: 5.0602, Val Loss: 5.1285\n",
      "Epoch 15/500 - Train Loss: 5.0454, Val Loss: 5.0358\n",
      "Epoch 16/500 - Train Loss: 4.9656, Val Loss: 5.0266\n",
      "Epoch 17/500 - Train Loss: 4.9001, Val Loss: 4.9540\n",
      "Epoch 18/500 - Train Loss: 4.8673, Val Loss: 4.9235\n",
      "Epoch 19/500 - Train Loss: 4.7798, Val Loss: 4.8656\n",
      "Epoch 20/500 - Train Loss: 4.7462, Val Loss: 4.8076\n",
      "Epoch 21/500 - Train Loss: 4.6939, Val Loss: 4.7595\n",
      "Epoch 22/500 - Train Loss: 4.6371, Val Loss: 4.7027\n",
      "Epoch 23/500 - Train Loss: 4.5893, Val Loss: 4.6545\n",
      "Epoch 24/500 - Train Loss: 4.5319, Val Loss: 4.6221\n",
      "Epoch 25/500 - Train Loss: 4.4978, Val Loss: 4.5497\n",
      "Epoch 26/500 - Train Loss: 4.4314, Val Loss: 4.5305\n",
      "Epoch 27/500 - Train Loss: 4.3936, Val Loss: 4.4842\n",
      "Epoch 28/500 - Train Loss: 4.3861, Val Loss: 4.4070\n",
      "Epoch 29/500 - Train Loss: 4.2890, Val Loss: 4.3753\n",
      "Epoch 30/500 - Train Loss: 4.2839, Val Loss: 4.3241\n",
      "Epoch 31/500 - Train Loss: 4.2308, Val Loss: 4.3356\n",
      "Epoch 32/500 - Train Loss: 4.1907, Val Loss: 4.2827\n",
      "Epoch 33/500 - Train Loss: 4.1443, Val Loss: 4.2536\n",
      "Epoch 34/500 - Train Loss: 4.1485, Val Loss: 4.2676\n",
      "Epoch 35/500 - Train Loss: 4.1468, Val Loss: 4.2493\n",
      "Epoch 36/500 - Train Loss: 4.1003, Val Loss: 4.2302\n",
      "Epoch 37/500 - Train Loss: 4.0731, Val Loss: 4.1513\n",
      "Epoch 38/500 - Train Loss: 4.0699, Val Loss: 4.1444\n",
      "Epoch 39/500 - Train Loss: 4.0294, Val Loss: 4.1312\n",
      "Epoch 40/500 - Train Loss: 3.9834, Val Loss: 4.0982\n",
      "Epoch 41/500 - Train Loss: 3.9827, Val Loss: 4.0888\n",
      "Epoch 42/500 - Train Loss: 3.9569, Val Loss: 4.0702\n",
      "Epoch 43/500 - Train Loss: 3.9519, Val Loss: 4.0420\n",
      "Epoch 44/500 - Train Loss: 3.9090, Val Loss: 4.0125\n",
      "Epoch 45/500 - Train Loss: 3.8821, Val Loss: 4.0519\n",
      "Epoch 46/500 - Train Loss: 3.8765, Val Loss: 4.0552\n",
      "Epoch 47/500 - Train Loss: 3.8565, Val Loss: 4.0061\n",
      "Epoch 48/500 - Train Loss: 3.8412, Val Loss: 3.9568\n",
      "Epoch 49/500 - Train Loss: 3.8219, Val Loss: 3.9566\n",
      "Epoch 50/500 - Train Loss: 3.8116, Val Loss: 3.9362\n",
      "Epoch 51/500 - Train Loss: 3.7774, Val Loss: 3.9769\n",
      "Epoch 52/500 - Train Loss: 3.7508, Val Loss: 3.9428\n",
      "Epoch 53/500 - Train Loss: 3.7477, Val Loss: 3.9508\n",
      "Epoch 54/500 - Train Loss: 3.7483, Val Loss: 3.9148\n",
      "Epoch 55/500 - Train Loss: 3.7424, Val Loss: 3.9031\n",
      "Epoch 56/500 - Train Loss: 3.7451, Val Loss: 3.8953\n",
      "Epoch 57/500 - Train Loss: 3.6924, Val Loss: 3.8848\n",
      "Epoch 58/500 - Train Loss: 3.6992, Val Loss: 3.8515\n",
      "Epoch 59/500 - Train Loss: 3.6894, Val Loss: 3.8625\n",
      "Epoch 60/500 - Train Loss: 3.7093, Val Loss: 3.8637\n",
      "Epoch 61/500 - Train Loss: 3.7016, Val Loss: 3.8257\n",
      "Epoch 62/500 - Train Loss: 3.6614, Val Loss: 3.7917\n",
      "Epoch 63/500 - Train Loss: 3.6424, Val Loss: 3.8195\n",
      "Epoch 64/500 - Train Loss: 3.6471, Val Loss: 3.8031\n",
      "Epoch 65/500 - Train Loss: 3.6512, Val Loss: 3.7919\n",
      "Epoch 66/500 - Train Loss: 3.6139, Val Loss: 3.8278\n",
      "Epoch 67/500 - Train Loss: 3.6221, Val Loss: 3.8251\n",
      "Epoch 68/500 - Train Loss: 3.5989, Val Loss: 3.8066\n",
      "Epoch 69/500 - Train Loss: 3.5983, Val Loss: 3.8430\n",
      "Epoch 70/500 - Train Loss: 3.6056, Val Loss: 3.7626\n",
      "Epoch 71/500 - Train Loss: 3.5979, Val Loss: 3.8241\n",
      "Epoch 72/500 - Train Loss: 3.5618, Val Loss: 3.7641\n",
      "Epoch 73/500 - Train Loss: 3.5570, Val Loss: 3.7760\n",
      "Epoch 74/500 - Train Loss: 3.5929, Val Loss: 3.7443\n",
      "Epoch 75/500 - Train Loss: 3.6055, Val Loss: 3.7319\n",
      "Epoch 76/500 - Train Loss: 3.5563, Val Loss: 3.7281\n",
      "Epoch 77/500 - Train Loss: 3.6160, Val Loss: 3.7423\n",
      "Epoch 78/500 - Train Loss: 3.5558, Val Loss: 3.7443\n",
      "Epoch 79/500 - Train Loss: 3.5430, Val Loss: 3.7118\n",
      "Epoch 80/500 - Train Loss: 3.5840, Val Loss: 3.6857\n",
      "Epoch 81/500 - Train Loss: 3.5050, Val Loss: 3.6727\n",
      "Epoch 82/500 - Train Loss: 3.5424, Val Loss: 3.6873\n",
      "Epoch 83/500 - Train Loss: 3.5855, Val Loss: 3.7019\n",
      "Epoch 84/500 - Train Loss: 3.4889, Val Loss: 3.6764\n",
      "Epoch 85/500 - Train Loss: 3.5228, Val Loss: 3.7015\n",
      "Epoch 86/500 - Train Loss: 3.5199, Val Loss: 3.6998\n",
      "Epoch 87/500 - Train Loss: 3.5634, Val Loss: 3.6906\n",
      "Epoch 88/500 - Train Loss: 3.5077, Val Loss: 3.7061\n",
      "Epoch 89/500 - Train Loss: 3.4918, Val Loss: 3.6766\n",
      "Epoch 90/500 - Train Loss: 3.4829, Val Loss: 3.6509\n",
      "Epoch 91/500 - Train Loss: 3.5058, Val Loss: 3.6296\n",
      "Epoch 92/500 - Train Loss: 3.4760, Val Loss: 3.6835\n",
      "Epoch 93/500 - Train Loss: 3.4536, Val Loss: 3.6746\n",
      "Epoch 94/500 - Train Loss: 3.4970, Val Loss: 3.7312\n",
      "Epoch 95/500 - Train Loss: 3.4617, Val Loss: 3.6535\n",
      "Epoch 96/500 - Train Loss: 3.5134, Val Loss: 3.6956\n",
      "Epoch 97/500 - Train Loss: 3.4816, Val Loss: 3.6437\n",
      "Epoch 98/500 - Train Loss: 3.4748, Val Loss: 3.6482\n",
      "Epoch 99/500 - Train Loss: 3.4895, Val Loss: 3.6872\n",
      "Epoch 100/500 - Train Loss: 3.4641, Val Loss: 3.6571\n",
      "Epoch 101/500 - Train Loss: 3.4630, Val Loss: 3.6587\n",
      "Epoch 102/500 - Train Loss: 3.4368, Val Loss: 3.6457\n",
      "Epoch 103/500 - Train Loss: 3.4746, Val Loss: 3.6593\n",
      "Epoch 104/500 - Train Loss: 3.4845, Val Loss: 3.6156\n",
      "Epoch 105/500 - Train Loss: 3.4651, Val Loss: 3.6214\n",
      "Epoch 106/500 - Train Loss: 3.4490, Val Loss: 3.6530\n",
      "Epoch 107/500 - Train Loss: 3.5179, Val Loss: 3.6602\n",
      "Epoch 108/500 - Train Loss: 3.4353, Val Loss: 3.6546\n",
      "Epoch 109/500 - Train Loss: 3.4218, Val Loss: 3.6213\n",
      "Epoch 110/500 - Train Loss: 3.5049, Val Loss: 3.6200\n",
      "Epoch 111/500 - Train Loss: 3.4257, Val Loss: 3.6309\n",
      "Epoch 112/500 - Train Loss: 3.4624, Val Loss: 3.6398\n",
      "Epoch 113/500 - Train Loss: 3.4370, Val Loss: 3.6178\n",
      "Epoch 114/500 - Train Loss: 3.4537, Val Loss: 3.6538\n",
      "Epoch 115/500 - Train Loss: 3.4207, Val Loss: 3.5933\n",
      "Epoch 116/500 - Train Loss: 3.3706, Val Loss: 3.6083\n",
      "Epoch 117/500 - Train Loss: 3.4117, Val Loss: 3.6030\n",
      "Epoch 118/500 - Train Loss: 3.4581, Val Loss: 3.6024\n",
      "Epoch 119/500 - Train Loss: 3.4602, Val Loss: 3.6447\n",
      "Epoch 120/500 - Train Loss: 3.4314, Val Loss: 3.5799\n",
      "Epoch 121/500 - Train Loss: 3.3982, Val Loss: 3.6238\n",
      "Epoch 122/500 - Train Loss: 3.4031, Val Loss: 3.6340\n",
      "Epoch 123/500 - Train Loss: 3.3929, Val Loss: 3.5856\n",
      "Epoch 124/500 - Train Loss: 3.4255, Val Loss: 3.6387\n",
      "Epoch 125/500 - Train Loss: 3.3983, Val Loss: 3.5808\n",
      "Epoch 126/500 - Train Loss: 3.4055, Val Loss: 3.5903\n",
      "Epoch 127/500 - Train Loss: 3.3986, Val Loss: 3.5672\n",
      "Epoch 128/500 - Train Loss: 3.4230, Val Loss: 3.5797\n",
      "Epoch 129/500 - Train Loss: 3.3878, Val Loss: 3.5826\n",
      "Epoch 130/500 - Train Loss: 3.4154, Val Loss: 3.6372\n",
      "Epoch 131/500 - Train Loss: 3.4030, Val Loss: 3.6166\n",
      "Epoch 132/500 - Train Loss: 3.4110, Val Loss: 3.5841\n",
      "Epoch 133/500 - Train Loss: 3.3945, Val Loss: 3.5830\n",
      "Epoch 134/500 - Train Loss: 3.3888, Val Loss: 3.5599\n",
      "Epoch 135/500 - Train Loss: 3.3544, Val Loss: 3.5837\n",
      "Epoch 136/500 - Train Loss: 3.3973, Val Loss: 3.5502\n",
      "Epoch 137/500 - Train Loss: 3.3429, Val Loss: 3.6186\n",
      "Epoch 138/500 - Train Loss: 3.3897, Val Loss: 3.5477\n",
      "Epoch 139/500 - Train Loss: 3.3748, Val Loss: 3.5667\n",
      "Epoch 140/500 - Train Loss: 3.3657, Val Loss: 3.5961\n",
      "Epoch 141/500 - Train Loss: 3.3667, Val Loss: 3.5758\n",
      "Epoch 142/500 - Train Loss: 3.3802, Val Loss: 3.5644\n",
      "Epoch 143/500 - Train Loss: 3.4086, Val Loss: 3.6010\n",
      "Epoch 144/500 - Train Loss: 3.3489, Val Loss: 3.5659\n",
      "Epoch 145/500 - Train Loss: 3.3501, Val Loss: 3.5450\n",
      "Epoch 146/500 - Train Loss: 3.3660, Val Loss: 3.5311\n",
      "Epoch 147/500 - Train Loss: 3.3432, Val Loss: 3.5738\n",
      "Epoch 148/500 - Train Loss: 3.3565, Val Loss: 3.5586\n",
      "Epoch 149/500 - Train Loss: 3.3463, Val Loss: 3.5872\n",
      "Epoch 150/500 - Train Loss: 3.3589, Val Loss: 3.5252\n",
      "Epoch 151/500 - Train Loss: 3.3843, Val Loss: 3.5171\n",
      "Epoch 152/500 - Train Loss: 3.3607, Val Loss: 3.4797\n",
      "Epoch 153/500 - Train Loss: 3.3634, Val Loss: 3.5436\n",
      "Epoch 154/500 - Train Loss: 3.3351, Val Loss: 3.5542\n",
      "Epoch 155/500 - Train Loss: 3.3414, Val Loss: 3.6139\n",
      "Epoch 156/500 - Train Loss: 3.3698, Val Loss: 3.5298\n",
      "Epoch 157/500 - Train Loss: 3.3759, Val Loss: 3.5715\n",
      "Epoch 158/500 - Train Loss: 3.3555, Val Loss: 3.5713\n",
      "Epoch 159/500 - Train Loss: 3.3541, Val Loss: 3.5387\n",
      "Epoch 160/500 - Train Loss: 3.3313, Val Loss: 3.5198\n",
      "Epoch 161/500 - Train Loss: 3.3552, Val Loss: 3.5472\n",
      "Epoch 162/500 - Train Loss: 3.3065, Val Loss: 3.5449\n",
      "Epoch 163/500 - Train Loss: 3.3424, Val Loss: 3.5215\n",
      "Epoch 164/500 - Train Loss: 3.2917, Val Loss: 3.5515\n",
      "Epoch 165/500 - Train Loss: 3.2966, Val Loss: 3.5452\n",
      "Epoch 166/500 - Train Loss: 3.3219, Val Loss: 3.5498\n",
      "Epoch 167/500 - Train Loss: 3.3239, Val Loss: 3.5458\n",
      "Epoch 168/500 - Train Loss: 3.3231, Val Loss: 3.5437\n",
      "Epoch 169/500 - Train Loss: 3.2762, Val Loss: 3.5721\n",
      "Epoch 170/500 - Train Loss: 3.3435, Val Loss: 3.4544\n",
      "Epoch 171/500 - Train Loss: 3.3156, Val Loss: 3.5574\n",
      "Epoch 172/500 - Train Loss: 3.3294, Val Loss: 3.5458\n",
      "Epoch 173/500 - Train Loss: 3.2497, Val Loss: 3.5348\n",
      "Epoch 174/500 - Train Loss: 3.3027, Val Loss: 3.5221\n",
      "Epoch 175/500 - Train Loss: 3.3048, Val Loss: 3.5027\n",
      "Epoch 176/500 - Train Loss: 3.3140, Val Loss: 3.5410\n",
      "Epoch 177/500 - Train Loss: 3.3189, Val Loss: 3.4783\n",
      "Epoch 178/500 - Train Loss: 3.2503, Val Loss: 3.5327\n",
      "Epoch 179/500 - Train Loss: 3.2804, Val Loss: 3.4524\n",
      "Epoch 180/500 - Train Loss: 3.2923, Val Loss: 3.5217\n",
      "Epoch 181/500 - Train Loss: 3.3153, Val Loss: 3.4798\n",
      "Epoch 182/500 - Train Loss: 3.2964, Val Loss: 3.4824\n",
      "Epoch 183/500 - Train Loss: 3.3007, Val Loss: 3.4899\n",
      "Epoch 184/500 - Train Loss: 3.2468, Val Loss: 3.5124\n",
      "Epoch 185/500 - Train Loss: 3.3046, Val Loss: 3.5050\n",
      "Epoch 186/500 - Train Loss: 3.3190, Val Loss: 3.4999\n",
      "Epoch 187/500 - Train Loss: 3.3223, Val Loss: 3.4771\n",
      "Epoch 188/500 - Train Loss: 3.2345, Val Loss: 3.4870\n",
      "Epoch 189/500 - Train Loss: 3.2588, Val Loss: 3.4744\n",
      "Epoch 190/500 - Train Loss: 3.2902, Val Loss: 3.5058\n",
      "Epoch 191/500 - Train Loss: 3.2416, Val Loss: 3.5240\n",
      "Epoch 192/500 - Train Loss: 3.2709, Val Loss: 3.5092\n",
      "Epoch 193/500 - Train Loss: 3.2203, Val Loss: 3.4552\n",
      "Epoch 194/500 - Train Loss: 3.2436, Val Loss: 3.5373\n",
      "Epoch 195/500 - Train Loss: 3.2023, Val Loss: 3.4454\n",
      "Epoch 196/500 - Train Loss: 3.2439, Val Loss: 3.4675\n",
      "Epoch 197/500 - Train Loss: 3.2688, Val Loss: 3.4820\n",
      "Epoch 198/500 - Train Loss: 3.2368, Val Loss: 3.4594\n",
      "Epoch 199/500 - Train Loss: 3.2376, Val Loss: 3.4757\n",
      "Epoch 200/500 - Train Loss: 3.2122, Val Loss: 3.4756\n",
      "Epoch 201/500 - Train Loss: 3.2033, Val Loss: 3.4678\n",
      "Epoch 202/500 - Train Loss: 3.2094, Val Loss: 3.4625\n",
      "Epoch 203/500 - Train Loss: 3.2353, Val Loss: 3.4511\n",
      "Epoch 204/500 - Train Loss: 3.1729, Val Loss: 3.5037\n",
      "Epoch 205/500 - Train Loss: 3.2523, Val Loss: 3.4118\n",
      "Epoch 206/500 - Train Loss: 3.2446, Val Loss: 3.4525\n",
      "Epoch 207/500 - Train Loss: 3.1868, Val Loss: 3.4076\n",
      "Epoch 208/500 - Train Loss: 3.2413, Val Loss: 3.4156\n",
      "Epoch 209/500 - Train Loss: 3.2008, Val Loss: 3.4387\n",
      "Epoch 210/500 - Train Loss: 3.1952, Val Loss: 3.4565\n",
      "Epoch 211/500 - Train Loss: 3.1797, Val Loss: 3.4079\n",
      "Epoch 212/500 - Train Loss: 3.1909, Val Loss: 3.4292\n",
      "Epoch 213/500 - Train Loss: 3.1325, Val Loss: 3.3930\n",
      "Epoch 214/500 - Train Loss: 3.2050, Val Loss: 3.4635\n",
      "Epoch 215/500 - Train Loss: 3.1889, Val Loss: 3.4268\n",
      "Epoch 216/500 - Train Loss: 3.1286, Val Loss: 3.4636\n",
      "Epoch 217/500 - Train Loss: 3.1714, Val Loss: 3.4439\n",
      "Epoch 218/500 - Train Loss: 3.1398, Val Loss: 3.4181\n",
      "Epoch 219/500 - Train Loss: 3.1893, Val Loss: 3.4182\n",
      "Epoch 220/500 - Train Loss: 3.1297, Val Loss: 3.4614\n",
      "Epoch 221/500 - Train Loss: 3.1536, Val Loss: 3.4117\n",
      "Epoch 222/500 - Train Loss: 3.1512, Val Loss: 3.4014\n",
      "Epoch 223/500 - Train Loss: 3.1785, Val Loss: 3.4295\n",
      "Epoch 224/500 - Train Loss: 3.1720, Val Loss: 3.3818\n",
      "Epoch 225/500 - Train Loss: 3.0835, Val Loss: 3.3938\n",
      "Epoch 226/500 - Train Loss: 3.1107, Val Loss: 3.3847\n",
      "Epoch 227/500 - Train Loss: 3.1195, Val Loss: 3.3327\n",
      "Epoch 228/500 - Train Loss: 3.1313, Val Loss: 3.3707\n",
      "Epoch 229/500 - Train Loss: 3.1121, Val Loss: 3.3815\n",
      "Epoch 230/500 - Train Loss: 3.1187, Val Loss: 3.3495\n",
      "Epoch 231/500 - Train Loss: 3.0981, Val Loss: 3.3130\n",
      "Epoch 232/500 - Train Loss: 3.0971, Val Loss: 3.3805\n",
      "Epoch 233/500 - Train Loss: 3.1242, Val Loss: 3.3573\n",
      "Epoch 234/500 - Train Loss: 3.1075, Val Loss: 3.3758\n",
      "Epoch 235/500 - Train Loss: 3.1233, Val Loss: 3.3106\n",
      "Epoch 236/500 - Train Loss: 3.1100, Val Loss: 3.3437\n",
      "Epoch 237/500 - Train Loss: 3.0657, Val Loss: 3.4091\n",
      "Epoch 238/500 - Train Loss: 3.0937, Val Loss: 3.3685\n",
      "Epoch 239/500 - Train Loss: 3.1000, Val Loss: 3.3714\n",
      "Epoch 240/500 - Train Loss: 3.1456, Val Loss: 3.2889\n",
      "Epoch 241/500 - Train Loss: 3.1039, Val Loss: 3.3658\n",
      "Epoch 242/500 - Train Loss: 3.1335, Val Loss: 3.4006\n",
      "Epoch 243/500 - Train Loss: 3.0976, Val Loss: 3.3423\n",
      "Epoch 244/500 - Train Loss: 3.0493, Val Loss: 3.3366\n",
      "Epoch 245/500 - Train Loss: 3.0371, Val Loss: 3.2703\n",
      "Epoch 246/500 - Train Loss: 3.0395, Val Loss: 3.3482\n",
      "Epoch 247/500 - Train Loss: 3.0773, Val Loss: 3.3228\n",
      "Epoch 248/500 - Train Loss: 3.0496, Val Loss: 3.2654\n",
      "Epoch 249/500 - Train Loss: 3.0205, Val Loss: 3.3370\n",
      "Epoch 250/500 - Train Loss: 3.0068, Val Loss: 3.2463\n",
      "Epoch 251/500 - Train Loss: 3.0021, Val Loss: 3.2871\n",
      "Epoch 252/500 - Train Loss: 2.9746, Val Loss: 3.3237\n",
      "Epoch 253/500 - Train Loss: 3.0452, Val Loss: 3.2920\n",
      "Epoch 254/500 - Train Loss: 3.0493, Val Loss: 3.2521\n",
      "Epoch 255/500 - Train Loss: 3.0185, Val Loss: 3.3293\n",
      "Epoch 256/500 - Train Loss: 3.0512, Val Loss: 3.2711\n",
      "Epoch 257/500 - Train Loss: 3.0038, Val Loss: 3.2981\n",
      "Epoch 258/500 - Train Loss: 3.0249, Val Loss: 3.3044\n",
      "Epoch 259/500 - Train Loss: 2.9700, Val Loss: 3.3428\n",
      "Epoch 260/500 - Train Loss: 2.9595, Val Loss: 3.3247\n",
      "Epoch 261/500 - Train Loss: 2.9969, Val Loss: 3.3159\n",
      "Epoch 262/500 - Train Loss: 2.9489, Val Loss: 3.2630\n",
      "Epoch 263/500 - Train Loss: 2.9737, Val Loss: 3.2439\n",
      "Epoch 264/500 - Train Loss: 2.9514, Val Loss: 3.2404\n",
      "Epoch 265/500 - Train Loss: 2.9983, Val Loss: 3.2815\n",
      "Epoch 266/500 - Train Loss: 3.0059, Val Loss: 3.2918\n",
      "Epoch 267/500 - Train Loss: 2.9782, Val Loss: 3.3285\n",
      "Epoch 268/500 - Train Loss: 2.9369, Val Loss: 3.2587\n",
      "Epoch 269/500 - Train Loss: 2.9755, Val Loss: 3.2612\n",
      "Epoch 270/500 - Train Loss: 2.9454, Val Loss: 3.2920\n",
      "Epoch 271/500 - Train Loss: 2.9714, Val Loss: 3.2721\n",
      "Epoch 272/500 - Train Loss: 2.9393, Val Loss: 3.2469\n",
      "Epoch 273/500 - Train Loss: 2.9612, Val Loss: 3.1825\n",
      "Epoch 274/500 - Train Loss: 2.9237, Val Loss: 3.2608\n",
      "Epoch 275/500 - Train Loss: 2.9577, Val Loss: 3.2552\n",
      "Epoch 276/500 - Train Loss: 2.9741, Val Loss: 3.3046\n",
      "Epoch 277/500 - Train Loss: 2.9341, Val Loss: 3.2093\n",
      "Epoch 278/500 - Train Loss: 2.9319, Val Loss: 3.2490\n",
      "Epoch 279/500 - Train Loss: 2.9755, Val Loss: 3.2880\n",
      "Epoch 280/500 - Train Loss: 2.9542, Val Loss: 3.2091\n",
      "Epoch 281/500 - Train Loss: 2.9178, Val Loss: 3.2747\n",
      "Epoch 282/500 - Train Loss: 2.9523, Val Loss: 3.2506\n",
      "Epoch 283/500 - Train Loss: 2.9278, Val Loss: 3.2929\n",
      "Epoch 284/500 - Train Loss: 2.8699, Val Loss: 3.2081\n",
      "Epoch 285/500 - Train Loss: 2.8649, Val Loss: 3.2460\n",
      "Epoch 286/500 - Train Loss: 2.8460, Val Loss: 3.2264\n",
      "Epoch 287/500 - Train Loss: 2.8824, Val Loss: 3.1836\n",
      "Epoch 288/500 - Train Loss: 2.9286, Val Loss: 3.2031\n",
      "Epoch 289/500 - Train Loss: 2.8756, Val Loss: 3.2061\n",
      "Epoch 290/500 - Train Loss: 2.8689, Val Loss: 3.1979\n",
      "Epoch 291/500 - Train Loss: 2.8824, Val Loss: 3.1933\n",
      "Epoch 292/500 - Train Loss: 2.8268, Val Loss: 3.2309\n",
      "Epoch 293/500 - Train Loss: 2.8836, Val Loss: 3.1925\n",
      "Epoch 294/500 - Train Loss: 2.8991, Val Loss: 3.2088\n",
      "Epoch 295/500 - Train Loss: 2.8931, Val Loss: 3.2424\n",
      "Epoch 296/500 - Train Loss: 2.8766, Val Loss: 3.1750\n",
      "Epoch 297/500 - Train Loss: 2.8809, Val Loss: 3.2110\n",
      "Epoch 298/500 - Train Loss: 2.8570, Val Loss: 3.2037\n",
      "Epoch 299/500 - Train Loss: 2.8767, Val Loss: 3.2004\n",
      "Epoch 300/500 - Train Loss: 2.8791, Val Loss: 3.1798\n",
      "Epoch 301/500 - Train Loss: 2.8475, Val Loss: 3.1388\n",
      "Epoch 302/500 - Train Loss: 2.8342, Val Loss: 3.1988\n",
      "Epoch 303/500 - Train Loss: 2.8359, Val Loss: 3.1569\n",
      "Epoch 304/500 - Train Loss: 2.8603, Val Loss: 3.1786\n",
      "Epoch 305/500 - Train Loss: 2.8513, Val Loss: 3.1689\n",
      "Epoch 306/500 - Train Loss: 2.7645, Val Loss: 3.1965\n",
      "Epoch 307/500 - Train Loss: 2.8528, Val Loss: 3.1619\n",
      "Epoch 308/500 - Train Loss: 2.8199, Val Loss: 3.1798\n",
      "Epoch 309/500 - Train Loss: 2.7780, Val Loss: 3.1128\n",
      "Epoch 310/500 - Train Loss: 2.8418, Val Loss: 3.1696\n",
      "Epoch 311/500 - Train Loss: 2.7475, Val Loss: 3.1755\n",
      "Epoch 312/500 - Train Loss: 2.7959, Val Loss: 3.1462\n",
      "Epoch 313/500 - Train Loss: 2.8258, Val Loss: 3.1726\n",
      "Epoch 314/500 - Train Loss: 2.8253, Val Loss: 3.1767\n",
      "Epoch 315/500 - Train Loss: 2.8020, Val Loss: 3.1303\n",
      "Epoch 316/500 - Train Loss: 2.7994, Val Loss: 3.1730\n",
      "Epoch 317/500 - Train Loss: 2.8476, Val Loss: 3.1883\n",
      "Epoch 318/500 - Train Loss: 2.8192, Val Loss: 3.1569\n",
      "Epoch 319/500 - Train Loss: 2.7907, Val Loss: 3.0453\n",
      "Epoch 320/500 - Train Loss: 2.7858, Val Loss: 3.1849\n",
      "Epoch 321/500 - Train Loss: 2.8264, Val Loss: 3.1664\n",
      "Epoch 322/500 - Train Loss: 2.7449, Val Loss: 3.1312\n",
      "Epoch 323/500 - Train Loss: 2.7665, Val Loss: 3.1904\n",
      "Epoch 324/500 - Train Loss: 2.7635, Val Loss: 3.0803\n",
      "Epoch 325/500 - Train Loss: 2.7449, Val Loss: 3.2442\n",
      "Epoch 326/500 - Train Loss: 2.7973, Val Loss: 3.1341\n",
      "Epoch 327/500 - Train Loss: 2.7882, Val Loss: 3.1078\n",
      "Epoch 328/500 - Train Loss: 2.7088, Val Loss: 3.0729\n",
      "Epoch 329/500 - Train Loss: 2.7738, Val Loss: 3.1248\n",
      "Epoch 330/500 - Train Loss: 2.7289, Val Loss: 3.1113\n",
      "Epoch 331/500 - Train Loss: 2.7183, Val Loss: 3.1305\n",
      "Epoch 332/500 - Train Loss: 2.7631, Val Loss: 3.1215\n",
      "Epoch 333/500 - Train Loss: 2.7474, Val Loss: 3.0965\n",
      "Epoch 334/500 - Train Loss: 2.7444, Val Loss: 3.0632\n",
      "Epoch 335/500 - Train Loss: 2.7297, Val Loss: 3.0934\n",
      "Epoch 336/500 - Train Loss: 2.7395, Val Loss: 3.0556\n",
      "Epoch 337/500 - Train Loss: 2.7326, Val Loss: 3.1290\n",
      "Epoch 338/500 - Train Loss: 2.7426, Val Loss: 3.1101\n",
      "Epoch 339/500 - Train Loss: 2.7736, Val Loss: 3.0798\n",
      "Epoch 340/500 - Train Loss: 2.7046, Val Loss: 3.1199\n",
      "Epoch 341/500 - Train Loss: 2.7039, Val Loss: 3.1280\n",
      "Epoch 342/500 - Train Loss: 2.8076, Val Loss: 3.0970\n",
      "Epoch 343/500 - Train Loss: 2.7596, Val Loss: 3.0994\n",
      "Epoch 344/500 - Train Loss: 2.6755, Val Loss: 3.0939\n",
      "Epoch 345/500 - Train Loss: 2.7073, Val Loss: 3.1083\n",
      "Epoch 346/500 - Train Loss: 2.7094, Val Loss: 3.0751\n",
      "Epoch 347/500 - Train Loss: 2.7719, Val Loss: 3.0159\n",
      "Epoch 348/500 - Train Loss: 2.7234, Val Loss: 3.1068\n",
      "Epoch 349/500 - Train Loss: 2.6743, Val Loss: 3.1206\n",
      "Epoch 350/500 - Train Loss: 2.7052, Val Loss: 3.1105\n",
      "Epoch 351/500 - Train Loss: 2.6824, Val Loss: 3.1221\n",
      "Epoch 352/500 - Train Loss: 2.6915, Val Loss: 3.0317\n",
      "Epoch 353/500 - Train Loss: 2.7350, Val Loss: 3.1097\n",
      "Epoch 354/500 - Train Loss: 2.7496, Val Loss: 3.0818\n",
      "Epoch 355/500 - Train Loss: 2.6899, Val Loss: 3.0565\n",
      "Epoch 356/500 - Train Loss: 2.7331, Val Loss: 3.0719\n",
      "Epoch 357/500 - Train Loss: 2.6780, Val Loss: 3.1408\n",
      "Epoch 358/500 - Train Loss: 2.7007, Val Loss: 3.0754\n",
      "Epoch 359/500 - Train Loss: 2.6894, Val Loss: 3.0075\n",
      "Epoch 360/500 - Train Loss: 2.6466, Val Loss: 3.0681\n",
      "Epoch 361/500 - Train Loss: 2.7371, Val Loss: 3.0628\n",
      "Epoch 362/500 - Train Loss: 2.6565, Val Loss: 3.1047\n",
      "Epoch 363/500 - Train Loss: 2.7149, Val Loss: 3.1203\n",
      "Epoch 364/500 - Train Loss: 2.7537, Val Loss: 3.0461\n",
      "Epoch 365/500 - Train Loss: 2.6762, Val Loss: 3.0498\n",
      "Epoch 366/500 - Train Loss: 2.7122, Val Loss: 3.1226\n",
      "Epoch 367/500 - Train Loss: 2.6455, Val Loss: 3.0960\n",
      "Epoch 368/500 - Train Loss: 2.6528, Val Loss: 3.0568\n",
      "Epoch 369/500 - Train Loss: 2.6834, Val Loss: 3.0564\n",
      "Epoch 370/500 - Train Loss: 2.6755, Val Loss: 3.0389\n",
      "Epoch 371/500 - Train Loss: 2.6521, Val Loss: 3.0978\n",
      "Epoch 372/500 - Train Loss: 2.6627, Val Loss: 3.0671\n",
      "Epoch 373/500 - Train Loss: 2.6908, Val Loss: 3.0531\n",
      "Epoch 374/500 - Train Loss: 2.6360, Val Loss: 3.0062\n",
      "Epoch 375/500 - Train Loss: 2.6747, Val Loss: 3.0603\n",
      "Epoch 376/500 - Train Loss: 2.6139, Val Loss: 3.0387\n",
      "Epoch 377/500 - Train Loss: 2.6213, Val Loss: 3.0598\n",
      "Epoch 378/500 - Train Loss: 2.6234, Val Loss: 3.0675\n",
      "Epoch 379/500 - Train Loss: 2.6721, Val Loss: 3.0388\n",
      "Epoch 380/500 - Train Loss: 2.7217, Val Loss: 3.0411\n",
      "Epoch 381/500 - Train Loss: 2.6609, Val Loss: 2.9987\n",
      "Epoch 382/500 - Train Loss: 2.6504, Val Loss: 3.0713\n",
      "Epoch 383/500 - Train Loss: 2.6970, Val Loss: 3.0046\n",
      "Epoch 384/500 - Train Loss: 2.6404, Val Loss: 3.0159\n",
      "Epoch 385/500 - Train Loss: 2.6199, Val Loss: 3.0012\n",
      "Epoch 386/500 - Train Loss: 2.6487, Val Loss: 3.0364\n",
      "Epoch 387/500 - Train Loss: 2.5958, Val Loss: 3.1183\n",
      "Epoch 388/500 - Train Loss: 2.6424, Val Loss: 3.0039\n",
      "Epoch 389/500 - Train Loss: 2.6307, Val Loss: 3.0580\n",
      "Epoch 390/500 - Train Loss: 2.5623, Val Loss: 2.9995\n",
      "Epoch 391/500 - Train Loss: 2.6280, Val Loss: 3.0868\n",
      "Epoch 392/500 - Train Loss: 2.6545, Val Loss: 3.0332\n",
      "Epoch 393/500 - Train Loss: 2.6046, Val Loss: 3.0467\n",
      "Epoch 394/500 - Train Loss: 2.5597, Val Loss: 3.1239\n",
      "Epoch 395/500 - Train Loss: 2.6613, Val Loss: 3.0005\n",
      "Epoch 396/500 - Train Loss: 2.6145, Val Loss: 3.1062\n",
      "Epoch 397/500 - Train Loss: 2.6314, Val Loss: 3.0149\n",
      "Epoch 398/500 - Train Loss: 2.6549, Val Loss: 3.1238\n",
      "Epoch 399/500 - Train Loss: 2.5958, Val Loss: 2.9721\n",
      "Epoch 400/500 - Train Loss: 2.6373, Val Loss: 3.0375\n",
      "Epoch 401/500 - Train Loss: 2.5786, Val Loss: 2.9961\n",
      "Epoch 402/500 - Train Loss: 2.6098, Val Loss: 3.0017\n",
      "Epoch 403/500 - Train Loss: 2.6007, Val Loss: 3.0070\n",
      "Epoch 404/500 - Train Loss: 2.6335, Val Loss: 2.9918\n",
      "Epoch 405/500 - Train Loss: 2.6093, Val Loss: 3.0055\n",
      "Epoch 406/500 - Train Loss: 2.6731, Val Loss: 3.0350\n",
      "Epoch 407/500 - Train Loss: 2.6040, Val Loss: 2.9890\n",
      "Epoch 408/500 - Train Loss: 2.6552, Val Loss: 3.0005\n",
      "Epoch 409/500 - Train Loss: 2.5951, Val Loss: 3.0341\n",
      "Epoch 410/500 - Train Loss: 2.6302, Val Loss: 2.9926\n",
      "Epoch 411/500 - Train Loss: 2.5614, Val Loss: 3.0032\n",
      "Epoch 412/500 - Train Loss: 2.6040, Val Loss: 3.0370\n",
      "Epoch 413/500 - Train Loss: 2.6108, Val Loss: 3.0481\n",
      "Epoch 414/500 - Train Loss: 2.6003, Val Loss: 2.9463\n",
      "Epoch 415/500 - Train Loss: 2.5695, Val Loss: 2.9777\n",
      "Epoch 416/500 - Train Loss: 2.5541, Val Loss: 2.9846\n",
      "Epoch 417/500 - Train Loss: 2.5577, Val Loss: 2.9840\n",
      "Epoch 418/500 - Train Loss: 2.5908, Val Loss: 3.0096\n",
      "Epoch 419/500 - Train Loss: 2.6235, Val Loss: 3.0181\n",
      "Epoch 420/500 - Train Loss: 2.6451, Val Loss: 3.0503\n",
      "Epoch 421/500 - Train Loss: 2.6143, Val Loss: 2.9924\n",
      "Epoch 422/500 - Train Loss: 2.6107, Val Loss: 3.1028\n",
      "Epoch 423/500 - Train Loss: 2.5612, Val Loss: 2.9971\n",
      "Epoch 424/500 - Train Loss: 2.4929, Val Loss: 2.9979\n",
      "Epoch 425/500 - Train Loss: 2.5451, Val Loss: 2.9778\n",
      "Epoch 426/500 - Train Loss: 2.5278, Val Loss: 3.0375\n",
      "Epoch 427/500 - Train Loss: 2.5444, Val Loss: 2.9891\n",
      "Epoch 428/500 - Train Loss: 2.5638, Val Loss: 3.0058\n",
      "Epoch 429/500 - Train Loss: 2.6143, Val Loss: 3.0718\n",
      "Epoch 430/500 - Train Loss: 2.6064, Val Loss: 2.9532\n",
      "Epoch 431/500 - Train Loss: 2.5879, Val Loss: 3.0235\n",
      "Epoch 432/500 - Train Loss: 2.5755, Val Loss: 2.9824\n",
      "Epoch 433/500 - Train Loss: 2.5194, Val Loss: 2.9372\n",
      "Epoch 434/500 - Train Loss: 2.5465, Val Loss: 2.9277\n",
      "Epoch 435/500 - Train Loss: 2.5741, Val Loss: 2.9901\n",
      "Epoch 436/500 - Train Loss: 2.5554, Val Loss: 2.9273\n",
      "Epoch 437/500 - Train Loss: 2.5653, Val Loss: 2.9836\n",
      "Epoch 438/500 - Train Loss: 2.5405, Val Loss: 2.9352\n",
      "Epoch 439/500 - Train Loss: 2.5360, Val Loss: 2.9930\n",
      "Epoch 440/500 - Train Loss: 2.5253, Val Loss: 3.0301\n",
      "Epoch 441/500 - Train Loss: 2.5525, Val Loss: 2.9342\n",
      "Epoch 442/500 - Train Loss: 2.5463, Val Loss: 2.9887\n",
      "Epoch 443/500 - Train Loss: 2.5728, Val Loss: 3.0049\n",
      "Epoch 444/500 - Train Loss: 2.5351, Val Loss: 2.9839\n",
      "Epoch 445/500 - Train Loss: 2.5512, Val Loss: 2.9587\n",
      "Epoch 446/500 - Train Loss: 2.5838, Val Loss: 2.9728\n",
      "Epoch 447/500 - Train Loss: 2.5465, Val Loss: 3.0050\n",
      "Epoch 448/500 - Train Loss: 2.5512, Val Loss: 2.9296\n",
      "Epoch 449/500 - Train Loss: 2.5039, Val Loss: 2.9176\n",
      "Epoch 450/500 - Train Loss: 2.5309, Val Loss: 2.9793\n",
      "Epoch 451/500 - Train Loss: 2.5128, Val Loss: 2.9401\n",
      "Epoch 452/500 - Train Loss: 2.5567, Val Loss: 3.0089\n",
      "Epoch 453/500 - Train Loss: 2.5609, Val Loss: 2.9641\n",
      "Epoch 454/500 - Train Loss: 2.4584, Val Loss: 2.8836\n",
      "Epoch 455/500 - Train Loss: 2.5346, Val Loss: 2.9628\n",
      "Epoch 456/500 - Train Loss: 2.4968, Val Loss: 2.9866\n",
      "Epoch 457/500 - Train Loss: 2.4394, Val Loss: 3.0086\n",
      "Epoch 458/500 - Train Loss: 2.5436, Val Loss: 2.9630\n",
      "Epoch 459/500 - Train Loss: 2.5195, Val Loss: 3.0272\n",
      "Epoch 460/500 - Train Loss: 2.5407, Val Loss: 2.9758\n",
      "Epoch 461/500 - Train Loss: 2.5324, Val Loss: 2.9382\n",
      "Epoch 462/500 - Train Loss: 2.4774, Val Loss: 2.9468\n",
      "Epoch 463/500 - Train Loss: 2.5041, Val Loss: 2.9612\n",
      "Epoch 464/500 - Train Loss: 2.5592, Val Loss: 2.9172\n",
      "Epoch 465/500 - Train Loss: 2.4965, Val Loss: 2.9265\n",
      "Epoch 466/500 - Train Loss: 2.4822, Val Loss: 2.9351\n",
      "Epoch 467/500 - Train Loss: 2.5509, Val Loss: 3.0527\n",
      "Epoch 468/500 - Train Loss: 2.4963, Val Loss: 3.0690\n",
      "Epoch 469/500 - Train Loss: 2.4764, Val Loss: 2.9468\n",
      "Epoch 470/500 - Train Loss: 2.4908, Val Loss: 2.9097\n",
      "Epoch 471/500 - Train Loss: 2.4622, Val Loss: 2.9123\n",
      "Epoch 472/500 - Train Loss: 2.4800, Val Loss: 2.9389\n",
      "Epoch 473/500 - Train Loss: 2.5128, Val Loss: 2.9459\n",
      "Epoch 474/500 - Train Loss: 2.5213, Val Loss: 2.9402\n",
      "Epoch 475/500 - Train Loss: 2.4834, Val Loss: 2.9558\n",
      "Epoch 476/500 - Train Loss: 2.4819, Val Loss: 3.0082\n",
      "Epoch 477/500 - Train Loss: 2.5258, Val Loss: 2.9171\n",
      "Epoch 478/500 - Train Loss: 2.4730, Val Loss: 3.0024\n",
      "Epoch 479/500 - Train Loss: 2.4509, Val Loss: 2.9368\n",
      "Epoch 480/500 - Train Loss: 2.4876, Val Loss: 2.9873\n",
      "Epoch 481/500 - Train Loss: 2.4361, Val Loss: 2.9668\n",
      "Epoch 482/500 - Train Loss: 2.5080, Val Loss: 2.9572\n",
      "Epoch 483/500 - Train Loss: 2.5163, Val Loss: 2.8924\n",
      "Epoch 484/500 - Train Loss: 2.4477, Val Loss: 3.0383\n",
      "Epoch 485/500 - Train Loss: 2.4640, Val Loss: 2.9413\n",
      "Epoch 486/500 - Train Loss: 2.5082, Val Loss: 2.9436\n",
      "Epoch 487/500 - Train Loss: 2.4695, Val Loss: 2.8891\n",
      "Epoch 488/500 - Train Loss: 2.4623, Val Loss: 2.9466\n",
      "Epoch 489/500 - Train Loss: 2.4173, Val Loss: 2.8905\n",
      "Epoch 490/500 - Train Loss: 2.4287, Val Loss: 2.9180\n",
      "Epoch 491/500 - Train Loss: 2.4889, Val Loss: 2.9139\n",
      "Epoch 492/500 - Train Loss: 2.4456, Val Loss: 2.9781\n",
      "Epoch 493/500 - Train Loss: 2.4940, Val Loss: 2.9592\n",
      "Epoch 494/500 - Train Loss: 2.4844, Val Loss: 2.9558\n",
      "Epoch 495/500 - Train Loss: 2.4638, Val Loss: 2.9662\n",
      "Epoch 496/500 - Train Loss: 2.4890, Val Loss: 2.9522\n",
      "Epoch 497/500 - Train Loss: 2.4653, Val Loss: 2.9579\n",
      "Epoch 498/500 - Train Loss: 2.4572, Val Loss: 2.8952\n",
      "Epoch 499/500 - Train Loss: 2.4866, Val Loss: 2.9522\n",
      "Epoch 500/500 - Train Loss: 2.5014, Val Loss: 2.9792\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "language_model.train_model(\n",
    "    data_loader = data_handler,\n",
    "    epochs = epochs, \n",
    "    lr = learning_rate, \n",
    "    batch_size = batch_size,\n",
    "    eval_iters = eval_iters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000Tart thou hast scolder in a done:\n",
      "Say she shall save but it your ships,\n",
      "And to think not be bound Go, young low incheeks of mine,\n",
      "Before I have not to my sick go, not your passing revenge\n",
      "Would not re\n"
     ]
    }
   ],
   "source": [
    "# Generate some text from the trained model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "# Decode and display the generated text\n",
    "print(tokenizer.decode(language_model.generate(context, max_new_tokens=100).squeeze().tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
