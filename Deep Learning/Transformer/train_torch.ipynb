{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from lib.torch.utils import device\n",
    "from lib.utils import load_txt_file\n",
    "from lib.torch.data_loader import DataLoader\n",
    "from lib.torch.transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dataset_path = os.path.join(os.getcwd(), 'dataset', 'input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "train_val_split = 0.9 # 90% of the data will be used for training, 10% for validation\n",
    "batch_size = 64 # The number of samples to use for each batch\n",
    "block_size = 256 # The size of the sequence length (the context window)\n",
    "learning_rate = 3e-4 # The learning rate for the optimizer\n",
    "epochs = 500 # The number of epochs to train the model for\n",
    "n_embed = 384 # The size of the token embeddings (the dimensionality of the embeddings)\n",
    "eval_iters = 1 # The number of iterations to evaluate the model\n",
    "num_attention_heads = 6 # The number of attention heads in the multi-head attention mechanism\n",
    "num_transformer_blocks = 6 # The number of transformer blocks in the model\n",
    "dropout = 0.2 # The dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11daefbb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(1337);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "text = load_txt_file(dataset_path)\n",
    "\n",
    "# Extract the unique characters (vocabulary)\n",
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Creating a simple mapping from characters to integers\n",
    "char_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_char = {i: c for i, c in enumerate(vocab)}\n",
    "\n",
    "# Creating the encoding and decoding functions\n",
    "encode = lambda text: [char_to_int[c] for c in text]\n",
    "decode = lambda tokens: ''.join([int_to_char[t] for t in tokens])\n",
    "\n",
    "# Convert the data to a tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the data handler\n",
    "data_handler = DataLoader(\n",
    "    data = data, \n",
    "    train_val_split = train_val_split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to device: mps\n"
     ]
    }
   ],
   "source": [
    "# Create the language model\n",
    "language_model = Transformer(\n",
    "    vocab_size = vocab_size,\n",
    "    n_embed = n_embed,\n",
    "    n_heads = num_attention_heads,\n",
    "    block_size = block_size,\n",
    "    n_transformer_blocks = num_transformer_blocks,\n",
    "    dropout = dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 - Train Loss: 4.2895, Val Loss: 4.2849\n",
      "Epoch 2/500 - Train Loss: 3.5412, Val Loss: 3.5796\n",
      "Epoch 3/500 - Train Loss: 3.3211, Val Loss: 3.3152\n",
      "Epoch 4/500 - Train Loss: 3.1753, Val Loss: 3.2280\n",
      "Epoch 5/500 - Train Loss: 3.1116, Val Loss: 3.1755\n",
      "Epoch 6/500 - Train Loss: 3.1316, Val Loss: 3.1379\n",
      "Epoch 7/500 - Train Loss: 3.0509, Val Loss: 3.0978\n",
      "Epoch 8/500 - Train Loss: 3.0011, Val Loss: 3.0701\n",
      "Epoch 9/500 - Train Loss: 2.9825, Val Loss: 2.9918\n",
      "Epoch 10/500 - Train Loss: 2.9705, Val Loss: 2.9956\n",
      "Epoch 11/500 - Train Loss: 2.8752, Val Loss: 2.9527\n",
      "Epoch 12/500 - Train Loss: 2.8687, Val Loss: 2.9161\n",
      "Epoch 13/500 - Train Loss: 2.8459, Val Loss: 2.8675\n",
      "Epoch 14/500 - Train Loss: 2.8429, Val Loss: 2.8522\n",
      "Epoch 15/500 - Train Loss: 2.8013, Val Loss: 2.8086\n",
      "Epoch 16/500 - Train Loss: 2.8151, Val Loss: 2.8516\n",
      "Epoch 17/500 - Train Loss: 2.7592, Val Loss: 2.7936\n",
      "Epoch 18/500 - Train Loss: 2.7705, Val Loss: 2.7897\n",
      "Epoch 19/500 - Train Loss: 2.7719, Val Loss: 2.7690\n",
      "Epoch 20/500 - Train Loss: 2.7346, Val Loss: 2.7353\n",
      "Epoch 21/500 - Train Loss: 2.7368, Val Loss: 2.7279\n",
      "Epoch 22/500 - Train Loss: 2.7230, Val Loss: 2.7233\n",
      "Epoch 23/500 - Train Loss: 2.7072, Val Loss: 2.7190\n",
      "Epoch 24/500 - Train Loss: 2.6745, Val Loss: 2.7008\n",
      "Epoch 25/500 - Train Loss: 2.6659, Val Loss: 2.6933\n",
      "Epoch 26/500 - Train Loss: 2.6700, Val Loss: 2.6813\n",
      "Epoch 27/500 - Train Loss: 2.6595, Val Loss: 2.6550\n",
      "Epoch 28/500 - Train Loss: 2.6246, Val Loss: 2.6308\n",
      "Epoch 29/500 - Train Loss: 2.6275, Val Loss: 2.6589\n",
      "Epoch 30/500 - Train Loss: 2.6049, Val Loss: 2.6032\n",
      "Epoch 31/500 - Train Loss: 2.6025, Val Loss: 2.6055\n",
      "Epoch 32/500 - Train Loss: 2.6031, Val Loss: 2.6167\n",
      "Epoch 33/500 - Train Loss: 2.5863, Val Loss: 2.6026\n",
      "Epoch 34/500 - Train Loss: 2.5908, Val Loss: 2.5940\n",
      "Epoch 35/500 - Train Loss: 2.6042, Val Loss: 2.5864\n",
      "Epoch 36/500 - Train Loss: 2.5902, Val Loss: 2.5944\n",
      "Epoch 37/500 - Train Loss: 2.5619, Val Loss: 2.5639\n",
      "Epoch 38/500 - Train Loss: 2.5703, Val Loss: 2.5683\n",
      "Epoch 39/500 - Train Loss: 2.5545, Val Loss: 2.5629\n",
      "Epoch 40/500 - Train Loss: 2.5480, Val Loss: 2.5731\n",
      "Epoch 41/500 - Train Loss: 2.5627, Val Loss: 2.5617\n",
      "Epoch 42/500 - Train Loss: 2.5334, Val Loss: 2.5460\n",
      "Epoch 43/500 - Train Loss: 2.5509, Val Loss: 2.5486\n",
      "Epoch 44/500 - Train Loss: 2.5431, Val Loss: 2.5590\n",
      "Epoch 45/500 - Train Loss: 2.5258, Val Loss: 2.5745\n",
      "Epoch 46/500 - Train Loss: 2.5333, Val Loss: 2.5521\n",
      "Epoch 47/500 - Train Loss: 2.5226, Val Loss: 2.5512\n",
      "Epoch 48/500 - Train Loss: 2.5488, Val Loss: 2.5582\n",
      "Epoch 49/500 - Train Loss: 2.5406, Val Loss: 2.5460\n",
      "Epoch 50/500 - Train Loss: 2.5298, Val Loss: 2.5386\n",
      "Epoch 51/500 - Train Loss: 2.5343, Val Loss: 2.5420\n",
      "Epoch 52/500 - Train Loss: 2.5504, Val Loss: 2.5394\n",
      "Epoch 53/500 - Train Loss: 2.5262, Val Loss: 2.5288\n",
      "Epoch 54/500 - Train Loss: 2.5224, Val Loss: 2.5444\n",
      "Epoch 55/500 - Train Loss: 2.5069, Val Loss: 2.5315\n",
      "Epoch 56/500 - Train Loss: 2.5216, Val Loss: 2.5227\n",
      "Epoch 57/500 - Train Loss: 2.5067, Val Loss: 2.5097\n",
      "Epoch 58/500 - Train Loss: 2.5154, Val Loss: 2.5242\n",
      "Epoch 59/500 - Train Loss: 2.4937, Val Loss: 2.5292\n",
      "Epoch 60/500 - Train Loss: 2.5049, Val Loss: 2.4934\n",
      "Epoch 61/500 - Train Loss: 2.5213, Val Loss: 2.5018\n",
      "Epoch 62/500 - Train Loss: 2.4891, Val Loss: 2.5100\n",
      "Epoch 63/500 - Train Loss: 2.5097, Val Loss: 2.5313\n",
      "Epoch 64/500 - Train Loss: 2.5038, Val Loss: 2.4953\n",
      "Epoch 65/500 - Train Loss: 2.5189, Val Loss: 2.4979\n",
      "Epoch 66/500 - Train Loss: 2.4816, Val Loss: 2.5027\n",
      "Epoch 67/500 - Train Loss: 2.4952, Val Loss: 2.5217\n",
      "Epoch 68/500 - Train Loss: 2.4818, Val Loss: 2.5168\n",
      "Epoch 69/500 - Train Loss: 2.4908, Val Loss: 2.5022\n",
      "Epoch 70/500 - Train Loss: 2.4891, Val Loss: 2.4878\n",
      "Epoch 71/500 - Train Loss: 2.4704, Val Loss: 2.5166\n",
      "Epoch 72/500 - Train Loss: 2.5190, Val Loss: 2.5181\n",
      "Epoch 73/500 - Train Loss: 2.5058, Val Loss: 2.4719\n",
      "Epoch 74/500 - Train Loss: 2.5023, Val Loss: 2.4941\n",
      "Epoch 75/500 - Train Loss: 2.4835, Val Loss: 2.5039\n",
      "Epoch 76/500 - Train Loss: 2.5105, Val Loss: 2.5097\n",
      "Epoch 77/500 - Train Loss: 2.4858, Val Loss: 2.4952\n",
      "Epoch 78/500 - Train Loss: 2.4729, Val Loss: 2.5007\n",
      "Epoch 79/500 - Train Loss: 2.4782, Val Loss: 2.5141\n",
      "Epoch 80/500 - Train Loss: 2.5064, Val Loss: 2.4891\n",
      "Epoch 81/500 - Train Loss: 2.4787, Val Loss: 2.4739\n",
      "Epoch 82/500 - Train Loss: 2.4955, Val Loss: 2.5212\n",
      "Epoch 83/500 - Train Loss: 2.4883, Val Loss: 2.5156\n",
      "Epoch 84/500 - Train Loss: 2.4561, Val Loss: 2.4895\n",
      "Epoch 85/500 - Train Loss: 2.4742, Val Loss: 2.4991\n",
      "Epoch 86/500 - Train Loss: 2.4716, Val Loss: 2.4877\n",
      "Epoch 87/500 - Train Loss: 2.4820, Val Loss: 2.5062\n",
      "Epoch 88/500 - Train Loss: 2.4907, Val Loss: 2.4996\n",
      "Epoch 89/500 - Train Loss: 2.4693, Val Loss: 2.4853\n",
      "Epoch 90/500 - Train Loss: 2.4650, Val Loss: 2.4885\n",
      "Epoch 91/500 - Train Loss: 2.4816, Val Loss: 2.5117\n",
      "Epoch 92/500 - Train Loss: 2.4654, Val Loss: 2.4974\n",
      "Epoch 93/500 - Train Loss: 2.4655, Val Loss: 2.4923\n",
      "Epoch 94/500 - Train Loss: 2.4893, Val Loss: 2.4833\n",
      "Epoch 95/500 - Train Loss: 2.4549, Val Loss: 2.4837\n",
      "Epoch 96/500 - Train Loss: 2.4789, Val Loss: 2.4919\n",
      "Epoch 97/500 - Train Loss: 2.4833, Val Loss: 2.4860\n",
      "Epoch 98/500 - Train Loss: 2.4756, Val Loss: 2.4769\n",
      "Epoch 99/500 - Train Loss: 2.4807, Val Loss: 2.5084\n",
      "Epoch 100/500 - Train Loss: 2.4487, Val Loss: 2.4833\n",
      "Epoch 101/500 - Train Loss: 2.4545, Val Loss: 2.4937\n",
      "Epoch 102/500 - Train Loss: 2.4821, Val Loss: 2.4949\n",
      "Epoch 103/500 - Train Loss: 2.4726, Val Loss: 2.4676\n",
      "Epoch 104/500 - Train Loss: 2.4674, Val Loss: 2.4576\n",
      "Epoch 105/500 - Train Loss: 2.4862, Val Loss: 2.4625\n",
      "Epoch 106/500 - Train Loss: 2.4790, Val Loss: 2.4845\n",
      "Epoch 107/500 - Train Loss: 2.4709, Val Loss: 2.4799\n",
      "Epoch 108/500 - Train Loss: 2.4697, Val Loss: 2.4936\n",
      "Epoch 109/500 - Train Loss: 2.4717, Val Loss: 2.4762\n",
      "Epoch 110/500 - Train Loss: 2.4711, Val Loss: 2.4748\n",
      "Epoch 111/500 - Train Loss: 2.4550, Val Loss: 2.4764\n",
      "Epoch 112/500 - Train Loss: 2.4454, Val Loss: 2.4953\n",
      "Epoch 113/500 - Train Loss: 2.4554, Val Loss: 2.4606\n",
      "Epoch 114/500 - Train Loss: 2.4680, Val Loss: 2.4706\n",
      "Epoch 115/500 - Train Loss: 2.4643, Val Loss: 2.4742\n",
      "Epoch 116/500 - Train Loss: 2.4615, Val Loss: 2.4706\n",
      "Epoch 117/500 - Train Loss: 2.4290, Val Loss: 2.5068\n",
      "Epoch 118/500 - Train Loss: 2.4438, Val Loss: 2.4980\n",
      "Epoch 119/500 - Train Loss: 2.4347, Val Loss: 2.4921\n",
      "Epoch 120/500 - Train Loss: 2.4609, Val Loss: 2.4854\n",
      "Epoch 121/500 - Train Loss: 2.4644, Val Loss: 2.4806\n",
      "Epoch 122/500 - Train Loss: 2.4470, Val Loss: 2.4934\n",
      "Epoch 123/500 - Train Loss: 2.4587, Val Loss: 2.4670\n",
      "Epoch 124/500 - Train Loss: 2.4596, Val Loss: 2.4669\n",
      "Epoch 125/500 - Train Loss: 2.4608, Val Loss: 2.4610\n",
      "Epoch 126/500 - Train Loss: 2.4569, Val Loss: 2.4807\n",
      "Epoch 127/500 - Train Loss: 2.4480, Val Loss: 2.4625\n",
      "Epoch 128/500 - Train Loss: 2.4791, Val Loss: 2.4970\n",
      "Epoch 129/500 - Train Loss: 2.4266, Val Loss: 2.4516\n",
      "Epoch 130/500 - Train Loss: 2.4371, Val Loss: 2.4568\n",
      "Epoch 131/500 - Train Loss: 2.4596, Val Loss: 2.4528\n",
      "Epoch 132/500 - Train Loss: 2.4402, Val Loss: 2.4589\n",
      "Epoch 133/500 - Train Loss: 2.4330, Val Loss: 2.4462\n",
      "Epoch 134/500 - Train Loss: 2.4552, Val Loss: 2.4736\n",
      "Epoch 135/500 - Train Loss: 2.4284, Val Loss: 2.4737\n",
      "Epoch 136/500 - Train Loss: 2.4360, Val Loss: 2.4809\n",
      "Epoch 137/500 - Train Loss: 2.4472, Val Loss: 2.4646\n",
      "Epoch 138/500 - Train Loss: 2.4391, Val Loss: 2.4628\n",
      "Epoch 139/500 - Train Loss: 2.4394, Val Loss: 2.4805\n",
      "Epoch 140/500 - Train Loss: 2.4396, Val Loss: 2.4781\n",
      "Epoch 141/500 - Train Loss: 2.4471, Val Loss: 2.4779\n",
      "Epoch 142/500 - Train Loss: 2.4617, Val Loss: 2.4559\n",
      "Epoch 143/500 - Train Loss: 2.4397, Val Loss: 2.4386\n",
      "Epoch 144/500 - Train Loss: 2.4478, Val Loss: 2.4671\n",
      "Epoch 145/500 - Train Loss: 2.4288, Val Loss: 2.4634\n",
      "Epoch 146/500 - Train Loss: 2.4407, Val Loss: 2.4705\n",
      "Epoch 147/500 - Train Loss: 2.4479, Val Loss: 2.4594\n",
      "Epoch 148/500 - Train Loss: 2.4404, Val Loss: 2.4305\n",
      "Epoch 149/500 - Train Loss: 2.4256, Val Loss: 2.4566\n",
      "Epoch 150/500 - Train Loss: 2.4432, Val Loss: 2.4509\n",
      "Epoch 151/500 - Train Loss: 2.4319, Val Loss: 2.4664\n",
      "Epoch 152/500 - Train Loss: 2.4217, Val Loss: 2.4387\n",
      "Epoch 153/500 - Train Loss: 2.4439, Val Loss: 2.4602\n",
      "Epoch 154/500 - Train Loss: 2.4352, Val Loss: 2.4528\n",
      "Epoch 155/500 - Train Loss: 2.4259, Val Loss: 2.4522\n",
      "Epoch 156/500 - Train Loss: 2.4418, Val Loss: 2.4476\n",
      "Epoch 157/500 - Train Loss: 2.4147, Val Loss: 2.4477\n",
      "Epoch 158/500 - Train Loss: 2.4309, Val Loss: 2.4550\n",
      "Epoch 159/500 - Train Loss: 2.4246, Val Loss: 2.4636\n",
      "Epoch 160/500 - Train Loss: 2.4342, Val Loss: 2.4430\n",
      "Epoch 161/500 - Train Loss: 2.4426, Val Loss: 2.4559\n",
      "Epoch 162/500 - Train Loss: 2.4173, Val Loss: 2.4279\n",
      "Epoch 163/500 - Train Loss: 2.4225, Val Loss: 2.4590\n",
      "Epoch 164/500 - Train Loss: 2.4547, Val Loss: 2.4457\n",
      "Epoch 165/500 - Train Loss: 2.4419, Val Loss: 2.4380\n",
      "Epoch 166/500 - Train Loss: 2.4206, Val Loss: 2.4316\n",
      "Epoch 167/500 - Train Loss: 2.4192, Val Loss: 2.4380\n",
      "Epoch 168/500 - Train Loss: 2.4204, Val Loss: 2.4576\n",
      "Epoch 169/500 - Train Loss: 2.4036, Val Loss: 2.4368\n",
      "Epoch 170/500 - Train Loss: 2.4174, Val Loss: 2.4335\n",
      "Epoch 171/500 - Train Loss: 2.3961, Val Loss: 2.4330\n",
      "Epoch 172/500 - Train Loss: 2.4193, Val Loss: 2.4417\n",
      "Epoch 173/500 - Train Loss: 2.4233, Val Loss: 2.4408\n",
      "Epoch 174/500 - Train Loss: 2.4047, Val Loss: 2.4232\n",
      "Epoch 175/500 - Train Loss: 2.4167, Val Loss: 2.4267\n",
      "Epoch 176/500 - Train Loss: 2.4243, Val Loss: 2.4388\n",
      "Epoch 177/500 - Train Loss: 2.4092, Val Loss: 2.4382\n",
      "Epoch 178/500 - Train Loss: 2.4328, Val Loss: 2.4100\n",
      "Epoch 179/500 - Train Loss: 2.3982, Val Loss: 2.4349\n",
      "Epoch 180/500 - Train Loss: 2.4071, Val Loss: 2.4348\n",
      "Epoch 181/500 - Train Loss: 2.4351, Val Loss: 2.4205\n",
      "Epoch 182/500 - Train Loss: 2.3995, Val Loss: 2.4448\n",
      "Epoch 183/500 - Train Loss: 2.4340, Val Loss: 2.4259\n",
      "Epoch 184/500 - Train Loss: 2.4013, Val Loss: 2.4253\n",
      "Epoch 185/500 - Train Loss: 2.4189, Val Loss: 2.4265\n",
      "Epoch 186/500 - Train Loss: 2.4345, Val Loss: 2.4146\n",
      "Epoch 187/500 - Train Loss: 2.3945, Val Loss: 2.4182\n",
      "Epoch 188/500 - Train Loss: 2.3963, Val Loss: 2.4175\n",
      "Epoch 189/500 - Train Loss: 2.4217, Val Loss: 2.4384\n",
      "Epoch 190/500 - Train Loss: 2.4086, Val Loss: 2.4262\n",
      "Epoch 191/500 - Train Loss: 2.3954, Val Loss: 2.4137\n",
      "Epoch 192/500 - Train Loss: 2.3971, Val Loss: 2.4281\n",
      "Epoch 193/500 - Train Loss: 2.4269, Val Loss: 2.4128\n",
      "Epoch 194/500 - Train Loss: 2.3932, Val Loss: 2.4239\n",
      "Epoch 195/500 - Train Loss: 2.3910, Val Loss: 2.4416\n",
      "Epoch 196/500 - Train Loss: 2.3924, Val Loss: 2.4213\n",
      "Epoch 197/500 - Train Loss: 2.3970, Val Loss: 2.4085\n",
      "Epoch 198/500 - Train Loss: 2.3891, Val Loss: 2.4170\n",
      "Epoch 199/500 - Train Loss: 2.3821, Val Loss: 2.4245\n",
      "Epoch 200/500 - Train Loss: 2.3844, Val Loss: 2.4168\n",
      "Epoch 201/500 - Train Loss: 2.4043, Val Loss: 2.4191\n",
      "Epoch 202/500 - Train Loss: 2.3957, Val Loss: 2.4012\n",
      "Epoch 203/500 - Train Loss: 2.3874, Val Loss: 2.3962\n",
      "Epoch 204/500 - Train Loss: 2.3974, Val Loss: 2.3987\n",
      "Epoch 205/500 - Train Loss: 2.4013, Val Loss: 2.4057\n",
      "Epoch 206/500 - Train Loss: 2.3776, Val Loss: 2.4099\n",
      "Epoch 207/500 - Train Loss: 2.3858, Val Loss: 2.4169\n",
      "Epoch 208/500 - Train Loss: 2.3987, Val Loss: 2.4078\n",
      "Epoch 209/500 - Train Loss: 2.3772, Val Loss: 2.3928\n",
      "Epoch 210/500 - Train Loss: 2.3847, Val Loss: 2.4090\n",
      "Epoch 211/500 - Train Loss: 2.3875, Val Loss: 2.4024\n",
      "Epoch 212/500 - Train Loss: 2.3892, Val Loss: 2.4088\n",
      "Epoch 213/500 - Train Loss: 2.3623, Val Loss: 2.3871\n",
      "Epoch 214/500 - Train Loss: 2.3845, Val Loss: 2.4189\n",
      "Epoch 215/500 - Train Loss: 2.3718, Val Loss: 2.4461\n",
      "Epoch 216/500 - Train Loss: 2.3690, Val Loss: 2.4040\n",
      "Epoch 217/500 - Train Loss: 2.3743, Val Loss: 2.3973\n",
      "Epoch 218/500 - Train Loss: 2.3592, Val Loss: 2.4057\n",
      "Epoch 219/500 - Train Loss: 2.3701, Val Loss: 2.3867\n",
      "Epoch 220/500 - Train Loss: 2.3517, Val Loss: 2.3850\n",
      "Epoch 221/500 - Train Loss: 2.3648, Val Loss: 2.3932\n",
      "Epoch 222/500 - Train Loss: 2.3479, Val Loss: 2.4003\n",
      "Epoch 223/500 - Train Loss: 2.3685, Val Loss: 2.4128\n",
      "Epoch 224/500 - Train Loss: 2.3692, Val Loss: 2.3801\n",
      "Epoch 225/500 - Train Loss: 2.3582, Val Loss: 2.3768\n",
      "Epoch 226/500 - Train Loss: 2.3616, Val Loss: 2.4065\n",
      "Epoch 227/500 - Train Loss: 2.3609, Val Loss: 2.3791\n",
      "Epoch 228/500 - Train Loss: 2.3527, Val Loss: 2.3637\n",
      "Epoch 229/500 - Train Loss: 2.3614, Val Loss: 2.3780\n",
      "Epoch 230/500 - Train Loss: 2.3383, Val Loss: 2.3818\n",
      "Epoch 231/500 - Train Loss: 2.3481, Val Loss: 2.3775\n",
      "Epoch 232/500 - Train Loss: 2.3558, Val Loss: 2.3660\n",
      "Epoch 233/500 - Train Loss: 2.3643, Val Loss: 2.3554\n",
      "Epoch 234/500 - Train Loss: 2.3661, Val Loss: 2.3802\n",
      "Epoch 235/500 - Train Loss: 2.3378, Val Loss: 2.3584\n",
      "Epoch 236/500 - Train Loss: 2.3347, Val Loss: 2.3716\n",
      "Epoch 237/500 - Train Loss: 2.3405, Val Loss: 2.3481\n",
      "Epoch 238/500 - Train Loss: 2.3305, Val Loss: 2.3686\n",
      "Epoch 239/500 - Train Loss: 2.3521, Val Loss: 2.3301\n",
      "Epoch 240/500 - Train Loss: 2.3507, Val Loss: 2.3569\n",
      "Epoch 241/500 - Train Loss: 2.3431, Val Loss: 2.3676\n",
      "Epoch 242/500 - Train Loss: 2.3479, Val Loss: 2.3413\n",
      "Epoch 243/500 - Train Loss: 2.3349, Val Loss: 2.3529\n",
      "Epoch 244/500 - Train Loss: 2.3394, Val Loss: 2.3348\n",
      "Epoch 245/500 - Train Loss: 2.3396, Val Loss: 2.3517\n",
      "Epoch 246/500 - Train Loss: 2.3403, Val Loss: 2.3637\n",
      "Epoch 247/500 - Train Loss: 2.3211, Val Loss: 2.3366\n",
      "Epoch 248/500 - Train Loss: 2.3101, Val Loss: 2.3445\n",
      "Epoch 249/500 - Train Loss: 2.3289, Val Loss: 2.3293\n",
      "Epoch 250/500 - Train Loss: 2.3187, Val Loss: 2.3573\n",
      "Epoch 251/500 - Train Loss: 2.3153, Val Loss: 2.3370\n",
      "Epoch 252/500 - Train Loss: 2.3266, Val Loss: 2.3465\n",
      "Epoch 253/500 - Train Loss: 2.3192, Val Loss: 2.3699\n",
      "Epoch 254/500 - Train Loss: 2.3268, Val Loss: 2.3261\n",
      "Epoch 255/500 - Train Loss: 2.3181, Val Loss: 2.3528\n",
      "Epoch 256/500 - Train Loss: 2.3095, Val Loss: 2.3380\n",
      "Epoch 257/500 - Train Loss: 2.2969, Val Loss: 2.3158\n",
      "Epoch 258/500 - Train Loss: 2.2997, Val Loss: 2.3334\n",
      "Epoch 259/500 - Train Loss: 2.2845, Val Loss: 2.3113\n",
      "Epoch 260/500 - Train Loss: 2.3139, Val Loss: 2.3413\n",
      "Epoch 261/500 - Train Loss: 2.3121, Val Loss: 2.3034\n",
      "Epoch 262/500 - Train Loss: 2.3093, Val Loss: 2.3461\n",
      "Epoch 263/500 - Train Loss: 2.2864, Val Loss: 2.3251\n",
      "Epoch 264/500 - Train Loss: 2.2881, Val Loss: 2.3144\n",
      "Epoch 265/500 - Train Loss: 2.2894, Val Loss: 2.3697\n",
      "Epoch 266/500 - Train Loss: 2.2808, Val Loss: 2.3182\n",
      "Epoch 267/500 - Train Loss: 2.3076, Val Loss: 2.3179\n",
      "Epoch 268/500 - Train Loss: 2.2639, Val Loss: 2.3092\n",
      "Epoch 269/500 - Train Loss: 2.3163, Val Loss: 2.2784\n",
      "Epoch 270/500 - Train Loss: 2.2731, Val Loss: 2.3285\n",
      "Epoch 271/500 - Train Loss: 2.2750, Val Loss: 2.2981\n",
      "Epoch 272/500 - Train Loss: 2.2928, Val Loss: 2.3267\n",
      "Epoch 273/500 - Train Loss: 2.2809, Val Loss: 2.2915\n",
      "Epoch 274/500 - Train Loss: 2.2886, Val Loss: 2.3210\n",
      "Epoch 275/500 - Train Loss: 2.2804, Val Loss: 2.3125\n",
      "Epoch 276/500 - Train Loss: 2.2439, Val Loss: 2.3046\n",
      "Epoch 277/500 - Train Loss: 2.2554, Val Loss: 2.2994\n",
      "Epoch 278/500 - Train Loss: 2.2673, Val Loss: 2.2847\n",
      "Epoch 279/500 - Train Loss: 2.2407, Val Loss: 2.2743\n",
      "Epoch 280/500 - Train Loss: 2.2745, Val Loss: 2.3129\n",
      "Epoch 281/500 - Train Loss: 2.2437, Val Loss: 2.3001\n",
      "Epoch 282/500 - Train Loss: 2.2634, Val Loss: 2.3015\n",
      "Epoch 283/500 - Train Loss: 2.2614, Val Loss: 2.3049\n",
      "Epoch 284/500 - Train Loss: 2.2575, Val Loss: 2.2722\n",
      "Epoch 285/500 - Train Loss: 2.2536, Val Loss: 2.3117\n",
      "Epoch 286/500 - Train Loss: 2.2509, Val Loss: 2.3087\n",
      "Epoch 287/500 - Train Loss: 2.2458, Val Loss: 2.2778\n",
      "Epoch 288/500 - Train Loss: 2.2613, Val Loss: 2.2888\n",
      "Epoch 289/500 - Train Loss: 2.2641, Val Loss: 2.2851\n",
      "Epoch 290/500 - Train Loss: 2.2854, Val Loss: 2.3022\n",
      "Epoch 291/500 - Train Loss: 2.2293, Val Loss: 2.2791\n",
      "Epoch 292/500 - Train Loss: 2.2428, Val Loss: 2.2526\n",
      "Epoch 293/500 - Train Loss: 2.2523, Val Loss: 2.2959\n",
      "Epoch 294/500 - Train Loss: 2.2447, Val Loss: 2.2681\n",
      "Epoch 295/500 - Train Loss: 2.2416, Val Loss: 2.2607\n",
      "Epoch 296/500 - Train Loss: 2.2427, Val Loss: 2.2563\n",
      "Epoch 297/500 - Train Loss: 2.2476, Val Loss: 2.2641\n",
      "Epoch 298/500 - Train Loss: 2.2268, Val Loss: 2.2532\n",
      "Epoch 299/500 - Train Loss: 2.2078, Val Loss: 2.2650\n",
      "Epoch 300/500 - Train Loss: 2.2224, Val Loss: 2.2735\n",
      "Epoch 301/500 - Train Loss: 2.2136, Val Loss: 2.2659\n",
      "Epoch 302/500 - Train Loss: 2.2112, Val Loss: 2.2561\n",
      "Epoch 303/500 - Train Loss: 2.2058, Val Loss: 2.2439\n",
      "Epoch 304/500 - Train Loss: 2.1948, Val Loss: 2.2473\n",
      "Epoch 305/500 - Train Loss: 2.1977, Val Loss: 2.2314\n",
      "Epoch 306/500 - Train Loss: 2.2038, Val Loss: 2.2527\n",
      "Epoch 307/500 - Train Loss: 2.2158, Val Loss: 2.2449\n",
      "Epoch 308/500 - Train Loss: 2.1924, Val Loss: 2.2787\n",
      "Epoch 309/500 - Train Loss: 2.1891, Val Loss: 2.2206\n",
      "Epoch 310/500 - Train Loss: 2.1979, Val Loss: 2.2511\n",
      "Epoch 311/500 - Train Loss: 2.1948, Val Loss: 2.2424\n",
      "Epoch 312/500 - Train Loss: 2.1920, Val Loss: 2.2214\n",
      "Epoch 313/500 - Train Loss: 2.1803, Val Loss: 2.2530\n",
      "Epoch 314/500 - Train Loss: 2.1873, Val Loss: 2.1954\n",
      "Epoch 315/500 - Train Loss: 2.1930, Val Loss: 2.2224\n",
      "Epoch 316/500 - Train Loss: 2.1701, Val Loss: 2.2328\n",
      "Epoch 317/500 - Train Loss: 2.1620, Val Loss: 2.2141\n",
      "Epoch 318/500 - Train Loss: 2.1692, Val Loss: 2.2111\n",
      "Epoch 319/500 - Train Loss: 2.1916, Val Loss: 2.2000\n",
      "Epoch 320/500 - Train Loss: 2.1633, Val Loss: 2.1949\n",
      "Epoch 321/500 - Train Loss: 2.1504, Val Loss: 2.2124\n",
      "Epoch 322/500 - Train Loss: 2.1797, Val Loss: 2.2227\n",
      "Epoch 323/500 - Train Loss: 2.1531, Val Loss: 2.2079\n",
      "Epoch 324/500 - Train Loss: 2.1695, Val Loss: 2.2467\n",
      "Epoch 325/500 - Train Loss: 2.1381, Val Loss: 2.2227\n",
      "Epoch 326/500 - Train Loss: 2.1843, Val Loss: 2.2338\n",
      "Epoch 327/500 - Train Loss: 2.1709, Val Loss: 2.2286\n",
      "Epoch 328/500 - Train Loss: 2.1695, Val Loss: 2.2152\n",
      "Epoch 329/500 - Train Loss: 2.1350, Val Loss: 2.1939\n",
      "Epoch 330/500 - Train Loss: 2.1660, Val Loss: 2.2030\n",
      "Epoch 331/500 - Train Loss: 2.1461, Val Loss: 2.1973\n",
      "Epoch 332/500 - Train Loss: 2.1395, Val Loss: 2.1944\n",
      "Epoch 333/500 - Train Loss: 2.1835, Val Loss: 2.1992\n",
      "Epoch 334/500 - Train Loss: 2.1421, Val Loss: 2.1904\n",
      "Epoch 335/500 - Train Loss: 2.1656, Val Loss: 2.2081\n",
      "Epoch 336/500 - Train Loss: 2.1430, Val Loss: 2.1994\n",
      "Epoch 337/500 - Train Loss: 2.1322, Val Loss: 2.1786\n",
      "Epoch 338/500 - Train Loss: 2.1473, Val Loss: 2.1710\n",
      "Epoch 339/500 - Train Loss: 2.1541, Val Loss: 2.1823\n",
      "Epoch 340/500 - Train Loss: 2.1244, Val Loss: 2.1812\n",
      "Epoch 341/500 - Train Loss: 2.1183, Val Loss: 2.1850\n",
      "Epoch 342/500 - Train Loss: 2.1205, Val Loss: 2.2014\n",
      "Epoch 343/500 - Train Loss: 2.0982, Val Loss: 2.1876\n",
      "Epoch 344/500 - Train Loss: 2.1013, Val Loss: 2.1590\n",
      "Epoch 345/500 - Train Loss: 2.1182, Val Loss: 2.1726\n",
      "Epoch 346/500 - Train Loss: 2.1234, Val Loss: 2.1840\n",
      "Epoch 347/500 - Train Loss: 2.1061, Val Loss: 2.1338\n",
      "Epoch 348/500 - Train Loss: 2.1043, Val Loss: 2.1571\n",
      "Epoch 349/500 - Train Loss: 2.1432, Val Loss: 2.1542\n",
      "Epoch 350/500 - Train Loss: 2.0868, Val Loss: 2.1647\n",
      "Epoch 351/500 - Train Loss: 2.0748, Val Loss: 2.1446\n",
      "Epoch 352/500 - Train Loss: 2.0891, Val Loss: 2.1481\n",
      "Epoch 353/500 - Train Loss: 2.0878, Val Loss: 2.1591\n",
      "Epoch 354/500 - Train Loss: 2.0874, Val Loss: 2.1600\n",
      "Epoch 355/500 - Train Loss: 2.1035, Val Loss: 2.1383\n",
      "Epoch 356/500 - Train Loss: 2.0996, Val Loss: 2.1390\n",
      "Epoch 357/500 - Train Loss: 2.0877, Val Loss: 2.1451\n",
      "Epoch 358/500 - Train Loss: 2.0914, Val Loss: 2.1539\n",
      "Epoch 359/500 - Train Loss: 2.0758, Val Loss: 2.1311\n",
      "Epoch 360/500 - Train Loss: 2.0717, Val Loss: 2.1393\n",
      "Epoch 361/500 - Train Loss: 2.0844, Val Loss: 2.1804\n",
      "Epoch 362/500 - Train Loss: 2.0931, Val Loss: 2.1448\n",
      "Epoch 363/500 - Train Loss: 2.0682, Val Loss: 2.1725\n",
      "Epoch 364/500 - Train Loss: 2.1008, Val Loss: 2.1805\n",
      "Epoch 365/500 - Train Loss: 2.0739, Val Loss: 2.1616\n",
      "Epoch 366/500 - Train Loss: 2.0617, Val Loss: 2.1252\n",
      "Epoch 367/500 - Train Loss: 2.0643, Val Loss: 2.1386\n",
      "Epoch 368/500 - Train Loss: 2.0755, Val Loss: 2.1234\n",
      "Epoch 369/500 - Train Loss: 2.0586, Val Loss: 2.1448\n",
      "Epoch 370/500 - Train Loss: 2.0603, Val Loss: 2.1436\n",
      "Epoch 371/500 - Train Loss: 2.0706, Val Loss: 2.1311\n",
      "Epoch 372/500 - Train Loss: 2.0576, Val Loss: 2.1522\n",
      "Epoch 373/500 - Train Loss: 2.0489, Val Loss: 2.1207\n",
      "Epoch 374/500 - Train Loss: 2.0484, Val Loss: 2.1211\n",
      "Epoch 375/500 - Train Loss: 2.0824, Val Loss: 2.1251\n",
      "Epoch 376/500 - Train Loss: 2.0510, Val Loss: 2.1263\n",
      "Epoch 377/500 - Train Loss: 2.0451, Val Loss: 2.1035\n",
      "Epoch 378/500 - Train Loss: 2.0578, Val Loss: 2.1409\n",
      "Epoch 379/500 - Train Loss: 2.0717, Val Loss: 2.1199\n",
      "Epoch 380/500 - Train Loss: 2.0258, Val Loss: 2.1247\n",
      "Epoch 381/500 - Train Loss: 2.0426, Val Loss: 2.1016\n",
      "Epoch 382/500 - Train Loss: 2.0413, Val Loss: 2.1009\n",
      "Epoch 383/500 - Train Loss: 2.0191, Val Loss: 2.1111\n",
      "Epoch 384/500 - Train Loss: 2.0263, Val Loss: 2.1120\n",
      "Epoch 385/500 - Train Loss: 2.0267, Val Loss: 2.1071\n",
      "Epoch 386/500 - Train Loss: 2.0358, Val Loss: 2.1449\n",
      "Epoch 387/500 - Train Loss: 2.0328, Val Loss: 2.1240\n",
      "Epoch 388/500 - Train Loss: 2.0070, Val Loss: 2.0786\n",
      "Epoch 389/500 - Train Loss: 2.0455, Val Loss: 2.0883\n",
      "Epoch 390/500 - Train Loss: 2.0507, Val Loss: 2.1347\n",
      "Epoch 391/500 - Train Loss: 2.0230, Val Loss: 2.0979\n",
      "Epoch 392/500 - Train Loss: 2.0382, Val Loss: 2.1068\n",
      "Epoch 393/500 - Train Loss: 2.0316, Val Loss: 2.0981\n",
      "Epoch 394/500 - Train Loss: 2.0519, Val Loss: 2.0740\n",
      "Epoch 395/500 - Train Loss: 2.0290, Val Loss: 2.0654\n",
      "Epoch 396/500 - Train Loss: 2.0032, Val Loss: 2.1155\n",
      "Epoch 397/500 - Train Loss: 2.0052, Val Loss: 2.0945\n",
      "Epoch 398/500 - Train Loss: 2.0263, Val Loss: 2.0854\n",
      "Epoch 399/500 - Train Loss: 2.0126, Val Loss: 2.0687\n",
      "Epoch 400/500 - Train Loss: 2.0026, Val Loss: 2.0872\n",
      "Epoch 401/500 - Train Loss: 2.0290, Val Loss: 2.0775\n",
      "Epoch 402/500 - Train Loss: 2.0153, Val Loss: 2.0898\n",
      "Epoch 403/500 - Train Loss: 2.0363, Val Loss: 2.0944\n",
      "Epoch 404/500 - Train Loss: 1.9937, Val Loss: 2.1018\n",
      "Epoch 405/500 - Train Loss: 2.0092, Val Loss: 2.0735\n",
      "Epoch 406/500 - Train Loss: 2.0223, Val Loss: 2.0831\n",
      "Epoch 407/500 - Train Loss: 2.0171, Val Loss: 2.0907\n",
      "Epoch 408/500 - Train Loss: 2.0070, Val Loss: 2.0686\n",
      "Epoch 409/500 - Train Loss: 1.9973, Val Loss: 2.1271\n",
      "Epoch 410/500 - Train Loss: 1.9839, Val Loss: 2.0803\n",
      "Epoch 411/500 - Train Loss: 2.0150, Val Loss: 2.1212\n",
      "Epoch 412/500 - Train Loss: 2.0197, Val Loss: 2.1021\n",
      "Epoch 413/500 - Train Loss: 2.0072, Val Loss: 2.0711\n",
      "Epoch 414/500 - Train Loss: 1.9777, Val Loss: 2.0570\n",
      "Epoch 415/500 - Train Loss: 1.9875, Val Loss: 2.0801\n",
      "Epoch 416/500 - Train Loss: 1.9841, Val Loss: 2.0809\n",
      "Epoch 417/500 - Train Loss: 1.9713, Val Loss: 2.0815\n",
      "Epoch 418/500 - Train Loss: 1.9921, Val Loss: 2.0911\n",
      "Epoch 419/500 - Train Loss: 1.9903, Val Loss: 2.0527\n",
      "Epoch 420/500 - Train Loss: 2.0067, Val Loss: 2.0713\n",
      "Epoch 421/500 - Train Loss: 1.9836, Val Loss: 2.0782\n",
      "Epoch 422/500 - Train Loss: 1.9862, Val Loss: 2.0759\n",
      "Epoch 423/500 - Train Loss: 1.9455, Val Loss: 2.0564\n",
      "Epoch 424/500 - Train Loss: 1.9550, Val Loss: 2.0621\n",
      "Epoch 425/500 - Train Loss: 1.9706, Val Loss: 2.0487\n",
      "Epoch 426/500 - Train Loss: 1.9679, Val Loss: 2.0415\n",
      "Epoch 427/500 - Train Loss: 1.9706, Val Loss: 2.0803\n",
      "Epoch 428/500 - Train Loss: 1.9829, Val Loss: 2.0758\n",
      "Epoch 429/500 - Train Loss: 1.9861, Val Loss: 2.0411\n",
      "Epoch 430/500 - Train Loss: 1.9827, Val Loss: 2.0792\n",
      "Epoch 431/500 - Train Loss: 1.9609, Val Loss: 2.0296\n",
      "Epoch 432/500 - Train Loss: 1.9686, Val Loss: 2.0828\n",
      "Epoch 433/500 - Train Loss: 1.9637, Val Loss: 2.0658\n",
      "Epoch 434/500 - Train Loss: 1.9898, Val Loss: 2.0527\n",
      "Epoch 435/500 - Train Loss: 1.9613, Val Loss: 2.0708\n",
      "Epoch 436/500 - Train Loss: 1.9777, Val Loss: 2.0475\n",
      "Epoch 437/500 - Train Loss: 1.9580, Val Loss: 2.0653\n",
      "Epoch 438/500 - Train Loss: 1.9662, Val Loss: 2.0273\n",
      "Epoch 439/500 - Train Loss: 1.9508, Val Loss: 2.0506\n",
      "Epoch 440/500 - Train Loss: 1.9568, Val Loss: 2.0309\n",
      "Epoch 441/500 - Train Loss: 1.9774, Val Loss: 2.0466\n",
      "Epoch 442/500 - Train Loss: 1.9788, Val Loss: 2.0313\n",
      "Epoch 443/500 - Train Loss: 1.9464, Val Loss: 2.0499\n",
      "Epoch 444/500 - Train Loss: 1.9394, Val Loss: 2.0397\n",
      "Epoch 445/500 - Train Loss: 1.9441, Val Loss: 2.0705\n",
      "Epoch 446/500 - Train Loss: 1.9421, Val Loss: 2.0193\n",
      "Epoch 447/500 - Train Loss: 1.9348, Val Loss: 2.0382\n",
      "Epoch 448/500 - Train Loss: 1.9305, Val Loss: 2.0236\n",
      "Epoch 449/500 - Train Loss: 1.9449, Val Loss: 2.0478\n",
      "Epoch 450/500 - Train Loss: 1.9600, Val Loss: 2.0248\n",
      "Epoch 451/500 - Train Loss: 1.9447, Val Loss: 2.0408\n",
      "Epoch 452/500 - Train Loss: 1.9374, Val Loss: 2.0402\n",
      "Epoch 453/500 - Train Loss: 1.9723, Val Loss: 2.0222\n",
      "Epoch 454/500 - Train Loss: 1.9245, Val Loss: 2.0487\n",
      "Epoch 455/500 - Train Loss: 1.9196, Val Loss: 2.0378\n",
      "Epoch 456/500 - Train Loss: 1.9241, Val Loss: 2.0348\n",
      "Epoch 457/500 - Train Loss: 1.9436, Val Loss: 2.0166\n",
      "Epoch 458/500 - Train Loss: 1.9047, Val Loss: 2.0096\n",
      "Epoch 459/500 - Train Loss: 1.9194, Val Loss: 2.0047\n",
      "Epoch 460/500 - Train Loss: 1.9480, Val Loss: 2.0518\n",
      "Epoch 461/500 - Train Loss: 1.9465, Val Loss: 2.0067\n",
      "Epoch 462/500 - Train Loss: 1.9283, Val Loss: 2.0286\n",
      "Epoch 463/500 - Train Loss: 1.9286, Val Loss: 2.0198\n",
      "Epoch 464/500 - Train Loss: 1.9130, Val Loss: 2.0384\n",
      "Epoch 465/500 - Train Loss: 1.9409, Val Loss: 2.0460\n",
      "Epoch 466/500 - Train Loss: 1.9201, Val Loss: 2.0250\n",
      "Epoch 467/500 - Train Loss: 1.9250, Val Loss: 2.0234\n",
      "Epoch 468/500 - Train Loss: 1.9079, Val Loss: 2.0379\n",
      "Epoch 469/500 - Train Loss: 1.9244, Val Loss: 2.0174\n",
      "Epoch 470/500 - Train Loss: 1.9125, Val Loss: 1.9938\n",
      "Epoch 471/500 - Train Loss: 1.8937, Val Loss: 2.0170\n",
      "Epoch 472/500 - Train Loss: 1.9267, Val Loss: 1.9979\n",
      "Epoch 473/500 - Train Loss: 1.9091, Val Loss: 2.0304\n",
      "Epoch 474/500 - Train Loss: 1.9046, Val Loss: 2.0161\n",
      "Epoch 475/500 - Train Loss: 1.9217, Val Loss: 2.0202\n",
      "Epoch 476/500 - Train Loss: 1.8807, Val Loss: 2.0212\n",
      "Epoch 477/500 - Train Loss: 1.8914, Val Loss: 2.0020\n",
      "Epoch 478/500 - Train Loss: 1.9166, Val Loss: 2.0098\n",
      "Epoch 479/500 - Train Loss: 1.8955, Val Loss: 1.9980\n",
      "Epoch 480/500 - Train Loss: 1.8842, Val Loss: 2.0054\n",
      "Epoch 481/500 - Train Loss: 1.9271, Val Loss: 2.0197\n",
      "Epoch 482/500 - Train Loss: 1.9079, Val Loss: 1.9970\n",
      "Epoch 483/500 - Train Loss: 1.9140, Val Loss: 1.9817\n",
      "Epoch 484/500 - Train Loss: 1.8940, Val Loss: 2.0074\n",
      "Epoch 485/500 - Train Loss: 1.8875, Val Loss: 1.9973\n",
      "Epoch 486/500 - Train Loss: 1.9065, Val Loss: 1.9920\n",
      "Epoch 487/500 - Train Loss: 1.9127, Val Loss: 2.0105\n",
      "Epoch 488/500 - Train Loss: 1.8956, Val Loss: 2.0048\n",
      "Epoch 489/500 - Train Loss: 1.8951, Val Loss: 2.0071\n",
      "Epoch 490/500 - Train Loss: 1.8622, Val Loss: 2.0014\n",
      "Epoch 491/500 - Train Loss: 1.9065, Val Loss: 1.9872\n",
      "Epoch 492/500 - Train Loss: 1.8752, Val Loss: 2.0020\n",
      "Epoch 493/500 - Train Loss: 1.8815, Val Loss: 1.9966\n",
      "Epoch 494/500 - Train Loss: 1.8850, Val Loss: 2.0011\n",
      "Epoch 495/500 - Train Loss: 1.9016, Val Loss: 1.9790\n",
      "Epoch 496/500 - Train Loss: 1.8819, Val Loss: 1.9850\n",
      "Epoch 497/500 - Train Loss: 1.8786, Val Loss: 1.9785\n",
      "Epoch 498/500 - Train Loss: 1.8680, Val Loss: 2.0079\n",
      "Epoch 499/500 - Train Loss: 1.8968, Val Loss: 1.9808\n",
      "Epoch 500/500 - Train Loss: 1.8853, Val Loss: 2.0081\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "language_model.train_model(\n",
    "    data_loader = data_handler,\n",
    "    epochs = epochs, \n",
    "    lr = learning_rate, \n",
    "    batch_size = batch_size,\n",
    "    eval_iters = eval_iters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evircoliants, in it lighilan suft\n",
      "ond uspolaly plet liverse ancur heit's souch,\n",
      "for my dow; a whondi\n"
     ]
    }
   ],
   "source": [
    "# Generate some text from the trained model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(language_model.generate(context, max_new_tokens=100).squeeze().tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
