{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "from src.utils import *\n",
    "from src.gpt2 import GPT2\n",
    "from src.config import GPTConfig\n",
    "from src.tokenizer import Tokenizer\n",
    "from src.data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dataset_path = os.path.join(os.getcwd(), 'dataset', 'input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 4 # Batch size for training\n",
    "epochs = 50 # Number of training epochs\n",
    "sequence_length = 32 # Number of tokens in each training sequence\n",
    "train_val_split = 0.1 # Percentage of training data to use for validation\n",
    "learning_rate = 3e-4 # Learning rate for the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(1337);\n",
    "\n",
    "# Reduce the precision for the matmul operator to improve performance\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tokenizer\n",
    "tokenizer = Tokenizer('gpt2')\n",
    "\n",
    "# Extract the vocabulary size\n",
    "vocab_size = tokenizer.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the data loader\n",
    "data_loader = DataLoader(\n",
    "    txt_file = dataset_path,\n",
    "    tokenizer = tokenizer,\n",
    "    train_val_split = train_val_split\n",
    ")\n",
    "\n",
    "# Print the dataset statistics\n",
    "print(\"Training set size: \", len(data_loader.train_tokens))\n",
    "print(\"Validation set size: \", len(data_loader.val_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model configuration\n",
    "# The vocabulary size is 50304, instead of the classic 50257 if the gpt2 tokenizer,\n",
    "# because we add some padding tokens to the vocabulary in order to make the vocabulary \n",
    "# size a multiple of 8 in order to improve performance when using FP16 training.\n",
    "model_config = GPTConfig(\n",
    "    context_size = 1024,\n",
    "    vocab_size = 50304,\n",
    "    n_blocks = 12,\n",
    "    n_heads = 12,\n",
    "    n_embed = 768\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the GPT-2 model\n",
    "gpt2 = GPT2(model_config)\n",
    "\n",
    "# Move the model to the GPU if available \n",
    "# and set the precision to bfloat16 for improved performance\n",
    "gpt2 = gpt2.to(torch.bfloat16).to(device)\n",
    "\n",
    "# Compile the model to optimize performance\n",
    "gpt2 = torch.compile(gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "gpt2.fit(\n",
    "    data_loader = data_loader,\n",
    "    epochs = epochs,\n",
    "    lr = learning_rate,\n",
    "    batch_size = batch_size,\n",
    "    sequence_length = sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the context using the tokenizer and convert it to a tensor\n",
    "context = \"Hello, my name is\"\n",
    "context = torch.tensor(tokenizer.encode(context), dtype=torch.long).unsqueeze(0)\n",
    "context = context.to(device) # Move the tensor to the GPU if available\n",
    "\n",
    "# Decode and display the generated text\n",
    "print(tokenizer.decode(gpt2.generate(context, max_new_tokens=20).squeeze().tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
