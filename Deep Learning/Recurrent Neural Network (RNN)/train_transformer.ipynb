{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36912af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "from typing import Literal\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Add the path to the custom library to the system path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import the module from the custom library\n",
    "from src.architectures.transformer import DecoderTransformer\n",
    "from src.architectures.auto_regressive import AutoRegressive\n",
    "from src.core.utils import data_analysis, data_processing, context_manager\n",
    "from src import Tensor, layers, loss_functions, optimizers, metrics, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370a12a",
   "metadata": {},
   "source": [
    "### Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to where the model will be saved\n",
    "model_path = os.path.join(os.getcwd(), 'checkpoints', 'stock_prediction_transformer')\n",
    "dataset_path = os.path.join(os.getcwd(), 'dataset', 'SPX.csv')\n",
    "\n",
    "# Define the feature and target columns\n",
    "feature_columns = ['Open', 'High', 'Low', 'Close']\n",
    "close_price_idx = feature_columns.index('Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5aea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for the model\n",
    "normalization: Literal['minmax', 'standard'] = 'standard'\n",
    "train_test_split_pct = 0.1\n",
    "train_valid_split_pct = 0.1\n",
    "n_attention_heads = 4\n",
    "n_decoder_blocks = 4\n",
    "learning_rate = 5e-4\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "n_embed = 64\n",
    "seq_len = 128\n",
    "seed = 1234\n",
    "\n",
    "# Volatility prediction settings\n",
    "predict_volatility = True\n",
    "volatility_window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34abdf8a",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Load data from a CSV file with proper date parsing.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: str, path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    - data: list[dict], list of dictionaries containing the data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a list to hold the data\n",
    "    data = []\n",
    "    \n",
    "    # Open the CSV file and read its contents\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Use DictReader to read the CSV file into a list of dictionaries\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        # Iterate over each row in the CSV file\n",
    "        for row in reader:\n",
    "            # Ensure the 'Date' field is in the correct format\n",
    "            try:\n",
    "                # Parse the date and convert it to a datetime object\n",
    "                datetime.strptime(row['Date'], '%Y-%m-%d')\n",
    "                \n",
    "                # Add the row to the data list\n",
    "                data.append(row)\n",
    "                \n",
    "            except ValueError:\n",
    "                # If the date is invalid, skip this row\n",
    "                print(f\"Skipping row with invalid date: {row['Date']}\")\n",
    "                continue\n",
    "    \n",
    "    # Sort the data by date\n",
    "    data.sort(key=lambda x: x['Date'])\n",
    "    \n",
    "    # return the loaded and sorted data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9475ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = load_data(dataset_path)\n",
    "\n",
    "# Print the number of data points and the date range\n",
    "print(f\"Loaded {len(data)} data points\")\n",
    "print(f\"Date range: {data[0]['Date']} to {data[-1]['Date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature and timestamp data\n",
    "features = np.array([[np.float32(data[j][k]) for k in feature_columns] for j in range(len(data))])\n",
    "timestamps = np.array([datetime.strptime(data[j]['Date'], '%Y-%m-%d').timestamp() for j in range(len(data))], dtype=np.int32)\n",
    "\n",
    "# Print the shapes of the features and dates arrays\n",
    "print(f\"Price data shape: {features.shape}\")\n",
    "print(f\"Timestamps shape: {timestamps.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8eaf0",
   "metadata": {},
   "source": [
    "### Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd0e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_returns(prices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate log returns: ln(P_t / P_{t-1})\n",
    "    \n",
    "    Parameters:\n",
    "    - prices: np.ndarray, absolute prices with shape (T, F)\n",
    "    \n",
    "    Returns:\n",
    "    - log_returns: np.ndarray, log returns with shape (T-1, F)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute log returns using the formula ln(P_t / P_{t-1})\n",
    "    return np.log(prices[1:] / prices[:-1])\n",
    "\n",
    "\n",
    "def compute_realized_volatility(log_returns: np.ndarray, window: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate realized volatility using rolling window of squared returns.\n",
    "    \n",
    "    Parameters:\n",
    "    - log_returns: np.ndarray, log returns with shape (T, F)\n",
    "    - window: int, rolling window size\n",
    "    \n",
    "    Returns:\n",
    "    - volatility: np.ndarray, realized volatility\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute squared log returns\n",
    "    squared_returns = log_returns ** 2\n",
    "    \n",
    "    # Initialize an empty list to hold the volatility values\n",
    "    volatility = []\n",
    "    \n",
    "    # Iterate over the log returns with a rolling window\n",
    "    for i in range(window, len(squared_returns)):\n",
    "        # Calculate the volatility as the square root of the mean of squared returns\n",
    "        vol = np.sqrt(np.mean(squared_returns[i-window:i], axis=0))\n",
    "        \n",
    "        # Append the calculated volatility to the list\n",
    "        volatility.append(vol)\n",
    "    \n",
    "    # Convert the list to a numpy array and return\n",
    "    return np.array(volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log returns\n",
    "log_returns = compute_log_returns(features)\n",
    "timestamps_returns = timestamps[1:]\n",
    "\n",
    "# Print the shape and statistics of the log returns\n",
    "print(f\"Log returns shape: {log_returns.shape}\")\n",
    "print(f\"Log returns stats - Mean: {np.mean(log_returns, axis=0)}\")\n",
    "print(f\"Log returns stats - Std: {np.std(log_returns, axis=0)}\")\n",
    "\n",
    "# Compute realized volatility if required\n",
    "if predict_volatility:\n",
    "    # Compute realized volatility using a rolling window\n",
    "    realized_volatility = compute_realized_volatility(log_returns, volatility_window)\n",
    "    \n",
    "    # Adjust timestamp for the volatility data\n",
    "    timestamps_volatility = timestamps_returns[volatility_window:]\n",
    "    \n",
    "    # Print the shape and statistics of the realized volatility\n",
    "    print(f\"Realized volatility shape: {realized_volatility.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609a013",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe965bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target data based on whether we are predicting volatility or log returns\n",
    "if predict_volatility:\n",
    "    # Use realized volatility as target data\n",
    "    target_data = realized_volatility\n",
    "    input_data = log_returns[volatility_window:] \n",
    "    input_timestamps = timestamps_volatility\n",
    "    \n",
    "else:\n",
    "    # Use log returns as target data\n",
    "    target_data = log_returns\n",
    "    input_data = log_returns\n",
    "    input_timestamps = timestamps_returns\n",
    "\n",
    "# Perform the temporal split\n",
    "print(f\"Target data shape: {target_data.shape}\")\n",
    "print(f\"Input data shape: {input_data.shape}\")\n",
    "print(f\"Input timestamps shape: {input_timestamps.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270408fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(input_data: np.ndarray, target_data: np.ndarray, timestamps: np.ndarray, seq_length: int) -> tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Build sequences\n",
    "    \n",
    "    Parameters:\n",
    "    - input_data: np.ndarray, input features\n",
    "    - target_data: np.ndarray, target values (aligned with input_data)\n",
    "    - timestamps: np.ndarray, timestamps corresponding to input_data\n",
    "    - seq_length: int, length of input sequences\n",
    "    \n",
    "    Returns:\n",
    "    - tuple[Tensor, Tensor, Tensor]: input sequences, target values, and timestamps as Tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize lists to hold sequences and targets\n",
    "    X, y, ts = [], [], []\n",
    "    \n",
    "    # Iterate over the input data to create sequences\n",
    "    for i in range(seq_length, len(input_data)):\n",
    "        # Append the sequence of input and the corresponding target\n",
    "        X.append(input_data[i-seq_length:i])\n",
    "        y.append(target_data[i])\n",
    "        ts.append(timestamps[i])\n",
    "    \n",
    "    # Convert the lists to numpy arrays and return as Tensors\n",
    "    return (\n",
    "        Tensor(np.array(X, dtype=np.float32)),\n",
    "        Tensor(np.array(y, dtype=np.float32)),\n",
    "        Tensor(np.array(ts, dtype=np.int32))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0677c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequences\n",
    "features, targets, sequence_timestamps = build_sequences(input_data, target_data, input_timestamps, seq_len)\n",
    "\n",
    "# Print the shapes of the features and targets\n",
    "print(f\"Features shape: {features.shape()}\")\n",
    "print(f\"Targets shape: {targets.shape()}\")\n",
    "print(f\"Timestamps shape: {sequence_timestamps.shape()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a6229",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and testing sets\n",
    "(X_train, X_test, y_train, y_test), train_test_shuffle_indices = data_processing.split_data((features, targets), train_test_split_pct, shuffle=True)\n",
    "(X_train, X_valid, y_train, y_valid), train_valid_shuffle_indices = data_processing.split_data((X_train, y_train), train_valid_split_pct, shuffle=True)\n",
    "\n",
    "# Ensure the shuffle indices are numpy arrays\n",
    "assert isinstance(train_test_shuffle_indices, np.ndarray), \"train_test_shuffle_indices should be a numpy array\"\n",
    "assert isinstance(train_valid_shuffle_indices, np.ndarray), \"train_valid_shuffle_indices should be a numpy array\"\n",
    "\n",
    "# Extract the timestamps for each set\n",
    "train_timestamps, test_timestamps = data_processing.split_data(sequence_timestamps[train_test_shuffle_indices], train_test_split_pct, shuffle=False)[0]\n",
    "train_timestamps, valid_timestamps = data_processing.split_data(train_timestamps[train_valid_shuffle_indices], train_valid_split_pct, shuffle=False)[0]\n",
    "\n",
    "# Print the dataset information\n",
    "print('Training set:', X_train.shape(), y_train.shape())\n",
    "print('Validation set:', X_valid.shape(), y_valid.shape())\n",
    "print('Testing set:', X_test.shape(), y_test.shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38547b2c",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8445526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute normalization statistics from training data only\n",
    "mean, std, min, max = data_processing.compute_stats(X=X_train, axis=(0, 1))\n",
    "\n",
    "# Print the normalization statistics\n",
    "print('Normalization statistics (from training data only):')\n",
    "print('Min:', min.to_numpy())\n",
    "print('Max:', max.to_numpy())\n",
    "print('Mean:', mean.to_numpy())\n",
    "print('Std:', std.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e910a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define normalization functions based on the normalization method\n",
    "def normalization_fn(x: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Normalize input data based on the specified normalization method.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Tensor, input data to normalize\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor, normalized data\n",
    "    \"\"\"\n",
    "    \n",
    "    if normalization == 'minmax':\n",
    "        # Normalize using Min-Max normalization\n",
    "        return data_processing.min_max_normalize(x, min, max)\n",
    "    else:\n",
    "        # Normalize using Z-score normalization\n",
    "        return data_processing.z_score_normalize(x, mean, std)\n",
    "\n",
    "\n",
    "# Define denormalization functions based on the normalization method\n",
    "def denormalization_fn(x: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Denormalize input data based on the specified normalization method.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Tensor, normalized data to denormalize\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor, denormalized data\n",
    "    \"\"\"\n",
    "    \n",
    "    if normalization == 'minmax':\n",
    "        # Denormalize using Min-Max normalization\n",
    "        return data_processing.min_max_denormalize(x, min, max)\n",
    "    else:\n",
    "        # Denormalize using Z-score normalization\n",
    "        return data_processing.z_score_denormalize(x, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca894cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the training, validation, and test sets using training statistics\n",
    "X_train_norm = normalization_fn(X_train)\n",
    "X_valid_norm = normalization_fn(X_valid)\n",
    "X_test_norm = normalization_fn(X_test)\n",
    "\n",
    "# Normalize the target variable using the computed statistics\n",
    "y_train_norm = normalization_fn(y_train)\n",
    "y_valid_norm = normalization_fn(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151906f",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a29015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = AutoRegressive(\n",
    "    name = \"Stock prediction model\",\n",
    "    sequence_length = seq_len,\n",
    "    modules = [\n",
    "        DecoderTransformer(\n",
    "            input_dim = len(feature_columns),\n",
    "            n_embed = n_embed,\n",
    "            n_attention_heads = n_attention_heads,\n",
    "            n_decoder_blocks = n_decoder_blocks,\n",
    "            sequence_length = seq_len,\n",
    "            dropout = 0.3,\n",
    "            data_type = 'continuous'\n",
    "        ),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_units=32),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_units=y_train.shape()[-1])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the optimizer with lower learning rate\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = loss_functions.MeanSquareError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8964da2",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model with a first batch to initialize the weights\n",
    "# This is not necessary, but it is useful to know the input size\n",
    "\n",
    "# Disable gradient computation\n",
    "with context_manager.no_grad():\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Call the model with a batch of data to initialize it\n",
    "    model(X_train_norm[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a96c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model summary\n",
    "model.summary(recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ec304",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf48ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with early stopping\n",
    "history = model.fit(\n",
    "    X_train = X_train_norm,\n",
    "    y_train = y_train_norm,\n",
    "    X_valid = X_valid_norm,\n",
    "    y_valid = y_valid_norm,\n",
    "    epochs = epochs,\n",
    "    loss_fn = loss_fn,\n",
    "    optimizer = optimizer,\n",
    "    batch_size = batch_size,\n",
    "    metrics = [metrics.mean_absolute_error],\n",
    "    callbacks = [callbacks.EarlyStopping(monitor='val_loss', patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae677f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115df94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "data_analysis.plot_history(\n",
    "    train_loss = model.history[\"loss\"], \n",
    "    valid_loss = model.history[\"val_loss\"],\n",
    "    title = \"Training and Validation Loss\",\n",
    "    xlabel = \"Epoch\",\n",
    "    ylabel = \"Loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5fc223",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5076789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient computation for evaluation\n",
    "with context_manager.no_grad():\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Compute the predictions\n",
    "    predictions_norm = model(X_test_norm, batch_size=batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b883acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_accuracy(pred: Tensor, true: Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute the directional accuracy of predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - pred: Tensor, predicted values\n",
    "    - true: Tensor, true values\n",
    "    \n",
    "    Returns:\n",
    "    - float, directional accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # If predicting volatility, check if it increases or decreases\n",
    "    if predict_volatility:\n",
    "        # Check if the predicted and true values are increasing or decreasing\n",
    "        pred_direction = (pred.to_numpy()[1:] > pred.to_numpy()[:-1]).astype(int)\n",
    "        true_direction = (true.to_numpy()[1:] > true.to_numpy()[:-1]).astype(int)\n",
    "    else:\n",
    "        # For log returns, check if the predicted and true values are positive or negative\n",
    "        pred_direction = (pred.to_numpy() > 0).astype(int)\n",
    "        true_direction = (true.to_numpy() > 0).astype(int)\n",
    "    \n",
    "    # Calculate the directional accuracy\n",
    "    accuracy = np.mean(pred_direction == true_direction)\n",
    "    \n",
    "    # Return the directional accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286731c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize the predictions using the target normalization statistics\n",
    "predictions = denormalization_fn(predictions_norm)\n",
    "\n",
    "# Compute the mean absolute error on the test set\n",
    "mae = metrics.mean_absolute_error(predictions, y_test)\n",
    "\n",
    "# Print the test results\n",
    "print(f\"Test Results:\")\n",
    "print(f\"Mean Absolute Error: {mae.to_numpy()}\")\n",
    "\n",
    "# Compute the directional accuracy\n",
    "dir_acc = directional_accuracy(predictions, y_test)\n",
    "\n",
    "# Print the directional accuracy\n",
    "print(f\"Directional Accuracy: {dir_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096dc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the plot object based on whether we are predicting volatility or log returns\n",
    "plot_obj = \"Volatility\" if predict_volatility else \"Log Returns\"\n",
    "samples_to_plot = 100  # Number of samples to plot\n",
    "\n",
    "# Sort the test timestamps and corresponding predictions and true values\n",
    "sorted_indices = np.argsort(test_timestamps.to_numpy())\n",
    "sorted_test_timestamps = test_timestamps.to_numpy()[sorted_indices]\n",
    "sorted_y_test = y_test.to_numpy()[sorted_indices]\n",
    "sorted_predictions = predictions.to_numpy()[sorted_indices]\n",
    "\n",
    "# Extract the test dates by converting timestamps to datetime\n",
    "test_dates = np.array([datetime.fromtimestamp(int(t)) for t in sorted_test_timestamps])\n",
    "\n",
    "# Create a figure for plotting\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot the time series of true and predicted volatility\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(test_dates[-samples_to_plot:], sorted_y_test[-samples_to_plot:, close_price_idx], label=f'True {plot_obj}', alpha=0.7)\n",
    "plt.plot(test_dates[-samples_to_plot:], sorted_predictions[-samples_to_plot:, close_price_idx], label=f'Predicted {plot_obj}', alpha=0.7)\n",
    "plt.title(f'{plot_obj} Prediction Results')\n",
    "plt.ylabel(plot_obj)\n",
    "plt.legend()\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(nbins=20))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Scatter plot of true vs predicted volatility\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(y_test.to_numpy()[:, close_price_idx], predictions.to_numpy()[:, close_price_idx], alpha=0.5)\n",
    "plt.plot(\n",
    "    [y_test.to_numpy()[:, close_price_idx].min(), y_test.to_numpy()[:, close_price_idx].max()],\n",
    "    [y_test.to_numpy()[:, close_price_idx].min(), y_test.to_numpy()[:, close_price_idx].max()],\n",
    "    'r--', label='Perfect Prediction'\n",
    ")\n",
    "plt.xlabel(f'True {plot_obj}')\n",
    "plt.ylabel(f'Predicted {plot_obj}')\n",
    "plt.title('Prediction Scatter Plot')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9511aa9",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62287795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of generation steps\n",
    "num_generation_steps = 8\n",
    "\n",
    "# Select the middle index for the test set\n",
    "test_idx = len(X_test_norm.to_numpy()) // 2\n",
    "\n",
    "# Take the initial sequence and true continuation for evaluation\n",
    "initial_seq = X_test_norm[test_idx:test_idx+1]\n",
    "\n",
    "# Extract the corresponding timestamp for the test index\n",
    "test_timestamp = test_timestamps[test_idx].to_numpy()\n",
    "\n",
    "# Find the position of the test index in the sorted timestamps\n",
    "sorted_test_indices = np.argsort(test_timestamps.to_numpy())\n",
    "position_in_sorted = np.where(sorted_test_indices == test_idx)[0][0]\n",
    "\n",
    "# Take the true continuation from the test set\n",
    "if position_in_sorted + num_generation_steps <= len(sorted_test_indices):\n",
    "    consecutive_indices = sorted_test_indices[position_in_sorted:position_in_sorted + num_generation_steps]\n",
    "    true_continuation = y_test[consecutive_indices]\n",
    "else:\n",
    "    # If not enough steps are available, adjust the number of generation steps\n",
    "    available_steps = len(sorted_test_indices) - position_in_sorted\n",
    "    consecutive_indices = sorted_test_indices[position_in_sorted:position_in_sorted + available_steps]\n",
    "    true_continuation = y_test[consecutive_indices]\n",
    "    num_generation_steps = available_steps\n",
    "\n",
    "# Perform autoregressive prediction\n",
    "out_seq = model.autoregressive_generation(\n",
    "    x = initial_seq,\n",
    "    num_steps = num_generation_steps,\n",
    "    postprocess_fn = denormalization_fn\n",
    ")\n",
    "\n",
    "# Extract the generated sequence if out_seq is a generator\n",
    "generated_sequence = model.concat_generation(out_seq) if not isinstance(out_seq, Tensor) else out_seq\n",
    "\n",
    "# Extract the first batch of the generated sequence\n",
    "generated_sequence = generated_sequence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot autoregressive results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define the steps for plotting\n",
    "steps = range(num_generation_steps)\n",
    "feature_idx = close_price_idx\n",
    "\n",
    "# Plot the true continuation and generated sequence\n",
    "plt.plot(steps, true_continuation.to_numpy()[:num_generation_steps, close_price_idx], 'b-', label='True Values', linewidth=2, marker='o')\n",
    "plt.plot(steps, generated_sequence.to_numpy()[:, close_price_idx], 'r--', label='Generated Values', linewidth=2, marker='s')\n",
    "\n",
    "# Define the plot configurations\n",
    "plt.title(f'Autoregressive Prediction - {\"Volatility\" if predict_volatility else \"Log Returns\"}')\n",
    "plt.xlabel('Prediction Steps')\n",
    "plt.ylabel(f'{\"Volatility\" if predict_volatility else \"Log Returns\"}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13209d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the autoregressive directional accuracy\n",
    "auto_dir_acc = directional_accuracy(generated_sequence[:, close_price_idx], true_continuation[:, close_price_idx])\n",
    "\n",
    "# Print the autoregressive directional accuracy\n",
    "print(f\"Autoregressive Directional Accuracy: {auto_dir_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
