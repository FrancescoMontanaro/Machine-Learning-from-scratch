{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Add the path to the custom library to the system path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import the module from the custom library\n",
    "from src import utils\n",
    "from src import Model\n",
    "from src import layers\n",
    "from src import Tensor\n",
    "from src import metrics\n",
    "from src import callbacks\n",
    "from src import optimizers\n",
    "from src import activations\n",
    "from src import loss_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_pct = 0.1 # Percentage of samples to use for testing\n",
    "train_valid_split = 0.1 # Percentage of samples to use for validation\n",
    "learning_rate = 1e-4 # Learning rate for the optimizer\n",
    "batch_size = 128 # Number of samples to use for each batch\n",
    "epochs = 30 # Number of epochs to train the model\n",
    "seed = 1234 # Seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "ds_train = tfds.load('fashion_mnist', split='train', as_supervised=True)\n",
    "ds_test = tfds.load('fashion_mnist', split='test', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the images and labels from the dataset\n",
    "train_images, train_labels = zip(*tfds.as_numpy(ds_train))\n",
    "test_images, test_labels = zip(*tfds.as_numpy(ds_test))\n",
    "\n",
    "# Extract the number of classes in the dataset\n",
    "num_classes = len(np.unique(train_labels))    \n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X_train = Tensor(np.array(train_images))\n",
    "y_train = Tensor(np.array(train_labels), dtype=np.int8)\n",
    "X_test = Tensor(np.array(test_images))\n",
    "y_test = Tensor(np.array(test_labels), dtype=np.int8)\n",
    "\n",
    "# Split the training set into training, validation and testing sets\n",
    "X_train, X_valid, y_train, y_valid = utils.split_data(data=(X_train, y_train), split_pct=train_valid_split, shuffle=True)\n",
    "X_train, X_valid, y_train, y_valid = utils.split_data(data=(X_train, y_train), split_pct=train_valid_split, shuffle=True)\n",
    "\n",
    "# Print the dataset information\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print('Training set:', X_train.shape(), y_train.shape())\n",
    "print('Validation set:', X_valid.shape(), y_valid.shape())\n",
    "print('Testing set:', X_test.shape(), y_test.shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def normalize(X: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Normalize the input data by dividing by the maximum value in the training set.\n",
    "    \n",
    "    Parameters:\n",
    "    - X (Tensor): The input data to normalize\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor: The normalized input data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize the input data\n",
    "    return X / 255.0\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = normalize(X_train)\n",
    "X_valid = normalize(X_valid)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target values to one-hot encoded vectors\n",
    "y_train_encoded = utils.one_hot_encoding(y=y_train, n_classes=num_classes)\n",
    "y_valid_encoded = utils.one_hot_encoding(y=y_valid, n_classes=num_classes)\n",
    "\n",
    "# Print one sample encoding\n",
    "print(\"Sample target value:\", y_train[0])\n",
    "print(\"One-hot encoded value:\", y_valid_encoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(samples: list[np.ndarray], labels: list[np.ndarray]) -> None:\n",
    "    \"\"\"\n",
    "    Plot the samples in a grid.\n",
    "    \n",
    "    Parameters:\n",
    "    - samples (list[np.ndarray]): The samples to plot\n",
    "    - labels (list[np.ndarray]): The labels of the samples\n",
    "    \"\"\"\n",
    "        \n",
    "    # Plot the samples in a grid\n",
    "    fig, axes = plt.subplots(1, len(samples), figsize=(20, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(samples[i], cmap='gray')\n",
    "        ax.set_title(f'Label: {labels[i]}')\n",
    "        ax.axis('off')\n",
    "        \n",
    "# Plot the first 10 samples\n",
    "plot_samples(list(X_train.data[:10]), list(y_train.data[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model(\n",
    "    name = \"Fashion MNIST Classifier\",\n",
    "    modules = [\n",
    "        layers.Conv2D(num_filters=32, kernel_size=(3, 3), activation=activations.ReLU(), padding='same'),\n",
    "        layers.MaxPool2D(size=(2, 2), stride=(2, 2)),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Conv2D(num_filters=16, kernel_size=(3, 3), activation=activations.ReLU(), padding='same'),\n",
    "        layers.MaxPool2D(size=(2, 2), stride=(2, 2)),\n",
    "        layers.Dropout(rate=0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.BatchNormalization(momentum=0.9),\n",
    "        layers.Dense(num_units=64, activation=activations.ReLU()),\n",
    "        layers.Dense(num_units=num_classes, activation=activations.Softmax())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = loss_functions.CategoricalCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model with a first batch to initialize the weights\n",
    "# This is not necessary, but it is useful to know the input size\n",
    "model(X_train[:batch_size]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train = X_train,\n",
    "    y_train = y_train_encoded,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = loss_fn,\n",
    "    X_valid = X_valid,\n",
    "    y_valid = y_valid_encoded,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    metrics = [metrics.accuracy],\n",
    "    callbacks = [callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "utils.plot_history(history[\"loss\"], history[\"val_loss\"], \"Training and Validation Loss\", \"Epoch\", \"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels\n",
    "predictions = model(X_test, batch_size=batch_size, verbose=True)\n",
    "\n",
    "# Apply the argmax function to the predictions\n",
    "predictions = Tensor(np.argmax(predictions.data, axis=1), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy\n",
    "accuracy = metrics.accuracy(y_test, predictions)\n",
    "confusion_matrix = metrics.confusion_matrix(num_classes, y_test, predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy.data:.2f}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "utils.plot_confusion_matrix(confusion_matrix.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
